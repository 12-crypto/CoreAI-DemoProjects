{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Name: Amex Default Prediction\n",
    "\n",
    "## Introduction and Objective\n",
    "\n",
    "This Jupyter Notebook is dedicated to predicting default risk using the American Express Default Prediction dataset. By leveraging advanced machine learning techniques to analyze and predict customer default likelihood, this project aims to enhance financial decision-making processes.\n",
    "\n",
    "## Description\n",
    "\n",
    "Credit default prediction is crucial for managing risk in a consumer lending business. This project uses an industrial-scale dataset provided by American Express, the world's largest payment card issuer, to build a model that challenges the current models in production. Through this competition, participants can apply their machine learning skills to predict credit default, potentially improving the customer experience and influencing future credit risk management.\n",
    "\n",
    "## Dataset Overview and Description\n",
    "\n",
    "The objective of this competition is to predict the probability that a customer will not pay back their credit card balance amount in the future based on their monthly customer profile. The target binary variable is calculated by observing an 18-month performance window after the latest credit card statement.\n",
    "\n",
    "### Features\n",
    "The dataset contains aggregated profile features for each customer at each statement date, categorized as follows:\n",
    "\n",
    "- **D_*** = Delinquency variables\n",
    "- **S_*** = Spend variables\n",
    "- **P_*** = Payment variables\n",
    "- **B_*** = Balance variables\n",
    "- **R_*** = Risk variables\n",
    "\n",
    "### Categorical Features:\n",
    "['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
    "\n",
    "### Required Files:\n",
    "\n",
    "- From the [Kaggle Dataset Main Page](https://www.kaggle.com/competitions/amex-default-prediction/data), download the files and place them in a `data` directory (must be accesssible by this notebook):\n",
    "  - `train_labels.csv` - Target label for each customer_ID\n",
    "  - `sample_submission.csv` - A sample submission file in the correct format\n",
    "\n",
    "\n",
    "- From the Kaggle Feather Files from [Kaggle Feather Dataset](https://www.kaggle.com/datasets/munumbutt/amexfeather), download the `.ftr` (Feather format) files for efficient data handling and place them in the same `data` directory (must be accesssible by this notebook):\n",
    "  - `train_data.ftr`\n",
    "  - `test_data.ftr`\n",
    "\n",
    "## Install Required Packages\n",
    "\n",
    "To enhance the functionality of the environment, you may need to install some libraries not pre-installed in the CoreAI environment but required for this notebook. Follow these steps to install the necessary libraries from the `requirements.txt` file:\n",
    "\n",
    "### Create and Activate the Virtual Environment:\n",
    "\n",
    "Open your terminal or command prompt within the Jupyter notebook. `File -> New -> Terminal` and type `bash` to get a shell compatible with the following commands.\n",
    "\n",
    "Navigate to the project directory where the notebook is to set up the environment.\n",
    "\n",
    "Execute the following commands to create and activate the virtual environment:\n",
    "\n",
    "```bash\n",
    "python3 -m venv --system-site-packages myvenv\n",
    "source myvenv/bin/activate\n",
    "pip3 install ipykernel\n",
    "python -m ipykernel install --user --name=myvenv --display-name=\"Python (myvenv)\"\n",
    "```\n",
    "\n",
    "### Install Required Libraries\n",
    "\n",
    "Before running the following command in this Jupyter notebook, make sure you are in the directory where the Jupyter Notebook and virtual environment is located. Load the newly created \"Python (myenv)\" kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!. ./myvenv/bin/activate; pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import warnings\n",
    "\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feather files (Compressed version of the original data to handle with pandas)\n",
    "train_path = 'data/train_data.ftr'\n",
    "test_path = 'data/test_data.ftr'\n",
    "labels_path = 'data/train_labels.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Feather files for train and test data\n",
    "train_data = pd.read_feather(train_path)\n",
    "test_data = pd.read_feather(test_path)\n",
    "\n",
    "# Load the target labels CSV file\n",
    "labels_data = pd.read_csv(labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the target labels with the training data\n",
    "train_data = pd.merge(train_data, labels_data, on='customer_ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the shapes of the dataframes\n",
    "print(\"Shape of train data:\", train_data.shape)\n",
    "print(\"Shape of test data:\", test_data.shape)\n",
    "print(\"Shape of merged train data:\", train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info(max_cols=train_data.shape[1] ,show_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data1 = train_data.groupby('customer_ID').tail(1).set_index('customer_ID')\n",
    "test_data1 = test_data.groupby('customer_ID').tail(1).set_index('customer_ID')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "## Features weights on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_features(table: pd.DataFrame) -> dict:\n",
    "    '''\n",
    "    This function stores all columns that belong to each feature from the DataFrame.\n",
    "    '''\n",
    "    features = {'Delinquency':None, 'Spend':None, 'Payment':None, 'Balance':None, 'Risk':None}\n",
    "\n",
    "    for k, v in features.items():\n",
    "        features[k] = len([c for c in table.columns if c.startswith(k[0])])\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat = count_features(train_data1)\n",
    "test_feat = count_features(test_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('Train amount of features', 'Test amount of features'),\n",
    "    specs=[[{\"type\": \"pie\"}, {\"type\": \"pie\"}]]\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Pie(\n",
    "    labels=tuple(train_feat.keys()), values=tuple(train_feat.values()), name='Train'), row=1, col=1\n",
    "    )\n",
    "\n",
    "fig.add_trace(go.Pie(\n",
    "    labels=tuple(test_feat.keys()), values=tuple(test_feat.values()), name='Test'), row=1, col=2\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='Percentage of features'\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_features(table: pd.DataFrame) -> dict:\n",
    "    '''\n",
    "    Finds the percentage of missing data per feature of the DataFrame.\n",
    "    '''\n",
    "    features = {'Delinquency':None, 'Spend':None, 'Payment':None, 'Balance':None, 'Risk':None}\n",
    "\n",
    "    for k, v in features.items():\n",
    "        cols = [c for c in table.columns if c.startswith(k[0])]\n",
    "        temp_df = table[cols]\n",
    "        features[k] = round(np.mean(temp_df.isnull().sum()), 2)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_miss = null_features(train_data1)\n",
    "test_miss = null_features(test_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=1, cols=1,\n",
    "    subplot_titles=('Train/Test null values per feature', ''),\n",
    "    specs=[[{\"type\": \"bar\"}]]\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=tuple(train_miss.keys()), y=tuple(train_miss.values()), name='Train'\n",
    "))\n",
    "fig.add_trace(go.Bar(\n",
    "    x=tuple(test_miss.keys()), y=tuple(test_miss.values()), name='Test'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='Mean presence of null values'\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train_data['target'].value_counts()\n",
    "target_0 = round((target[0]/train_data['target'].count()*100), 2)\n",
    "target_1 = round((target[1]/train_data['target'].count()*100), 2)\n",
    "\n",
    "df_target_percentage = pd.DataFrame(data={'Target':['0', '1'], 'Percentage':[target_0, target_1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(df_target_percentage, x='Target', y='Percentage', color='Target', title=\"Target distribution percentage\")\n",
    "fig.update_layout()\n",
    "fig.show()\n",
    "\n",
    "del target, target_0, target_1, df_target_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of unique customers in train: {train_data[\"customer_ID\"].nunique()}')\n",
    "print(f'Represent the {round((train_data[\"customer_ID\"].nunique()/len(train_data))*100, 2)}% of the train data.')\n",
    "print()\n",
    "print(f'Each customer appear in the data around {round(len(train_data)/train_data[\"customer_ID\"].nunique(), 2)} times')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Delinquency = train_data1[[c for c in train_data1.columns if c.startswith('D')] +  ['target']]\n",
    "Spend       = train_data1[[c for c in train_data1.columns if c.startswith('S')] +  ['target']]\n",
    "Payment     = train_data1[[c for c in train_data1.columns if c.startswith('P')] +  ['target']]\n",
    "Balance     = train_data1[[c for c in train_data1.columns if c.startswith('B')] +  ['target']]\n",
    "Risk        = train_data1[[c for c in train_data1.columns if c.startswith('R')] +  ['target']]\n",
    "# Pandas 2.0 change to corr: # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr(table: pd.DataFrame, title: str, size: tuple = (12,9), annot: str = False):\n",
    "    plt.figure(figsize=size)\n",
    "    sns.heatmap(\n",
    "        table.corr(numeric_only=True),\n",
    "        annot=annot,\n",
    "        cmap='coolwarm'\n",
    "    )\n",
    "    plt.title(title, fontdict={'fontsize': 18});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr(Delinquency.corr(numeric_only=True), title='Delinquency correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr(Spend.corr(numeric_only=True), title='Spend correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr(Payment.corr(numeric_only=True), title='Payment correlation', annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr(Balance.corr(numeric_only=True), title='Balance correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr(Risk.corr(numeric_only=True), title='Risk correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del Delinquency, Spend, Payment, Balance, Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train_data1.corrwith(train_data1['target'], axis=0, numeric_only=True)\n",
    "val = [str(round(v ,1) *100) + '%' for v in target.values]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(y=target.index, x=target.values, orientation='h', text = val))\n",
    "fig.update_layout(title = \"Correlation of variables with Target\", width = 700, height = 3000)\n",
    "fig.show()\n",
    "\n",
    "del target, val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the Datasets to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data1.to_csv('data/train_data.csv')\n",
    "test_data1.to_csv('data/test_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting path for the CSV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'data/train_data.csv' # Already with target in it\n",
    "test_path = 'data/test_data.csv'\n",
    "sample_path = 'data/sample_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(train_path).set_index('customer_ID')\n",
    "test_data = pd.read_csv(test_path).set_index('customer_ID')\n",
    "sample_data = pd.read_csv(sample_path)\n",
    "\n",
    "print(f'Train shape: {train_data.shape}')\n",
    "print(f'Test shape: {test_data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with missing values and high correlation\n",
    "Variables >= 80% of missing values will be removed, and the rest will have imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_null = [c for c in train_data.columns if (train_data[c].isnull().sum()/train_data.shape[0]) >= 0.8]\n",
    "print(len(var_null))\n",
    "\n",
    "for v in var_null:\n",
    "    print(v, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = train_data.corr(numeric_only=True)\n",
    "drop_corr = []\n",
    "\n",
    "for i in range(len(corr.columns) - 1):\n",
    "   if corr.columns[i] in drop_corr:\n",
    "      continue\n",
    "\n",
    "   for j in range(i+1, len(corr.columns)):\n",
    "      if abs(corr.iloc[i,j]) > 0.9 and corr.columns[j] not in drop_corr:\n",
    "         drop_corr.append(corr.columns[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = var_null + drop_corr\n",
    "print(len(to_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop(to_drop, axis=1, inplace=True)\n",
    "test_data.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(f'Train shape: {train_data.shape}')\n",
    "print(f'Test shape: {test_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del to_drop, var_null, drop_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersample data for a balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay = train_data[train_data['target']==0]\n",
    "default = train_data[train_data['target']==1]\n",
    "\n",
    "print(f'Payable shape: {pay.shape}, and default shape: {default.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay = pay.head(len(pay)*3//4)\n",
    "\n",
    "print(f'Payable shape: {pay.shape}, and default shape: {default.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data2 = pd.concat([pay, default]).sort_values(by='S_2')\n",
    "\n",
    "print(train_data2.shape)\n",
    "train_data2['target'].value_counts()\n",
    "\n",
    "del pay, default,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label encoder for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def remove_or_label(table: pd.DataFrame, method: str) -> pd.DataFrame:\n",
    "    '''Remove or encode categorical features of a DataFrame.'''\n",
    "    cat_feat = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
    "\n",
    "    if method=='label_encoder':\n",
    "        le = LabelEncoder()\n",
    "        for feat in cat_feat:\n",
    "            if feat in table.columns:\n",
    "                table[feat] = le.fit_transform(table[feat])\n",
    "    \n",
    "    elif method=='remove':\n",
    "        for feat in cat_feat:\n",
    "            if feat in table.columns:\n",
    "                table.drop(feat, axis=1, inplace=True)\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.drop(['target', 'S_2'], axis=1)\n",
    "y = train_data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object', 'category']).columns\n",
    "print(\"Non-numeric columns:\", non_numeric_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical columns to numeric using LabelEncoder or a similar method\n",
    "for column in non_numeric_columns:\n",
    "    le = LabelEncoder()\n",
    "    X[column] = le.fit_transform(X[column].astype(str))\n",
    "\n",
    "# Now you can use SimpleImputer on the whole dataset\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "X_imputed = imp.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imp = SimpleImputer(strategy='mean')\n",
    "# scaler = StandardScaler()\n",
    "# pca = PCA(n_components=108, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_imputed = imp.fit_transform(X.values)\n",
    "# X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# pca.fit(X_scaled)\n",
    "\n",
    "# X_pca = pca.transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(np.cumsum(pca.explained_variance_ratio_*100))\n",
    "# plt.xlabel('Number of components')\n",
    "# plt.ylabel('Explained variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var_ex_ratio = np.cumsum(pca.explained_variance_ratio_*100)\n",
    "\n",
    "# for ver in range(0, len(var_ex_ratio), 2):\n",
    "#     print(f'Variance explained by the first {ver} principal components = {round(var_ex_ratio[ver], 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split and oversampling dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "X_ovr, y_ovr = BorderlineSMOTE(random_state=42).fit_resample(X_train, y_train)\n",
    "print(f'X shape: {X_ovr.shape}')\n",
    "print(f'y shape: {y_ovr.shape}')\n",
    "print(y_ovr.value_counts())\n",
    "\n",
    "X_train = X_train.round(decimals=2)\n",
    "X_test = X_test.round(decimals=2)\n",
    "\n",
    "del X, y, X_imputed, # var_ex_ratio, X_pca, X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'eval_metric': ['auc', 'binary_logloss']\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "xgb = XGBClassifier(learning_rate=0.02, n_estimators=600,\n",
    "        objective='binary:logistic', nthread=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search = RandomizedSearchCV(\n",
    "    xgb, param_distributions=xgb_params, n_iter=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-2, cv=3,\n",
    "    verbose=3,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc = XGBClassifier(n_jobs=-2)\n",
    "xgbc.fit(\n",
    "    X_ovr, y_ovr\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc_pred = xgbc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, xgbc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference of these params took from: https://www.kaggle.com/code/kellibelcher/amex-default-prediction-eda-lgbm-baseline\n",
    "params = {'boosting_type': 'gbdt',\n",
    "          'n_estimators': 1000,\n",
    "          'num_leaves': 50,\n",
    "          'learning_rate': 0.05,\n",
    "          'colsample_bytree': 0.9,\n",
    "          'min_child_samples': 2000,\n",
    "          'max_bins': 500,\n",
    "          'reg_alpha': 2,\n",
    "          'objective': 'binary',\n",
    "          'random_state': 21}\n",
    "\n",
    "model = LGBMClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X_ovr, y_ovr,\n",
    "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "    callbacks=[early_stopping(100), log_evaluation(200)],\n",
    "    eval_metric=['auc, binary_logloss']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "\n",
    "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
    "ax.set_title('Confusion Matrix\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['False','True'])\n",
    "ax.yaxis.set_ticklabels(['False','True'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! test -d models || mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/model_4.pkl', 'wb') as out:\n",
    "    pickle.dump(model, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/model_4.pkl', 'rb') as entry:\n",
    "    imp_model = pickle.load(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = remove_or_label(test_data, 'label_encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imputed = imp.fit_transform(test_data.drop('S_2', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_scaled = scaler.fit_transform(test_imputed)\n",
    "# test_pca = pca.fit_transform(test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = imp_model.predict_proba(test_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data['prediction'] = test_pred[:, 1]\n",
    "\n",
    "sample_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myvenv)",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d3b0fb52ec915459d29dcc336458c8a8cd5b7af1e1cec9131ee28055d11f861"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
