{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f56626ae-4bfa-4110-9afd-5cf0d30ebda8",
   "metadata": {},
   "source": [
    "# Image Backgound Removal\n",
    "\n",
    "### Introduction and Objective\n",
    "\n",
    "This project demonstrates the use of the U-2-NETp model for various image processing tasks, including background removal, bounding box creation, and salient feature highlighting. U-2-NETp is a lightweight model designed specifically for salient object detection, offering a balance between performance and computational efficiency. This makes it suitable for real-time applications where resources may be limited.\n",
    "\n",
    "The primary objective of this project is to showcase the capabilities of U-2-NETp in handling common image processing tasks by leveraging deep learning techniques. Users can upload their images to the system, which then processes these images to remove backgrounds, create bounding boxes around detected objects, and highlight salient features. The processed images can be used for various applications such as image editing, object detection, and feature extraction.\n",
    "\n",
    "### Ensure Proper Folder Structure\n",
    "\n",
    "To ensure that the notebook runs correctly, it is important to have the proper folder structure. The following folders should be created in the project root directory:\n",
    "\n",
    "- `images`: This folder will store the user-uploaded images.\n",
    "- `results`: This folder will store the processed images generated by the U-2-NETp model.\n",
    "\n",
    "Ensure that these directories are created before running the notebook cells. The paths within the notebook and the `u2net_test.py` script have been adjusted to use these directories.\n",
    "\n",
    "### Sample Images\n",
    "There are already 3 sample images available in the images folder. If you want to see the results without uploading your own images, you can skip the upload cell and run the next cell to see the results.\n",
    "\n",
    "### Model Usage\n",
    "\n",
    "The trained U-2-NETp model can be used for the following tasks:\n",
    "- **Background Removal**: This removes the background from images, isolating the main object.\n",
    "- **Bounding Box Creation**: This draws bounding boxes around the detected objects in the images.\n",
    "- **Salient Feature Highlighting**: This highlights the most salient features in the images.\n",
    "\n",
    "### How To Use\n",
    "\n",
    "\n",
    "1. **Run the Notebook:**\n",
    "\n",
    "   - Open the `U_2_Netp_Demonstration_Colab.ipynb` notebook in a Jupyter environment.\n",
    "   - Execute the notebook cells sequentially to perform data analysis, model training, and evaluation.\n",
    "\n",
    "2. **Model Usage:**\n",
    "\n",
    "   - Upload images using the function provided in the notebook.\n",
    "   - The images will be stored in the `images` folder.\n",
    "   - Run the `u2net_test.py` script to process the images and store the results in the `results` folder.\n",
    "   - Use the processed images for background removal, bounding box creation, and salient feature highlighting.\n",
    "  \n",
    "**Note:** \n",
    "- The libraries required for this project are already available in the CoreAI environment, so there is no need to create a virtual environment.\n",
    "- Make Sure Images are in `jpeg` format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d80afd2-287c-4411-bfd7-43533733ffe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Image\n",
    "import os\n",
    "target_directory = 'images'\n",
    "\n",
    "# Ensure the target directory exists\n",
    "if not os.path.exists(target_directory):\n",
    "    print(f\"Directory {target_directory} does not exist.\")\n",
    "else:\n",
    "    print(f\"Target directory: {target_directory}\")\n",
    "\n",
    "# Function to handle uploaded files\n",
    "def handle_upload(change):\n",
    "    print(\"Upload started...\")\n",
    "    # Print the structure of 'change' to understand its content\n",
    "    print(change)\n",
    "    \n",
    "    for file_upload in change['new']:\n",
    "        print(f\"Handling file: {file_upload}\")\n",
    "        filepath = os.path.join(target_directory, file_upload['name'])\n",
    "        print(f\"Saving to: {filepath}\")\n",
    "        with open(filepath, 'wb') as f:\n",
    "            f.write(file_upload['content'])\n",
    "        print(f'Saved {file_upload[\"name\"]} to {filepath}')\n",
    "    # List the files in the target directory after upload\n",
    "    print(f'Files in target directory ({target_directory}): {list(Path(target_directory).glob(\"*\"))}')\n",
    "    print(\"Upload completed.\")\n",
    "\n",
    "# Create an output widget to capture print statements\n",
    "output = widgets.Output()\n",
    "\n",
    "# Create an upload widget\n",
    "upload_widget = widgets.FileUpload()\n",
    "\n",
    "# Function to handle the change event using output widget\n",
    "def handle_upload_with_output(change):\n",
    "    with output:\n",
    "        handle_upload(change)\n",
    "\n",
    "# Attach the observer to the upload widget\n",
    "upload_widget.observe(handle_upload_with_output, names='value')\n",
    "\n",
    "# Display the upload widget and output widget\n",
    "display(upload_widget, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90a4c54-263a-4548-90f7-eccbd94faa5a",
   "metadata": {},
   "source": [
    "### `u2net_test.py` Script Execution\n",
    "\n",
    "The below command runs the `u2net_test.py` script, which:\n",
    "\n",
    "1. **Loads Images**: Reads images from the `images` directory using `glob`.\n",
    "2. **Prepares Data**: Uses a custom dataset (`SalObjDataset`) and applies transformations (resizing and tensor conversion) to prepare images for model input.\n",
    "3. **Loads Model**: Loads the pre-trained U2NET or U2NETP model, which are used for salient object detection, image background removal, and creating bounding boxes.\n",
    "4. **Inference**: Processes each image to generate segmentation maps, normalizes the predictions, and saves the results in the `results` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aa0d24-83ba-4a06-8634-56ce04b32bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -W ignore u2net_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8b5ad6-c43d-484b-8425-11c6720f7e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import numpy as np\n",
    "from PIL import Image as Img\n",
    "import cv2\n",
    "\n",
    "\n",
    "\n",
    "image_dir = os.path.join(os.getcwd(), 'results')\n",
    "names = [name[:-4] for name in os.listdir(image_dir)]\n",
    "THRESHOLD = 0.9\n",
    "RESCALE = 255\n",
    "LAYER = 2\n",
    "COLOR = (0, 0, 0)\n",
    "THICKNESS = 4\n",
    "SAL_SHIFT = 100\n",
    "\n",
    "print('1. Original Image, 2. Background Removed Image, 3. Bounding Box Detected Image, 4. Salient Feature Highlighted Image')\n",
    "for name in names:\n",
    "\n",
    "  # BACKGROUND REMOVAL\n",
    "\n",
    "  if name == '.ipynb_checkpo':\n",
    "    continue\n",
    "  print(name)  \n",
    "  output = load_img('results/'+name+'.png')\n",
    "  out_img = img_to_array(output)\n",
    "  out_img /= RESCALE\n",
    "\n",
    "  out_img[out_img > THRESHOLD] = 1\n",
    "  out_img[out_img <= THRESHOLD] = 0\n",
    "\n",
    "  shape = out_img.shape\n",
    "  a_layer_init = np.ones(shape = (shape[0],shape[1],1))\n",
    "  mul_layer = np.expand_dims(out_img[:,:,0],axis=2)\n",
    "  a_layer = mul_layer*a_layer_init\n",
    "  rgba_out = np.append(out_img,a_layer,axis=2)\n",
    "\n",
    "  input = load_img('images/'+name+'.jpeg')\n",
    "  inp_img = img_to_array(input)\n",
    "  inp_img /= RESCALE\n",
    "\n",
    "  a_layer = np.ones(shape = (shape[0],shape[1],1))\n",
    "  rgba_inp = np.append(inp_img,a_layer,axis=2)\n",
    "\n",
    "  rem_back = (rgba_inp*rgba_out)\n",
    "  rem_back_scaled = rem_back*RESCALE\n",
    "\n",
    "  # BOUNDING BOX CREATION\n",
    "\n",
    "  out_layer = out_img[:,:,LAYER]\n",
    "  x_starts = [np.where(out_layer[i]==1)[0][0] if len(np.where(out_layer[i]==1)[0])!=0 else out_layer.shape[0]+1 for i in range(out_layer.shape[0])]\n",
    "  x_ends = [np.where(out_layer[i]==1)[0][-1] if len(np.where(out_layer[i]==1)[0])!=0 else 0 for i in range(out_layer.shape[0])]\n",
    "  y_starts = [np.where(out_layer.T[i]==1)[0][0] if len(np.where(out_layer.T[i]==1)[0])!=0 else out_layer.T.shape[0]+1 for i in range(out_layer.T.shape[0])]\n",
    "  y_ends = [np.where(out_layer.T[i]==1)[0][-1] if len(np.where(out_layer.T[i]==1)[0])!=0 else 0 for i in range(out_layer.T.shape[0])]\n",
    "  \n",
    "  startx = min(x_starts)\n",
    "  endx = max(x_ends)\n",
    "  starty = min(y_starts)\n",
    "  endy = max(y_ends)\n",
    "  start = (startx,starty)\n",
    "  end = (endx,endy)\n",
    "\n",
    "  box_img = inp_img.copy()\n",
    "  box_img = cv2.rectangle(box_img, start, end, COLOR, THICKNESS)\n",
    "  box_img = np.append(box_img,a_layer,axis=2)\n",
    "  box_img_scaled = box_img*RESCALE\n",
    "\n",
    "  # SALIENT FEATURE MAP\n",
    "\n",
    "  sal_img = inp_img.copy()\n",
    "  add_layer = out_img.copy()\n",
    "  add_layer[add_layer==1] = SAL_SHIFT/RESCALE\n",
    "  sal_img[:,:,LAYER] += add_layer[:,:,LAYER]\n",
    "  sal_img = np.append(sal_img,a_layer,axis=2)\n",
    "  sal_img_scaled = sal_img*RESCALE\n",
    "  sal_img_scaled[sal_img_scaled>RESCALE] = RESCALE\n",
    "\n",
    "  # OUTPUT RESULTS\n",
    "\n",
    "  inp_img*=RESCALE\n",
    "  inp_img = np.append(inp_img,RESCALE*a_layer,axis=2)\n",
    "  inp_img = cv2.resize(inp_img,(int(shape[1]/3),int(shape[0]/3)))\n",
    "  rem_back = cv2.resize(rem_back_scaled,(int(shape[1]/3),int(shape[0]/3)))\n",
    "  box_img = cv2.resize(box_img_scaled,(int(shape[1]/3),int(shape[0]/3)))\n",
    "  sal_img = cv2.resize(sal_img_scaled,(int(shape[1]/3),int(shape[0]/3)))\n",
    "  result = np.concatenate((inp_img,rem_back,box_img,sal_img),axis=1)\n",
    "  result_img = Img.fromarray(result.astype('uint8'), 'RGBA')\n",
    "\n",
    "  display(result_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c3f0b8-b381-4534-8b26-8064b7683056",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
