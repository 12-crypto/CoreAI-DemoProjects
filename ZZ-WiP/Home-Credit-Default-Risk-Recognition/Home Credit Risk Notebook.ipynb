{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <b>  Home Credit Default Risk Recognition </b>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<b> Domain Background </b>\n",
    "\n",
    "An important fraction of the population finds it difficult to get their home loans approved due to insufficient or absent credit history. This prevents them to buy their own dream homes and at times even forces them to rely on other sources of money which may be unreliable and have exorbitant interest rates. Conversely, it is a major challenge for banks and other finance lending agencies to decide for which candidates to approve housing loans. The credit history is not always a sufficient tool for decisions, since it is possible that those borrowers with a long credit history can still default on the loan and some people with a good chance of loan repayment may simply not have a sufficiently long credit history. \n",
    "\n",
    "A number of recent researchers have applied machine learning to predict the loan default risk. This is important since a machine learning-based classification tool to predict the loan default risk which uses more features than just the traditional credit history can be of great help for both, potential borrowers, and the lending institutions. At a personal level, this project will help me gain an insight into which factors are the most important indicators for a bank when making a loan decision in case I decide to apply for a housing loan in the future.\n",
    "\n",
    "<b> Problem Statement </b>\n",
    "\n",
    "The [problem and associated data](https://www.kaggle.com/c/home-credit-default-risk/overview) has been provided by Home Call Credit Group for a Kaggle competition. The problem can be described as, <i> “A binary classification problem where the inputs are various features describing the financial and behavioral history of the loan applicants, in order to predict whether the loan will be repaid or defaulted.” </i> \n",
    "\n",
    "<b> Project Novelty </b>\n",
    "\n",
    "<i> The notebook provides a complete end-to-end workflow for building a binary classifier, and includes methods like automated feature engineering for connecting relational databases, comparison of different classifiers on imbalanced data, and hyperparameter tuning using Bayesian optimization. </i>\n",
    "\n",
    "<b> Datasets and Inputs </b>\n",
    "\n",
    "The [dataset files](https://www.kaggle.com/c/home-credit-default-risk/data) are provided on the Kaggle website in the form of multiple CSV files and are free to download. The dataset files are described as per Figure 1.\n",
    "\n",
    "![image](https://storage.googleapis.com/kaggle-media/competitions/home-credit/home_credit.png)\n",
    " \n",
    "Figure 1- Description and connectivity of the Home Credit Default Risk dataset\n",
    "\n",
    "As seen in Figure 1, the file application_{train|test}.csv contains the main table containing the training dataset (307511 samples) and test dataset (48744 samples), with each row representing one loan identified by the feature SK_ID_CURR. The training set contains the variable TARGET with binary values (0: the loan was repaid or 1: the loan was not repaid). There are many input files available, which will be analysed for input features to train the model. The large number of input features and training samples will allow me to identify the important factors and for constructing a credit default risk classification model.\n",
    "\n",
    "<b> Project Design and Solution </b>\n",
    "\n",
    "The project has been divided into five parts-\n",
    "\n",
    "1. <u>Data Preparation</u> - Before starting the modeling, we need to import the necessary libraries and the datasets. If there are more than one files, then all need to be imported before we can look at the feature types and number of rows/columns in each file. \n",
    "\n",
    "2. <u>Exploratory Data Analysis</u> - After data importing, we can investigate the data and answer questions like- How many features are present and how are they interlinked? What is the data quality, are there missing values? What are the different data types, are there many categorical features? Is the data imbalanced? And most importantly, are there any obvious patterns between the predictor and response features? \n",
    "\n",
    "3. <u>Feature Engineering</u> - After exploring the data distributions, we can conduct feature engineering to prepare the data for model training. This includes operations like replacing outliers, imputing missing values, one-hot encoding categorical variables, and rescaling the data. Since there are number of relational databases, we can use extract, transform, load (ETL) processes using automated feature Engineering with [Featuretools](https://www.featuretools.com/) to connect the datasets. The additional features from these datasets will help improve the results over the base case (logistic regression). \n",
    "\n",
    "4. <u>Classifier Models: Training, Prediction and Comparison</u> - After the dataset is split into training and testing sets, we can correct the data imbalances by undersampling the majority class. Then, we can training the different classifier models (Logistic Regression, Random Forest, Decision Tree, Gaussian Naive Bayes, XGBoost, Gradient Boosting, LightGBM) and compare their performance on the test data using metrics like accuracy, F1-score and ROC AUC. After choosing the best classifier, we can use K-fold cross validation to select the best model. This will help us choose parameters that correspond to the best performance without creating a separate validation dataset.\n",
    "\n",
    "5. <u>Hyperparameter Tuning</u> - After choosing the binary classifier, we can tune the hyperparameters for improving the model results through grid search. The hyperparameter tuning process will use an objective function on the given domain space, and an optimization algorithm to give the results.\n",
    "\n",
    "## How To Use\n",
    "\n",
    "1. **Setup Environment**:\n",
    "   - Clone the repository or download the specific project files.\n",
    "   - Ensure Python 3.x is installed.\n",
    "\n",
    "2. **Install Required Packages**:\n",
    "\n",
    "   - To enhance the functionality of the CoreAI environment, you may need to install some libraries not pre-installed but required for this notebook. Follow these steps to install the necessary libraries from the `requirements.txt` file:\n",
    "\n",
    "   **2.1 Create and Activate the Virtual Environment:**\n",
    "   \n",
    "   Open your terminal or command prompt within the jupyter notebook. `File -> New -> Terminal`\n",
    "   \n",
    "   Navigate to the project directory where you want to set up the environment.\n",
    "   \n",
    "   Execute the following commands to create and activate the virtual environment in a `bash` shell:\n",
    "   \n",
    "   ```\n",
    "   python3 -m venv --system-site-packages myvenv #myvenv is name of virtual environment you can change it\n",
    "   source myvenv/bin/activate\n",
    "   pip3 install ipykernel\n",
    "   python -m ipykernel install --user --name=myvenv --display-name=\"Python (myvenv)\"\n",
    "   ```\n",
    "\n",
    "   ### Important Note\n",
    "\n",
    "It is crucial to load the new \"myvenv\" kernel for the notebook to work correctly. If the new \"myvenv\" kernel is not loaded, the required libraries and environment settings will not be applied, and the notebook will not function as expected. Do this before attempting to install the required libraries.\n",
    "\n",
    "   **2.2 Install Required Libraries**\n",
    "   \n",
    "   Before running the following command in jupyter notebook, make sure you are in the directory where the Jupyter Notebook and virtual environment is located. This ensures the ./ path is always current. You can use the cd command to change to your project directory and pwd to verify your current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "!. ./myvenv/bin/activate; pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <b> 1.0 Data Preparation </b>\n",
    "\n",
    "Before starting the modeling, we can import the necessary libraries and the data. If there are more data files, then all need to be imported and then we can look at the features types and number of rows/columns in each data file. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 1.1 Install/Import packages </b>\n",
    "\n",
    "First all the packages required for the workflow are installed and imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries/packages\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns             \n",
    "from timeit import default_timer as timer\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "import json\n",
    "import itertools\n",
    "import pprint\n",
    "from pydash import at\n",
    "import gc\n",
    "import re\n",
    "\n",
    "#Import sklearn helper metrics and transformations\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,precision_score,recall_score,roc_auc_score,classification_report,roc_curve,auc, f1_score\n",
    "\n",
    "#Import models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "#import library for hyperparameter optimization\n",
    "from hyperopt import STATUS_OK\n",
    "from hyperopt import hp, tpe, Trials, fmin\n",
    "from hyperopt.pyll.stochastic import sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<b> 1.2 Set working directory and import datasets </b>\n",
    "\n",
    "The working directory is set to the address where data is located "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data from main file\n",
    "\n",
    "application_train = pd.read_csv(\"./data/application_train.csv\")\n",
    "application_train.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the main training data, there are 307511 total samples (each row a separate loan) with 122 features of types 41 integer, 65 float and 16 object datatypes. \n",
    "\n",
    "Out of these, the feature (SK_ID_CURR) serves as the index and TARGET is the response feature to be predicted. \n",
    "\n",
    "Then, all the other data files can be imported.\n",
    "\n",
    "<i> Since, we do not have access to the labeled version of the test dataset, we will not use it, instead we will split part of the training dataset to use as the test data. </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load other data files\n",
    "\n",
    "bureau                = pd.read_csv(\"./data/bureau.csv\")\n",
    "bureau_balance        = pd.read_csv(\"./data/bureau_balance.csv\")\n",
    "credit_card_balance   = pd.read_csv(\"./data/credit_card_balance.csv\")\n",
    "installments_payments = pd.read_csv(\"./data/installments_payments.csv\")\n",
    "pos_cash_balance      = pd.read_csv(\"./data/POS_CASH_balance.csv\")\n",
    "previous_application  = pd.read_csv(\"./data/previous_application.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<b> 1.3 Check dataset structures </b>\n",
    "\n",
    "We can see the number of rows and columns in each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"application_train     :\",application_train.shape)\n",
    "print (\"bureau                :\",bureau.shape)\n",
    "print (\"bureau_balance        :\",bureau_balance.shape)\n",
    "print (\"credit_card_balance   :\",credit_card_balance.shape)\n",
    "print (\"installments_payments :\",installments_payments.shape)\n",
    "print (\"pos_cash_balance      :\",pos_cash_balance.shape)\n",
    "print (\"previous_application  :\",previous_application.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling the data\n",
    "\n",
    "# Define a sample fraction\n",
    "sample_fraction = 0.1  # e.g., 20% of the data\n",
    "\n",
    "# Sample the data\n",
    "application_train = application_train.sample(frac=sample_fraction, random_state=1)\n",
    "bureau = bureau.sample(frac=sample_fraction, random_state=1)\n",
    "bureau_balance = bureau_balance.sample(frac=sample_fraction, random_state=1)\n",
    "credit_card_balance = credit_card_balance.sample(frac=sample_fraction, random_state=1)\n",
    "installments_payments = installments_payments.sample(frac=sample_fraction, random_state=1)\n",
    "pos_cash_balance = pos_cash_balance.sample(frac=sample_fraction, random_state=1)\n",
    "previous_application = previous_application.sample(frac=sample_fraction, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 3 columns of each dataset are displayed for data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\"application_train\")\n",
    "display(application_train.head(3))\n",
    "display(\"bureau\")\n",
    "display(bureau.head(3))\n",
    "display(\"bureau_balance\")\n",
    "display(bureau_balance.head(3))\n",
    "display(\"credit_card_balance\")\n",
    "display(credit_card_balance.head(3))\n",
    "display(\"installments_payments\")\n",
    "display(installments_payments.head(3))\n",
    "display(\"pos_cash_balance\")\n",
    "display(pos_cash_balance.head(3))\n",
    "display(\"previous_application\")\n",
    "display(previous_application.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <b> 2.0 Exploratory Data Analysis </b>\n",
    "\n",
    "After data importing, we can investigate the data and answer questions like- How many features are present and how are they interlinked? What is the data quality, are there missing values? What are the different data types, are there many categorical features? Is the data imbalanced? And most importantly, are there any obvious patterns between the predictor and response features? \n",
    "\n",
    "---\n",
    "\n",
    "<b> 2.1 Checking data imbalance </b>\n",
    "\n",
    "First we can observe the distribution of the predictor variable to check if the data is balanced or not. We can see that the dataset is imbalanced with the number of samples where loan is repaid (TARGET=0) more than 10 times the number of samples where loan is defaulted (TARGET=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target data distribution\n",
    "\n",
    "application_train['TARGET'].astype(int).plot.hist(color='forestgreen').set_xlabel('Target value: 0 or 1');\n",
    "\n",
    "count = application_train['TARGET'].value_counts()\n",
    "num_repaid = count[0]\n",
    "num_default = count[1]\n",
    "\n",
    "print(\"There are {} loans repaid on time (TARGET=0) and {} loans defaulted (TARGET=1) in the dataset\".format(num_repaid, num_default))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<b> 2.2 Checking missing data </b>\n",
    "\n",
    "Before we investigate the possible correlations between the features, it is important to check if the data contains missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing data in application (main) dataset\n",
    "\n",
    "fig = plt.figure(figsize=(18,6))\n",
    "miss_data = pd.DataFrame((application_train.isnull().sum())*100/application_train.shape[0]).reset_index()\n",
    "miss_data.columns = ['index', 'percentage']\n",
    "print(miss_data.head())\n",
    "miss_data[\"type\"] = \"application data\"\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "ax = sns.pointplot(x=\"index\", y=\"percentage\", data=miss_data, color='coral')\n",
    "plt.xticks(rotation=90, fontsize=7)\n",
    "plt.title(\"Percentage of Missing values in application data\")\n",
    "plt.ylabel(\"Percentage missing\")\n",
    "plt.xlabel(\"Columns\")\n",
    "plt.ylim((0, 100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the main training dataset, we can see that there are a number of features with missing values almost 50%. The features with missing data need to be discarded or missing values filled before training any model. Similar checks should be performed on the other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing data in other data\n",
    "\n",
    "plt.figure(figsize=(15,20))\n",
    "\n",
    "plt.subplot(231)\n",
    "bureau_data = pd.DataFrame(bureau.isnull().sum()/bureau.shape[0]*100).reset_index()\n",
    "bureau_data.columns = ['index', 'percentage']\n",
    "sns.pointplot(x=\"percentage\", y=\"index\", data=bureau_data, color='deepskyblue')\n",
    "plt.xlabel(\"Percentage missing\")\n",
    "plt.xlim((0,100))\n",
    "plt.title(\"bureau\")\n",
    "\n",
    "plt.subplot(232)\n",
    "bureau_balance_data = pd.DataFrame(bureau_balance.isnull().sum()/bureau_balance.shape[0]*100).reset_index()\n",
    "bureau_balance_data.columns = ['index', 'percentage']\n",
    "sns.pointplot(x=\"percentage\", y=\"index\", data=bureau_balance_data, color='deepskyblue')\n",
    "plt.xlabel(\"Percentage missing\")\n",
    "plt.xlim((0,100))\n",
    "plt.title(\"bureau_balance\")\n",
    "\n",
    "plt.subplot(233)\n",
    "credit_card_balance_data = pd.DataFrame(credit_card_balance.isnull().sum()/credit_card_balance.shape[0]*100).reset_index()\n",
    "credit_card_balance_data.columns = ['index', 'percentage']\n",
    "sns.pointplot(x=\"percentage\", y=\"index\", data=credit_card_balance_data, color='deepskyblue')\n",
    "plt.xlabel(\"Percentage missing\")\n",
    "plt.xlim((0,100))\n",
    "plt.title(\"credit_card_balance\")\n",
    "\n",
    "plt.subplot(234)\n",
    "installments_payments_data = pd.DataFrame(installments_payments.isnull().sum()/installments_payments.shape[0]*100).reset_index()\n",
    "installments_payments_data.columns = ['index', 'percentage']\n",
    "sns.pointplot(x=\"percentage\", y=\"index\", data=installments_payments_data, color='deepskyblue')\n",
    "plt.xlabel(\"Percentage missing\")\n",
    "plt.xlim((0,100))\n",
    "plt.title(\"installments_payments\")\n",
    "\n",
    "plt.subplot(235)\n",
    "pos_cash_balance_data = pd.DataFrame(pos_cash_balance.isnull().sum()/pos_cash_balance.shape[0]*100).reset_index()\n",
    "pos_cash_balance_data.columns = ['index', 'percentage']\n",
    "sns.pointplot(x=\"percentage\", y=\"index\", data=pos_cash_balance_data, color='deepskyblue')\n",
    "plt.xlabel(\"Percentage missing\")\n",
    "plt.xlim((0,100))\n",
    "plt.title(\"pos_cash_balance\")\n",
    "\n",
    "plt.subplot(236)\n",
    "previous_application_data = pd.DataFrame(previous_application.isnull().sum()/previous_application.shape[0]*100).reset_index()\n",
    "previous_application_data.columns = ['index', 'percentage']\n",
    "sns.pointplot(x=\"percentage\", y=\"index\", data=previous_application_data, color='deepskyblue')\n",
    "plt.xlabel(\"Percentage missing\")\n",
    "plt.xlim((0,100))\n",
    "plt.title(\"previous_application\")\n",
    "\n",
    "plt.subplots_adjust(wspace = 1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plots, we can see that while some datasets do not have any missing values, other datasets have significant sample values missing. These need to be addressed before training any models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<b> 2.3 Examine feature correlations and distributions </b>\n",
    "\n",
    "After inspecting any missing data, we can now continue the exploratory data analysis to see if any response features have significant differences for cases when loans are repaid as opposed to when loans are defaulted.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First create a function to create bar plots for each feature. \n",
    "Each figure contains the following- \n",
    "\n",
    "A) The total number of categories for each response feature\n",
    "\n",
    "B) The fraction of each category with loan defaulted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar_gen(feature, df = None, orientation_horizontal = True):\n",
    "    \n",
    "    if df is None:\n",
    "        df = application_train\n",
    "    else:\n",
    "        df = df\n",
    "    \n",
    "    temp = df[feature].value_counts()\n",
    "    df1 = pd.DataFrame({feature: temp.index,'Number of contracts': temp.values})\n",
    "\n",
    "    # Calculate the percentage of target=1 per category value\n",
    "    cat_perc = df[[feature, 'TARGET']].groupby([feature],as_index=False).mean()\n",
    "    cat_perc.sort_values(by='TARGET', ascending=False, inplace=True)\n",
    "    \n",
    "    sns.set_color_codes(\"colorblind\")\n",
    "    \n",
    "    if orientation_horizontal == True:\n",
    "        plt.figure(figsize=(15,5))\n",
    "        plt.subplot(121)\n",
    "        s1 = sns.barplot(y = feature, x=\"Number of contracts\",data=df1)\n",
    "        plt.subplot(122)\n",
    "        s2 = sns.barplot(y = feature, x='TARGET', data=cat_perc)\n",
    "        plt.xlabel('Fraction of loans defaulted', fontsize=12)\n",
    "        plt.ylabel(feature, fontsize=12)\n",
    "        \n",
    "    else:\n",
    "        plt.figure(figsize=(10,18))\n",
    "        plt.subplot(211)\n",
    "        s1 = sns.barplot(x = feature, y=\"Number of contracts\",data=df1)\n",
    "        s1.set_xticklabels(s1.get_xticklabels(),rotation=90)\n",
    "        plt.subplot(212)\n",
    "        s2 = sns.barplot(x = feature, y='TARGET', data=cat_perc)\n",
    "        s2.set_xticklabels(s2.get_xticklabels(),rotation=90)\n",
    "        plt.ylabel('Fraction of loans defaulted', fontsize=12)\n",
    "        plt.xlabel(feature, fontsize=12)\n",
    "    \n",
    "    plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "    plt.subplots_adjust(wspace = 0.6)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature, type of contract, we see that there are more cash loans taken than revolving loans and they are also more defaulted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_gen('NAME_CONTRACT_TYPE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature, gender, we see that there are more loans taken by males and they are also more defaulted by them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_gen('CODE_GENDER')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature, owning car, we see that there are more loans taken by those without car and they are also more defaulted by them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_gen('FLAG_OWN_CAR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature, owning realty, we see that there are more loans taken by those with realty and they are defaulted slightly more by those without realty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_gen('FLAG_OWN_REALTY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature, family status, we see that there are more loans taken by those who are married and they are more defaulted by those who are in a civil marriage/single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_gen('NAME_FAMILY_STATUS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature, income type, we see that there are more loans taken by those who are working and they are more defaulted by those who are on maternity leave or unemployed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_gen('NAME_INCOME_TYPE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature, children count, we see that there are more loans taken by those who have fewer children and they are more defaulted by those who have more children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_gen('CNT_CHILDREN',None,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature, family member count, we see that there are more loans taken by those who have fewer family member and they are more defaulted by those who have more members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_gen('CNT_FAM_MEMBERS',None,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature, occupation type, we see that there are more loans taken categories like laborers and they are more defaulted by them too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_gen('OCCUPATION_TYPE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature, organization type, we see that there are more loans taken by categories like business entities, self-employed,and miscellaneous groups not categorized. They are more defaulted by those in certain industries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_gen('ORGANIZATION_TYPE',None,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature, education type, we see that there are more loans taken by those with secondary education and they are more defaulted by those who have less education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_gen('NAME_EDUCATION_TYPE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature, housing type, we see that there are more loans taken by those with a house/apartment and they are more defaulted by those who have rented apartment or live with parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_gen('NAME_HOUSING_TYPE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature, region not living region, we see that there are more loans are taken by those living in same region (0) and they are more defaulted by those who don't live in same region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_gen('REG_REGION_NOT_LIVE_REGION',None,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature, region not work region, we see that there are more loans are taken by those working in same region (0) and they are more defaulted by those who don't work in same region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_gen('REG_REGION_NOT_WORK_REGION',None,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature, city not living city, we see that there are more loans are taken by those living in same city (0) and they are more defaulted by those who don't live in same city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_gen('REG_CITY_NOT_LIVE_CITY',None,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature, city not working city, we see that there are more loans are taken by those working in same city (0) and they are more defaulted by those who don't work in same city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_gen('REG_CITY_NOT_WORK_CITY',None,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For non-categorical features, we can plot bivariate distributions (with TARGET) to see if there is difference when loan is repaid or defaulted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of one feature with or without paid/default shown separately\n",
    "\n",
    "def plot_distribution_gen(feature,df=None,separate_target=False):\n",
    "    if df is None:\n",
    "        df = application_train\n",
    "    else:\n",
    "        df = df\n",
    "        \n",
    "    if separate_target == False:\n",
    "        plt.figure(figsize=(10,6))\n",
    "        plt.title(\"Distribution of %s\" % feature)\n",
    "        sns.distplot(df[feature].dropna(),color='red', kde=True,bins=100)\n",
    "    else:\n",
    "        t1 = df.loc[df['TARGET'] != 0]\n",
    "        t0 = df.loc[df['TARGET'] == 0]\n",
    "        \n",
    "        plt.figure(figsize=(10,6))\n",
    "        plt.title(\"Distribution of %s\" % feature)\n",
    "        sns.set_style('whitegrid')\n",
    "#         sns.kdeplot(t1[feature], bw=0.5,label=\"Loan defaulted (TARGET=1)\")\n",
    "#         sns.kdeplot(t0[feature], bw=0.5,label=\"Loan repaid (TARGET = 0)\")\n",
    "        \n",
    "        sns.kdeplot(df.loc[df['TARGET'] == 0, feature], label = 'target == 0')\n",
    "        sns.kdeplot(df.loc[df['TARGET'] == 1, feature], label = 'target == 1')\n",
    "    \n",
    "        plt.ylabel('Density plot', fontsize=12)\n",
    "        plt.xlabel(feature, fontsize=12)\n",
    "        plt.legend(loc=\"best\", labels=['Loan repaid (TARGET=0)', 'Loan defaulted (TARGET=1)'])\n",
    "        locs, labels = plt.xticks()\n",
    "        plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below can be used to identify outliers in the data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thresh(feature):\n",
    "    \"\"\" Outliers are usually > 3 standard deviations away from the mean. \"\"\"\n",
    "    ave=np.mean(application_train[feature])\n",
    "    sdev=np.std(application_train[feature])\n",
    "    threshold=round(ave+(3*sdev),2)\n",
    "    print('Threshold for',feature,':',threshold)\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature, days employed, we see that there is an issue in the data with outliers with days employed more than 350000 days (958 years) ! This needs to be corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution_gen('DAYS_EMPLOYED',None,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature, amount income total, we see that there is an issue in the data, possibly due to outliers. This needs to be corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution_gen('AMT_INCOME_TOTAL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the thresholds for outliers for the days employed and total income features, and replace the anomalous values with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_income = get_thresh('AMT_INCOME_TOTAL')\n",
    "thresh_employment = get_thresh('DAYS_EMPLOYED')\n",
    "\n",
    "anomalous_employment = application_train[application_train['DAYS_EMPLOYED'] > 0]\n",
    "normal_employment = application_train[application_train['DAYS_EMPLOYED'] < 0]\n",
    "\n",
    "print('The non-anomalies default on %0.2f%% of loans' % (100 * normal_employment['TARGET'].mean()))\n",
    "print('The anomalies default on %0.2f%% of loans' % (100 * anomalous_employment['TARGET'].mean()))\n",
    "print('There are %d anomalous days of employment' % len(anomalous_employment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the anomalous values with nan\n",
    "\n",
    "application_train['DAYS_EMPLOYED'].mask(application_train['DAYS_EMPLOYED'] > 0, inplace=True)\n",
    "application_train['AMT_INCOME_TOTAL'].mask(application_train['AMT_INCOME_TOTAL'] > thresh_income, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function can be used to convert features which have days to years. The function can then be applied on all such features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_day_to_year(df,ls_cols,newcol):\n",
    "    df[newcol] = round(np.abs(df[ls_cols[0]]/365))\n",
    "    df.drop(columns=ls_cols,inplace=True);\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature, AGE, we can see that there are more number of loans defaulted by younger people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_day_to_year(application_train,['DAYS_BIRTH'],'AGE');\n",
    "plot_distribution_gen('AGE',None,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature, YEARS_EMPLOYED, we can see that there are more number of loans defaulted by people who are employed for fewer years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_day_to_year(application_train,['DAYS_EMPLOYED'],'YEARS_EMPLOYED');\n",
    "plot_distribution_gen('YEARS_EMPLOYED',None,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can revisit the total income feature after removing outliers and we see the minor differences in distributions when loans are repaid or defaulted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution_gen('AMT_INCOME_TOTAL',None,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature, EXT_SOURCE_1, we can see that as value of EXT_SOURCE_1 increases, the probability of loan being repaid also increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution_gen('EXT_SOURCE_1',None,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature, EXT_SOURCE_2, we can see that as value of EXT_SOURCE_2 increases, the probability of loan being repaid also increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution_gen('EXT_SOURCE_2',None,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature, EXT_SOURCE_3, we can see that as value of EXT_SOURCE_3 increases, the probability of loan being repaid also increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution_gen('EXT_SOURCE_3',None,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the features related to amount ('AMT_INCOME_TOTAL','AMT_CREDIT','AMT_ANNUITY', 'AMT_GOODS_PRICE') can be plotted along with TARGET as a heatmap to see if there are any possible correlations. We see in most cases that the repaid loans (TARGET = 0) have higher values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amt = application_train[[ 'AMT_INCOME_TOTAL','AMT_CREDIT','AMT_ANNUITY', 'AMT_GOODS_PRICE',\"TARGET\"]]\n",
    "amt = amt[(amt[\"AMT_GOODS_PRICE\"].notnull()) & (amt[\"AMT_ANNUITY\"].notnull())]\n",
    "g = sns.pairplot(amt,hue=\"TARGET\",palette=[\"b\",\"r\"])  \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature, YEARS_REGISTRATION, we can see that there are more number of loans defaulted for lower number of years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_day_to_year(application_train,['DAYS_REGISTRATION'],'YEARS_REGISTRATION');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution_gen('YEARS_REGISTRATION',None,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature, YEARS_ID_PUBLISH, we can see that there are more number of loans defaulted for lower number of years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_day_to_year(application_train,['DAYS_ID_PUBLISH'],'YEARS_ID_PUBLISH');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution_gen('YEARS_ID_PUBLISH',None,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function can create income bands for the individuals using income ranges from income total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create INCOME_BAND to group individuals per income range\n",
    "\n",
    "def create_income_band(df):\n",
    "    df.loc[(df.AMT_INCOME_TOTAL < 30000),'INCOME_BAND'] = 1\n",
    "    df.loc[(df.AMT_INCOME_TOTAL >= 30000)&(df.AMT_INCOME_TOTAL < 65000),'INCOME_BAND'] = 2\n",
    "    df.loc[(df.AMT_INCOME_TOTAL >= 65000)&(df.AMT_INCOME_TOTAL < 95000),'INCOME_BAND'] = 3\n",
    "    df.loc[(df.AMT_INCOME_TOTAL >= 95000)&(df.AMT_INCOME_TOTAL < 130000),'INCOME_BAND'] = 4\n",
    "    df.loc[(df.AMT_INCOME_TOTAL >= 130000)&(df.AMT_INCOME_TOTAL < 160000),'INCOME_BAND'] = 5\n",
    "    df.loc[(df.AMT_INCOME_TOTAL >= 160000)&(df.AMT_INCOME_TOTAL < 190000),'INCOME_BAND'] = 6\n",
    "    df.loc[(df.AMT_INCOME_TOTAL >= 190000)&(df.AMT_INCOME_TOTAL < 220000),'INCOME_BAND'] = 7\n",
    "    df.loc[(df.AMT_INCOME_TOTAL >= 220000)&(df.AMT_INCOME_TOTAL < 275000),'INCOME_BAND'] = 8\n",
    "    df.loc[(df.AMT_INCOME_TOTAL >= 275000)&(df.AMT_INCOME_TOTAL < 325000),'INCOME_BAND'] = 9\n",
    "    df.loc[(df.AMT_INCOME_TOTAL >= 325000),'INCOME_BAND'] = 10\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_income_band(application_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the income bands for those with defaulting loans vs those with repaid loans, we do not see any major differences, but the distribution of repaid loans slightly increases for higher income bands. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "sns.countplot(data=application_train[application_train.TARGET==1],x='INCOME_BAND',hue='TARGET').set_title('Income data for people defaulting loans')\n",
    "plt.subplot(122)\n",
    "sns.countplot(data=application_train[application_train.TARGET==0],x='INCOME_BAND',hue='TARGET').set_title('Income data for people repaying loans')\n",
    "application_train.drop(columns=['INCOME_BAND'],inplace=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The exploratory data analysis can be continued into other datasets. For example, in the dataset, bureau, the feature distributions can be seen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_bureau_train = application_train.merge(bureau, left_on='SK_ID_CURR', right_on='SK_ID_CURR', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The resulting dataframe `application_bureau_train` has \",application_bureau_train.shape[0],\" rows and \", \n",
    "      application_bureau_train.shape[1],\" columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature, CREDIT_ACTIVE, we see that there are more loans taken by those without active credit and they are more defaulted by those who have bad debt or sold credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_gen('CREDIT_ACTIVE',application_bureau_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature, CREDIT_CURRENCY, we see that there are most loans taken by those using currency 1 and they are more defaulted by those who use currency 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_gen('CREDIT_CURRENCY',application_bureau_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature, CREDIT_TYPE, we see that there are most loans taken by those with consumer credit/credit card and they are more defaulted by those who use loans for equipment purchase or microloans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_gen('CREDIT_TYPE',application_bureau_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Continuing the analysis for the dataset, previous_application, the feature distributions can be seen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_prev_train = application_train.merge(previous_application, left_on='SK_ID_CURR', right_on='SK_ID_CURR', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The resulting dataframe `application_prev_train` has \",application_prev_train.shape[0],\" rows and \", \n",
    "      application_prev_train.shape[1],\" columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature, NAME_CONTRACT_TYPE, we see that there are almost equal previous cash/consumer loans and they are more defaulted by those whose info is not-available (XNA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_gen('NAME_CONTRACT_TYPE_y',application_prev_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature, NAME_CASH_LOAN_PURPOSE, we see that most previous loans were for XAP/XNA and they are more defaulted by those who refuse to name the goal or for hobbies, car repairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_gen('NAME_CASH_LOAN_PURPOSE',application_prev_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature, NAME_CONTRACT_STATUS, we see that most previous loans were approved and they are more defaulted by those who were refused or canceled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_gen('NAME_CONTRACT_STATUS',application_prev_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature, NAME_PAYMENT_TYPE, we see that most previous loans were for cash through bank and they are more defaulted by XNA and non-cash from account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_gen('NAME_PAYMENT_TYPE',application_prev_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature, NAME_CLIENT_TYPE, we see that most previous loans were for repeated clients and they are more defaulted by those without information or new clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_gen('NAME_CLIENT_TYPE',application_prev_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <b> 3.0 Feature Engineering </b>\n",
    "\n",
    "After exploring the data distributions, we can now conduct feature engineering to prepare the features before building the classifier models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<b> 3.1 Save/Load updated datasets </b> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a checkpoint to save the updated data files if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the csv files as a checkpoint if necessary (high memory and time intensive)\n",
    "\n",
    "save_files = False\n",
    "\n",
    "if save_files == True:\n",
    "    application_train.to_csv('application_train_updated.csv', index = False)\n",
    "    bureau.to_csv('bureau_updated.csv', index = False)\n",
    "    bureau_balance.to_csv('bureau_balance_updated.csv', index = False)\n",
    "    credit_card_balance.to_csv('credit_card_balance.csv', index = False)\n",
    "    installments_payments.to_csv('installments_payments_updated.csv', index = False)\n",
    "    pos_cash_balance.to_csv('pos_cash_balance_updated.csv', index = False)\n",
    "    previous_application.to_csv('previous_application_updated.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is followed by the option of loading the previously saved updated datafiles, in case we are restarting the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csv files from checkpoint if necessary\n",
    "# Otherwise, copy to new df and clean up memory \n",
    "\n",
    "load_files = False  # Existing variables in memory\n",
    "\n",
    "if load_files == False:\n",
    "    application_train_new = application_train.copy()\n",
    "    bureau_new = bureau.copy()\n",
    "    bureau_balance_new = bureau_balance.copy()\n",
    "    credit_card_balance_new = credit_card_balance.copy()\n",
    "    installments_payments_new = installments_payments.copy()\n",
    "    pos_cash_balance_new = pos_cash_balance.copy()\n",
    "    previous_application_new = previous_application.copy()\n",
    "    \n",
    "    gc.enable()\n",
    "    del application_train, bureau, bureau_balance, credit_card_balance, installments_payments, pos_cash_balance, previous_application\n",
    "    gc.collect()\n",
    "    \n",
    "else:\n",
    "    application_train_new = pd.read_csv(\"application_train_updated.csv\")\n",
    "    bureau_new = pd.read_csv(\"bureau_updated.csv\")            \n",
    "    bureau_balance_new  = pd.read_csv(\"bureau_balance_updated.csv\")      \n",
    "    credit_card_balance_new  = pd.read_csv(\"credit_card_balance.csv\")\n",
    "    installments_payments_new = pd.read_csv(\"installments_payments_updated.csv\")\n",
    "    pos_cash_balance_new = pd.read_csv(\"pos_cash_balance_updated.csv\")     \n",
    "    previous_application_new  = pd.read_csv(\"previous_application_updated.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<b> 3.2 Basic Feature Engineering (Replacing outliers, Imputing, One-hot encoding, Rescaling data) </b> \n",
    "\n",
    "First, a function is created and run to replace the day outliers previously seen across the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_day_outliers(df):\n",
    "    \"\"\"Replace 365243 with np.nan in any columns with DAYS\"\"\"\n",
    "    for col in df.columns:\n",
    "        if \"DAYS\" in col:\n",
    "            df[col] = df[col].replace({365243: np.nan})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all the day outliers\n",
    "application_train_new = replace_day_outliers(application_train_new)\n",
    "bureau_new = replace_day_outliers(bureau_new )              \n",
    "bureau_balance_new  = replace_day_outliers(bureau_balance_new)      \n",
    "credit_card_balance_new  = replace_day_outliers(credit_card_balance_new) \n",
    "installments_payments_new = replace_day_outliers(installments_payments_new )\n",
    "pos_cash_balance_new = replace_day_outliers(pos_cash_balance_new )   \n",
    "previous_application_new  = replace_day_outliers(previous_application_new )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function to remove columns which have missing values greater than 60%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for removing columns with missing values more than 60%\n",
    "\n",
    "def remove_missing_col(df):\n",
    "    miss_data = pd.DataFrame((df.isnull().sum())*100/df.shape[0])\n",
    "    miss_data_col=miss_data[miss_data[0]>60].index\n",
    "    data_new  = df[[i for i in df.columns if i not in miss_data_col]]\n",
    "    return data_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom imputer function for both numerical and categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom imputer for numerical and categorical variables (https://stackoverflow.com/questions/25239958/impute-categorical-missing-values-in-scikit-learn)\n",
    "\n",
    "class DataFrameImputer(TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Impute missing values.\n",
    "\n",
    "        Columns of dtype object are imputed with the most frequent value \n",
    "        in column.\n",
    "\n",
    "        Columns of other types are imputed with median of column.\n",
    "\n",
    "        \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0]\n",
    "            if X[c].dtype == np.dtype('O') else X[c].median() for c in X],\n",
    "            index=X.columns)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding of categorical variables in the main training dataset is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding of categorical variables\n",
    "base_case_train = pd.get_dummies(application_train_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After one-hot encoding, the index variable (SK_ID_CURR) is temporarily removed and the main dataset has values imputed to replace missing samples, and the features are rescaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the SK_ID from the training data\n",
    "skid_temp = application_train_new['SK_ID_CURR']\n",
    "train = base_case_train.drop(columns = ['SK_ID_CURR'])\n",
    "    \n",
    "# Feature names\n",
    "features = list(train.columns)\n",
    "\n",
    "# Scale each feature to 0-1\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "\n",
    "# Median imputation of missing values\n",
    "train = DataFrameImputer().fit_transform(train)\n",
    "\n",
    "## Repeat with the scaler\n",
    "scaler.fit(train)\n",
    "train = scaler.transform(train)\n",
    "base_case_train = pd.DataFrame(data=train, columns=features)\n",
    "\n",
    "print('Data shape: ', base_case_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can get the modified dataset for visual inspection. Looks ok!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_case_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index variable is reattached to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_case_train['SK_ID_CURR'] = skid_temp\n",
    "\n",
    "print('Data shape: ', base_case_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<b> 3.3 Base Case- Logistic Regression </b>\n",
    "\n",
    "Before we do advanced feature engineering and connect the other data files, we can run a logistic regression to see how a simple classifier model behaves as a sanity check. For this purpose, the dataset is temporarily split into training/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test split -- temporary\n",
    "# Logistic regression - sanity check \n",
    "\n",
    "#splitting application_train_newdf into train and test\n",
    "train,test = train_test_split(base_case_train,test_size=.25,random_state = 123)\n",
    "\n",
    "#separating dependent and independent variables\n",
    "train_X = train[[i for i in train.columns if i not in ['SK_ID_CURR'] + [ 'TARGET']]]\n",
    "train_Y = train[[\"TARGET\"]]\n",
    "\n",
    "test_X  = test[[i for i in test.columns if i not in ['SK_ID_CURR'] + [ 'TARGET']]]\n",
    "test_Y  = test[[\"TARGET\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a function for the classifier to train on data, predict using test data, and visualize the metrics and feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model function\n",
    "\n",
    "def model_base(algorithm,dtrain_X,dtrain_Y,dtest_X,dtest_Y,cols=None):\n",
    "    \n",
    "    # Extract feature names\n",
    "    feature_names = list(dtrain_X.columns)\n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "    \n",
    "    algorithm.fit(dtrain_X[cols],dtrain_Y)\n",
    "    predictions = algorithm.predict(dtest_X[cols])\n",
    "    prediction_probabilities = algorithm.predict_proba(dtest_X[cols])[:,1]\n",
    "    \n",
    "    # Record the feature importances\n",
    "    feature_importance_values= algorithm.coef_[0]\n",
    "    feature_importance_values = np.absolute(feature_importance_values)\n",
    "#     # Make the feature importance dataframe\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
    "    \n",
    "    print (algorithm)\n",
    "    print (\"Accuracy score : \", accuracy_score(predictions,dtest_Y))\n",
    "    print (\"classification report :\\n\",classification_report(predictions,dtest_Y))\n",
    "    fpr , tpr , thresholds   = roc_curve(dtest_Y,prediction_probabilities)\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,8))\n",
    "    ax  = fig.add_subplot(111)\n",
    "    ax.plot(fpr,tpr,label   = [\"Area under curve : \",auc(fpr,tpr)],linewidth=2,linestyle=\"dotted\")\n",
    "    ax.plot([0,1],[0,1],linewidth=2,linestyle=\"dashed\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"ROC-CURVE & AREA UNDER CURVE\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    \n",
    "    return feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On running the base case - logistic regression model, we see that it does a good job of predicting the cases when loan is repaid (f1-score = 0.96) but does not perform well to predict when loans are defaulted (f1-score = 0.02). \n",
    "\n",
    "However, since this is an imbalanced dataset, the accuracy score is still high at 0.9184. Hence, it is preferred to use test AUC ROC which does not depend on data imbalance and has a value of 0.7454 for the base case.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression()\n",
    "feature_importances_logit_base = model_base(logit,train_X,train_Y,test_X,test_Y,train_X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Receiver Operating Characteristic Area under curve (ROC AUC) is a metric for which can be suitably applied for imbalanced datasets since it does not generate 0 or 1 predictions, but rather a probability between 0 and 1. It measures the false positive rate (x-axis) vs true positive rate (y-axis). The area under curve (AUC) is the area between a model's ROC curve and the diagonal indicating a model with naive random guessing. If the ROC curve is more to the left/top of the diagonal, is indicates better performance with a higher ROC AUC.\n",
    "\n",
    "---\n",
    "\n",
    "After training a base case, we can return to feature engineering.\n",
    "\n",
    "First we apply our limited domain knowlege to create few more variables, specifically ratios accounting for the credit income %, annuity income %, credit term, and fraction of years employed  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Domain knowledge\n",
    "\n",
    "application_train_new['CREDIT_INCOME_PERCENT'] = application_train_new['AMT_CREDIT'] / application_train_new['AMT_INCOME_TOTAL']\n",
    "application_train_new['ANNUITY_INCOME_PERCENT'] = application_train_new['AMT_ANNUITY'] / application_train_new['AMT_INCOME_TOTAL']\n",
    "application_train_new['CREDIT_TERM'] = application_train_new['AMT_ANNUITY'] / application_train_new['AMT_CREDIT']\n",
    "application_train_new['YEARS_EMPLOYED_PERCENT'] = application_train_new['YEARS_EMPLOYED'] / application_train_new['AGE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<b> 3.4 Automated Feature Engineering using Featuretools </b>\n",
    "\n",
    "To improve the results over the base case and to fully utilized the different datasets provided, we need to use feature engineering and relational database techniques to connect the various datasets. This will be done using extract, transform, load (ETL) techniques for automated feature engineering using [Featuretools](https://www.featuretools.com/)\n",
    "\n",
    "We create a function to identify boolean variables in main training data and run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import featuretools for automated feature engineering\n",
    "import featuretools as ft\n",
    "from featuretools import selection\n",
    "\n",
    "# Iterate through the columns and record the Boolean columns\n",
    "\n",
    "def bool_type(df):\n",
    "\n",
    "    col_type = {}\n",
    "\n",
    "    for col in df:\n",
    "        # If column is a number with only two values, encode it as a Boolean\n",
    "        if (df[col].dtype != 'object') and (len(df[col].unique()) <= 2):\n",
    "            col_type[col] = ft.variable_types.Boolean\n",
    "\n",
    "    print('Number of boolean variables: ', len(col_type))\n",
    "    return col_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_col_type = bool_type(application_train_new)\n",
    "\n",
    "train_col_type['REGION_RATING_CLIENT'] = ft.variable_types.Ordinal\n",
    "train_col_type['REGION_RATING_CLIENT_W_CITY'] = ft.variable_types.Ordinal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start connecting the datasets using Featuretools by first creating an entity and then adding the datasets while connected by a common index (SK_ID_CURR). For other datasets, indexes are created as necessary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity set with id applications\n",
    "es = ft.EntitySet(id = 'clients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entities with a unique index\n",
    "es = es.entity_from_dataframe(entity_id = 'app', dataframe = application_train_new, index = 'SK_ID_CURR', variable_types = train_col_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = es.entity_from_dataframe(entity_id = 'bureau', dataframe = bureau_new, index = 'SK_ID_BUREAU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = es.entity_from_dataframe(entity_id = 'previous', dataframe = previous_application_new, index = 'SK_ID_PREV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entities that do not have a unique index\n",
    "es = es.entity_from_dataframe(entity_id = 'bureau_balance', dataframe = bureau_balance_new, \n",
    "                              make_index = True, index = 'bureaubalance_index') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = es.entity_from_dataframe(entity_id = 'cash', dataframe = pos_cash_balance_new, \n",
    "                              make_index = True, index = 'cash_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = es.entity_from_dataframe(entity_id = 'installments', dataframe = installments_payments_new,\n",
    "                              make_index = True, index = 'installments_index')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = es.entity_from_dataframe(entity_id = 'credit', dataframe = credit_card_balance_new,\n",
    "                              make_index = True, index = 'credit_index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationships between the different datasets are defined as per the below flowchart while ensuring that the parent-child relationship does not have any cross-connections \n",
    "\n",
    "\n",
    "![image](https://storage.googleapis.com/kaggle-media/competitions/home-credit/home_credit.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship between app and bureau\n",
    "r_app_bureau = ft.Relationship(es['app']['SK_ID_CURR'], es['bureau']['SK_ID_CURR'])\n",
    "\n",
    "# Relationship between bureau and bureau balance\n",
    "r_bureau_balance = ft.Relationship(es['bureau']['SK_ID_BUREAU'], es['bureau_balance']['SK_ID_BUREAU'])\n",
    "\n",
    "# Relationship between current app and previous apps\n",
    "r_app_previous = ft.Relationship(es['app']['SK_ID_CURR'], es['previous']['SK_ID_CURR'])\n",
    "\n",
    "# Relationships between previous apps and cash, installments, and credit\n",
    "r_previous_cash = ft.Relationship(es['previous']['SK_ID_PREV'], es['cash']['SK_ID_PREV'])\n",
    "r_previous_installments = ft.Relationship(es['previous']['SK_ID_PREV'], es['installments']['SK_ID_PREV'])\n",
    "r_previous_credit = ft.Relationship(es['previous']['SK_ID_PREV'], es['credit']['SK_ID_PREV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The individual relationships are pooled together and the entity set is shown for inspection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in the defined relationships\n",
    "es = es.add_relationships([r_app_bureau, r_bureau_balance, r_app_previous,\n",
    "                           r_previous_cash, r_previous_installments, r_previous_credit])\n",
    "# Print out the EntitySet\n",
    "es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Feature primitives](https://docs.featuretools.com/automated_feature_engineering/primitives.html) are operations conducted on tables to create a feature. The two common types are aggregation and transformation.\n",
    "\n",
    "An Aggregation groups together values from child dataset for each parent and then calculates a feature such as mean, min, max, or standard deviation. A transformation can be applied to one or more columns in a single table such as the difference between two columns or the absolute value of one. Some common primitives in featuretools are shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the primitives in a dataframe\n",
    "primitives = ft.list_primitives()\n",
    "pd.options.display.max_colwidth = 100\n",
    "primitives[primitives['type'] == 'aggregation'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primitives[primitives['type'] == 'transform'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few default primitives are selected for Deep Feature Synthesis (DFS), which is the process featuretools uses to make new features. The depth is the number of primitives that are stacked together to form new features. Initially, we only see the number of features by using features_only = TRUE which creates the feature combinations but does not actually do the calculations for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default primitives from featuretools\n",
    "default_agg_primitives =  ['sum', 'count', 'min', 'max', 'mean', 'mode']\n",
    "default_trans_primitives =  ['diff', 'cum_sum', 'cum_mean', 'percentile']\n",
    "\n",
    "# DFS with specified primitives\n",
    "feature_names = ft.dfs(entityset = es, target_entity = 'app',\n",
    "                       trans_primitives = default_trans_primitives,\n",
    "                       agg_primitives=default_agg_primitives, \n",
    "                       max_depth = 2, features_only=True)\n",
    "\n",
    "print('%d Total Features' % len(feature_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that 3534 features are expected to be created from the DFS process. Moving forward, we can create the features and display the created features for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DFS with default primitives\n",
    "\n",
    "feature_matrix, feature_names = ft.dfs(entityset = es, target_entity = 'app',\n",
    "                                       trans_primitives = default_trans_primitives,\n",
    "                                       agg_primitives=default_agg_primitives, \n",
    "                                        max_depth = 2, features_only=False, verbose = True)\n",
    "\n",
    "pd.options.display.max_columns = 1500\n",
    "feature_matrix.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above table, we see that there are a number of different features created, including a number of categorical ones. Moreover, many features have high missing values and other may have a high degree of correlations with each other. Such issues need to be accounted for before training models.\n",
    "\n",
    "---\n",
    "\n",
    "<b> 3.5 Return to manual feature engineering </b>\n",
    "\n",
    "First, we remove the features having more than 60% missing values from the newly constructed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing columns with missing values more than 60% in new df\n",
    "\n",
    "feature_matrix = remove_missing_col(feature_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we separate the reponse feature (TARGET) temporarily before conducting other operations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate TARGET feature temporarily \n",
    "target_temp = feature_matrix['TARGET']\n",
    "feature_matrix = feature_matrix.drop(columns = ['TARGET'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical variables are one-hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding of categorical variables\n",
    "feature_matrix = pd.get_dummies(feature_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features with low information (only 1 unique value) are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features with only one unique value\n",
    "feature_matrix2 = selection.remove_low_information_features(feature_matrix)\n",
    "\n",
    "print('Removed %d features' % (feature_matrix.shape[1]- feature_matrix2.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the one-hot encoding has increased the columns even if the other operations reduced them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are almost 3000 features (columns), it is important to account for correlated features which do not add unique information but contribute to the curse of dimensionality. Hence, we can create a correlation matrix to find the correlated features and remove one feature from each correlation pair which is larger than a given threshold (0.8).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix=feature_matrix2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold for removing correlated variables (without TARGET)\n",
    "threshold = 0.8\n",
    "\n",
    "# Calculate absolute value correlation matrix\n",
    "corr_matrix = feature_matrix.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool_))\n",
    "\n",
    "# Select columns with correlations above threshold\n",
    "collinear_features = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "\n",
    "print('There are %d features to remove.' % (len(collinear_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove collinear features\n",
    "feature_matrix = feature_matrix[[col for col in feature_matrix.columns if col not in collinear_features]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix['TARGET'] = target_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below shows the dataset at this stage, and we can see that there are still a number of NaN values and features that require scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, imputation of missing values is done, followed by scaling to make the dataset ready for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature names\n",
    "features = list(feature_matrix.columns)\n",
    "\n",
    "# Scale each feature to 0-1\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "\n",
    "# Median imputation of missing values\n",
    "train = DataFrameImputer().fit_transform(feature_matrix)\n",
    "train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "train.fillna(feature_matrix.median(), inplace=True)\n",
    "\n",
    "## Repeat with the scaler\n",
    "scaler.fit(train)\n",
    "train = scaler.transform(train)\n",
    "feature_matrix = pd.DataFrame(data=train, columns=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is finally ready for training. However, before that we can save it as a checkpoint or load it from this checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataset\n",
    "\n",
    "feature_matrix.to_csv('feature_matrix.csv')\n",
    "\n",
    "#load dataset\n",
    "\n",
    "#feature_matrix= pd.read_csv(\"feature_matrix.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### <b> 4.0 Classifier Models: Training, Prediction and Comparison </b> \n",
    "\n",
    "Now that we have created the final dataset, we can start building the classifier models and compare then.\n",
    "\n",
    "---\n",
    "\n",
    "<b> 4.1 Data split and imbalance correction </b>\n",
    "\n",
    "First, we split the data into training/testing set in the ratio 75:25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting application_train_newdf into train and test\n",
    "train,test = train_test_split(feature_matrix,test_size=.25,random_state = 123)\n",
    "\n",
    "#separating dependent and independent variables (no under/over sampling)\n",
    "train_X = train[[i for i in train.columns if i not in ['SK_ID_CURR'] + [ 'TARGET']]]\n",
    "train_Y = train[[\"TARGET\"]]\n",
    "\n",
    "test_X  = test[[i for i in test.columns if i not in ['SK_ID_CURR'] + [ 'TARGET']]]\n",
    "test_Y  = test[[\"TARGET\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dataset is imbalanced in the favor of majority samples being where the loans are repaid (TARGET=1) in a ratio more than 10:1, we can resample the data, by undersampling the majority class to make the data more balanced. So that we do not lose too much valuable data, the number of majority class samples is kept as twice the number of minority class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Down-sample Majority Class\n",
    "\n",
    "count = train['TARGET'].value_counts()\n",
    "num_majority = count[0]\n",
    "num_minority = count[1]\n",
    "\n",
    "#Number of undersampled majority class 2 x minority class\n",
    "num_undersample_majority = 2 * num_minority\n",
    "\n",
    "#separating majority and minority classes\n",
    "df_majority = train[train[\"TARGET\"] == 0]\n",
    "df_minority = train[train[\"TARGET\"] == 1]\n",
    "\n",
    "df_majority_undersampled = resample(df_majority, replace=False,\n",
    "                                   n_samples=num_undersample_majority,\n",
    "                                   random_state=123)\n",
    "\n",
    "df_undersampled = pd.concat([df_minority,df_majority_undersampled],axis=0)\n",
    "\n",
    "#splitting dependent and independent variables\n",
    "\n",
    "df_undersampled_X = df_undersampled[[i for i in df_undersampled.columns if i not in ['SK_ID_CURR'] + [ 'TARGET']]]\n",
    "df_undersampled_Y = df_undersampled[[\"TARGET\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<b> 4.2 Training classifier models </b>\n",
    "\n",
    "First, we create a function for the classifier to train on data, predict using test data, and visualize the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model function\n",
    "\n",
    "def model_run(algorithm,dtrain_X,dtrain_Y,dtest_X,dtest_Y,cols=None):\n",
    "\n",
    "    algorithm.fit(dtrain_X[cols],dtrain_Y)\n",
    "    predictions = algorithm.predict(dtest_X[cols])\n",
    "    prediction_probabilities = algorithm.predict_proba(dtest_X[cols])[:,1]\n",
    "    \n",
    "    accuracy = accuracy_score(dtest_Y,predictions)\n",
    "    classify_metrics = classification_report(dtest_Y,predictions)\n",
    "    f1=f1_score(dtest_Y,predictions)\n",
    "    \n",
    "    fpr , tpr , thresholds   = roc_curve(dtest_Y,prediction_probabilities)\n",
    "    auc_score = auc(fpr,tpr)\n",
    "    \n",
    "    print (algorithm)\n",
    "    print (\"Accuracy score : \", accuracy)\n",
    "    print (\"F1 score : \", f1)\n",
    "    print (\"AUC : \", auc_score)\n",
    "    print (\"classification report :\\n\", classify_metrics)\n",
    "    \n",
    "    return accuracy, classify_metrics, fpr , tpr, auc_score, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model 1: Logistic Regression***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression()\n",
    "accuracy_logit, classify_metrics_logit, fpr_logit , tpr_logit, auc_score_logit, f1_logit = model_run(logit,df_undersampled_X ,df_undersampled_Y ,test_X,test_Y,train_X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model 2: Random Forest Classifier***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "accuracy_rfc, classify_metrics_rfc, fpr_rfc , tpr_rfc, auc_score_rfc, f1_rfc = model_run(rfc,df_undersampled_X ,df_undersampled_Y ,test_X,test_Y,train_X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model 3: Decision Tree Classifier***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier()\n",
    "accuracy_dtc, classify_metrics_dtc, fpr_dtc , tpr_dtc, auc_score_dtc, f1_dtc = model_run(dtc,df_undersampled_X ,df_undersampled_Y ,test_X,test_Y,train_X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model 4: Gaussian Naive Bayes Classifier***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "accuracy_gnb, classify_metrics_gnb, fpr_gnb , tpr_gnb, auc_score_gnb, f1_gnb = model_run(gnb,df_undersampled_X ,df_undersampled_Y ,test_X,test_Y,train_X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model 5: XGBoost Classifier***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier()\n",
    "accuracy_xgb, classify_metrics_xgb, fpr_xgb, tpr_xgb, auc_score_xgb, f1_xgb = model_run(xgb,df_undersampled_X ,df_undersampled_Y ,test_X,test_Y,train_X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model 6: Gradient Boosting Classifier***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier()\n",
    "accuracy_gbc, classify_metrics_gbc, fpr_gbc, tpr_gbc, auc_score_gbc, f1_gbc = model_run(gbc,df_undersampled_X ,df_undersampled_Y ,test_X,test_Y,train_X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model 7: LightGBM Classifier***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n",
    "                                   class_weight = 'balanced', learning_rate = 0.05, \n",
    "                                   reg_alpha = 0.1, reg_lambda = 0.1, \n",
    "                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n",
    "\n",
    "#Split into training and evaluation sets\n",
    "x_train, x_eval, y_train, y_eval = train_test_split(df_undersampled_X, df_undersampled_Y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Remove possible JSON characters\n",
    "x_train = x_train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "x_eval = x_eval.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "y_train = y_train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "y_eval = y_eval.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, eval_set = [(x_eval, y_eval)], eval_names =['valid'], eval_metric ='auc', callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=200),\n",
    "    ]);\n",
    "\n",
    "# prediction\n",
    "predictions = model.predict(test_X);\n",
    "prediction_probabilities = model.predict_proba(test_X)[:,1];\n",
    "\n",
    "# metrics\n",
    "accuracy_lgbc = accuracy_score(test_Y,predictions)\n",
    "classify_metrics_lgbc = classification_report(test_Y,predictions)  \n",
    "fpr_lgbc , tpr_lgbc , thresholds   = roc_curve(test_Y,prediction_probabilities)\n",
    "auc_score_lgbc = auc(fpr_lgbc,tpr_lgbc)\n",
    "f1_lgbc=f1_score(test_Y,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Accuracy score : \", accuracy_lgbc)\n",
    "print (\"F1 score : \", f1_lgbc)\n",
    "print (\"AUC : \", auc_score_lgbc)\n",
    "print (\"classification report :\\n\", classify_metrics_lgbc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 4.3 Classifier model comparison </b>\n",
    "\n",
    "After training the different classifier models, we can compare their performance on the test data using metrics like accuracy, F1-score and ROC AUC with functions given below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine results together and compare stats for all\n",
    "\n",
    "classifier_names =['Logistic Regression','Random Forest','Decision Tree','Gaussian Naive Bayes','XGBoost','Gradient Boosting','LightGBM']\n",
    "accuracy_scores = [accuracy_logit, accuracy_rfc, accuracy_dtc, accuracy_gnb, accuracy_xgb, accuracy_gbc, accuracy_lgbc] \n",
    "f1_scores = [f1_logit, f1_rfc, f1_dtc, f1_gnb, f1_xgb, f1_gbc, f1_lgbc]\n",
    "auc_scores = [auc_score_logit, auc_score_rfc, auc_score_dtc, auc_score_gnb, auc_score_xgb, auc_score_gbc, auc_score_lgbc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(classifier_names))\n",
    "print(len(accuracy_scores))\n",
    "print(len(f1_scores))\n",
    "print(len(auc_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_color_codes(\"colorblind\")\n",
    "    \n",
    "plt.figure(figsize=(15,18))\n",
    "plt.subplot(311)\n",
    "plt.title('Classifier Comparison Scores: Accuracy, F1, ROC AUC')\n",
    "s1 = sns.barplot(x = classifier_names, y=accuracy_scores)\n",
    "s1.set_xticklabels(s1.get_xticklabels(),rotation=90)\n",
    "#s1.ylabel('accuracy scores', fontsize=12)\n",
    "plt.ylabel('Accuracy scores', fontsize=12)\n",
    "plt.subplot(312)\n",
    "s2 = sns.barplot(x = classifier_names, y=f1_scores)\n",
    "s2.set_xticklabels(s2.get_xticklabels(),rotation=90)\n",
    "#s2.ylabel('F1 scores', fontsize=12)\n",
    "plt.ylabel('F1 scores', fontsize=12)\n",
    "plt.subplot(313)\n",
    "s3 = sns.barplot(x = classifier_names, y=auc_scores)\n",
    "s3.set_xticklabels(s3.get_xticklabels(),rotation=90)\n",
    "plt.ylabel('AUC ROC scores', fontsize=12)\n",
    "   \n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.subplots_adjust(hspace = 0.5)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "ax  = fig.add_subplot(111)\n",
    "    \n",
    "ax.plot(fpr_logit,tpr_logit,label = [classifier_names[0], \"AUC ROC :\", round(auc_score_logit,3)],linewidth=2,linestyle=\"dotted\")\n",
    "ax.plot(fpr_rfc,tpr_rfc,label = [classifier_names[1], \"AUC ROC :\", round(auc_score_rfc,3)],linewidth=2,linestyle=\"dotted\")\n",
    "ax.plot(fpr_dtc,tpr_dtc,label = [classifier_names[2], \"AUC ROC :\", round(auc_score_dtc,3)],linewidth=2,linestyle=\"dotted\")\n",
    "ax.plot(fpr_gnb,tpr_gnb,label = [classifier_names[3], \"AUC ROC :\", round(auc_score_gnb,3)],linewidth=2,linestyle=\"dotted\")\n",
    "ax.plot(fpr_xgb,tpr_xgb,label = [classifier_names[4], \"AUC ROC :\", round(auc_score_xgb,3)],linewidth=2,linestyle=\"dotted\")\n",
    "ax.plot(fpr_gbc,tpr_gbc,label = [classifier_names[5], \"AUC ROC :\", round(auc_score_gbc,3)],linewidth=2,linestyle=\"dotted\")\n",
    "ax.plot(fpr_lgbc,tpr_lgbc,label = [classifier_names[6], \"AUC ROC :\", round(auc_score_lgbc,3)],linewidth=2,linestyle=\"dotted\")\n",
    "\n",
    "ax.plot([0,1],[0,1],linewidth=2,linestyle=\"dashed\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"ROC-CURVE & AREA UNDER CURVE\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "<b> 4.4 K-Fold Cross validation to improve model </b>\n",
    "\n",
    "After choosing the classifier, we can use K-fold cross validation to train multiple iterations of the model by subsampling the validation data from the training dataset during different folds. This will help us choose parameters that correspond to the best performance without creating a separate validation dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_classify(model_in, dtrain_X, dtrain_Y, dtest_X, dtest_Y, n_folds = 5):\n",
    "    \n",
    "    \"\"\"Train and test a model using cross validation. \n",
    "    \"\"\"\n",
    "       \n",
    "    # Extract the ids\n",
    "    train_ids = dtrain_X.index\n",
    "    test_ids = dtest_X.index\n",
    "    \n",
    "    print('Training Data Shape: ', dtrain_X.shape)\n",
    "    print('Testing Data Shape: ', dtest_X.shape)\n",
    "    \n",
    "    # Extract feature names\n",
    "    feature_names = list(dtrain_X.columns)\n",
    "    \n",
    "    # Convert to np arrays\n",
    "    features = np.array(dtrain_X)\n",
    "    test_features = np.array(dtest_X)\n",
    "    \n",
    "    # Create the kfold object\n",
    "    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n",
    "    \n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "    \n",
    "    # Empty array for test predictions\n",
    "    predictions = np.zeros(test_features.shape[0])\n",
    "    predictions_class = np.zeros(test_features.shape[0])\n",
    "    \n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "    \n",
    "    # Lists for recording validation and training scores\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "    \n",
    "    # print model\n",
    "    \n",
    "    print (model_in)\n",
    "    \n",
    "    # Iterate through each fold\n",
    "    for train_indices, valid_indices in k_fold.split(features):\n",
    "        \n",
    "        \n",
    "        # Training data for the fold\n",
    "        train_features, train_labels = features[train_indices], dtrain_Y.iloc[train_indices]\n",
    "        # Validation data for the fold\n",
    "        valid_features, valid_labels = features[valid_indices], dtrain_Y.iloc[valid_indices]\n",
    "        \n",
    "        model = model_in\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(train_features, train_labels, eval_metric = 'auc',\n",
    "                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
    "                  eval_names = ['valid', 'train'],\n",
    "                  callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=200),  ])\n",
    "        \n",
    "        # Record the best iteration\n",
    "        best_iteration = model.best_iteration_\n",
    "        \n",
    "        # Record the feature importances\n",
    "        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n",
    "        \n",
    "        # Make predictions    \n",
    "        predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n",
    "             \n",
    "        \n",
    "        # Record the out of fold predictions\n",
    "        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n",
    "        \n",
    "        # Record the best score\n",
    "        valid_score = model.best_score_['valid']['auc']\n",
    "        train_score = model.best_score_['train']['auc']\n",
    "        \n",
    "        valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.enable()\n",
    "        del train_features, valid_features\n",
    "        gc.collect()\n",
    "        \n",
    "    # Make the predictions dataframe\n",
    "    test_predictions = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': predictions})\n",
    "    \n",
    "    # Make the feature importance dataframe\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
    "    \n",
    "    # Overall validation score\n",
    "    valid_auc = roc_auc_score(dtrain_Y, out_of_fold)\n",
    "    \n",
    "    # Add the overall scores to the metrics\n",
    "    valid_scores.append(valid_auc)\n",
    "    train_scores.append(np.mean(train_scores))\n",
    "    \n",
    "    # Needed for creating dataframe of validation scores\n",
    "    fold_names = list(range(n_folds))\n",
    "    fold_names.append('overall')\n",
    "    \n",
    "    \n",
    "    # Dataframe of validation scores\n",
    "    fold_scores = pd.DataFrame({'fold': fold_names,\n",
    "                            'train': train_scores,\n",
    "                            'valid': valid_scores}) \n",
    "    \n",
    "    predictions_class = model.predict(test_features, num_iteration = best_iteration)\n",
    "\n",
    "    \n",
    "    fpr , tpr , thresholds   = roc_curve(dtest_Y,predictions)\n",
    "    auc_roc = auc(fpr,tpr)\n",
    "    f1_sc = f1_score(dtest_Y,predictions_class)\n",
    "    accuracy = accuracy_score(dtest_Y,predictions_class)\n",
    "    classify_metrics = classification_report(dtest_Y,predictions_class)\n",
    "       \n",
    "    print (model)\n",
    "    \n",
    "    return feature_importances, classify_metrics, fold_scores, accuracy, f1_sc, auc_roc, fpr, tpr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n",
    "                                   class_weight = 'balanced', learning_rate = 0.05, \n",
    "                                   reg_alpha = 0.1, reg_lambda = 0.1, \n",
    "                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n",
    "\n",
    "feature_importances, classify_metrics, fold_scores, accuracy, f1_sc, auc_score, fpr, tpr  = model_classify(model, df_undersampled_X, df_undersampled_Y, test_X, test_Y)\n",
    "\n",
    "print('Baseline metrics')\n",
    "print(fold_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and validation scores are seen to be in the same range, showing that the model is not overfitting the data. The best iteration can then be used for prediction on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Accuracy score : \", accuracy)\n",
    "print (\"F1 score : \", f1_sc)\n",
    "print (\"AUC : \", auc_score)\n",
    "print (\"classification report :\\n\", classify_metrics)\n",
    "    \n",
    "fig = plt.figure(figsize=(10,8))\n",
    "ax  = fig.add_subplot(111)\n",
    "ax.plot(fpr,tpr,label   = [\"Area under curve : \",auc_score],linewidth=2,linestyle=\"dotted\")\n",
    "ax.plot([0,1],[0,1],linewidth=2,linestyle=\"dashed\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"ROC-CURVE & AREA UNDER CURVE\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(df):\n",
    "    \"\"\"\n",
    "    Plot importances returned by a model. This can work with any measure of\n",
    "    feature importance provided that higher importance is better. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort features according to importance\n",
    "    df = df.sort_values('importance', ascending = False).reset_index()\n",
    "    \n",
    "    # Normalize the feature importances to add up to one\n",
    "    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n",
    "\n",
    "    # Make a horizontal bar chart of feature importances\n",
    "    plt.figure(figsize = (10, 6))\n",
    "    ax = plt.subplot()\n",
    "    \n",
    "    # Need to reverse the index to plot most important on top\n",
    "    ax.barh(list(reversed(list(df.index[:15]))), \n",
    "            df['importance_normalized'].head(15), \n",
    "            align = 'center', edgecolor = 'k')\n",
    "    \n",
    "    # Set the yticks and labels\n",
    "    ax.set_yticks(list(reversed(list(df.index[:15]))))\n",
    "    ax.set_yticklabels(df['feature'].head(15))\n",
    "    \n",
    "    # Plot labeling\n",
    "    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n",
    "    plt.show()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, examining the top 15 features of the LightGBM model, we can see that it contains a number of features that were created from domain knowledge and through the automated feature engineering (DFS) process. Among the original features, a number of them are those that had a high loan default rate during exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the feature importances for the new features\n",
    "feature_importances_sorted = plot_feature_importances(feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the top 15 features of the base case model, we can see that it contains a number of features that we saw had a high loan default rate during exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the feature importances for the default features\n",
    "feature_importances_sorted = plot_feature_importances(feature_importances_logit_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### <b> 5.0 Hyperparameter Tuning </b>\n",
    "\n",
    "After choosing LightGBM as the binary classifier, we can tune the hyperparameters for improving the model results through grid search, random search, and Bayesian optimization\n",
    "\n",
    "---\n",
    "\n",
    "<b> 5.1 Pre-processing </b>\n",
    "\n",
    "The hyperparameter tuning process needs an objective function, the domain space, optimization algorithm to give the results.\n",
    "\n",
    "First, the data is prepared for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(50)\n",
    "\n",
    "#Remove possible JSON characters\n",
    "df_undersampled_X = df_undersampled_X.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "df_undersampled_Y = df_undersampled_Y.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "test_X = test_X.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "test_Y = test_Y.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "# Create a training and testing dataset\n",
    "train_set = lgb.Dataset(data = df_undersampled_X, label = df_undersampled_Y)\n",
    "test_set = lgb.Dataset(data = test_X, label = test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The domain space is the range of hyperparameters over which the tuning process will be carried out. As seen below there are number of hyperparameters that can be modified. This grid can be used for grid/random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'boosting_type': ['gbdt', 'goss'],\n",
    "    'num_leaves': list(range(20, 150)),\n",
    "    'learning_rate': list(np.logspace(np.log10(0.005), np.log10(0.5), base = 10, num = 1000)),\n",
    "    'subsample_for_bin': list(range(20000, 300000, 20000)),\n",
    "    'min_child_samples': list(range(20, 500, 5)),\n",
    "    'reg_alpha': list(np.linspace(0, 1)),\n",
    "    'reg_lambda': list(np.linspace(0, 1)),\n",
    "    'colsample_bytree': list(np.linspace(0.6, 1, 10)),\n",
    "    'subsample': list(np.linspace(0.5, 1, 100)),\n",
    "    'is_unbalance': [True, False]\n",
    "}\n",
    "\n",
    "N_FOLDS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the tuning process will can be saved in the following dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EVALS = 50\n",
    "# Dataframes for random and grid search\n",
    "random_results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
    "                              index = list(range(MAX_EVALS)))\n",
    "\n",
    "grid_results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
    "                              index = list(range(MAX_EVALS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective function is the one which takes the input hyperparameters to test different models and optimize the values over multiple iterations. The function use cross-validation for each major iteration to minimize the loss (1- ROC AUC score). It also saves the ROC AUC scores, and the hyperparameter values for each iteration, and the function can be used for all 3 methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function\n",
    "OUT_FILE = 'hyperParam.csv'\n",
    "def objective(hyperparameters):\n",
    "    \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Optimization.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keep track of evals\n",
    "    global ITERATION\n",
    "    \n",
    "    ITERATION += 1\n",
    "    \n",
    "    print('current iteration:', ITERATION)\n",
    "    \n",
    "    # Using early stopping to find number of trees trained\n",
    "    if 'n_estimators' in hyperparameters:\n",
    "        del hyperparameters['n_estimators']\n",
    "    \n",
    "    if 'subsample' not in hyperparameters:  \n",
    "    \n",
    "        # Retrieve the subsample\n",
    "        subsample = hyperparameters['boosting_type'].get('subsample', 1.0)\n",
    "    \n",
    "        # Extract the boosting type and subsample to top level keys\n",
    "        hyperparameters['boosting_type'] = hyperparameters['boosting_type']['boosting_type']\n",
    "        hyperparameters['subsample'] = subsample\n",
    "    \n",
    "        # Make sure parameters that need to be integers are integers\n",
    "        for parameter_name in ['num_leaves', 'subsample_for_bin', 'min_child_samples']:\n",
    "            hyperparameters[parameter_name] = int(hyperparameters[parameter_name])\n",
    "            \n",
    "\n",
    "    start = timer()\n",
    "    \n",
    "    # Perform n_folds cross validation\n",
    "    cv_results = lgb.cv(hyperparameters, train_set, num_boost_round = 10000, nfold = N_FOLDS, \n",
    "                       callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=100),\n",
    "    ], metrics = 'auc', seed = 50)\n",
    "\n",
    "\n",
    "    run_time = timer() - start\n",
    "    \n",
    "    # Extract the best score\n",
    "    best_score = cv_results['valid auc-mean'][-1]\n",
    "    \n",
    "    # Loss must be minimized\n",
    "    loss = 1 - best_score\n",
    "    \n",
    "    # Boosting rounds that returned the highest cv score\n",
    "    n_estimators = len(cv_results['valid auc-mean'])\n",
    "    \n",
    "    # Add the number of estimators to the hyperparameters\n",
    "    hyperparameters['n_estimators'] = n_estimators\n",
    "\n",
    "    # Write to the csv file ('a' means append)\n",
    "    of_connection = open(OUT_FILE, 'a')\n",
    "    writer = csv.writer(of_connection)\n",
    "    writer.writerow([loss, hyperparameters, ITERATION, run_time, best_score])\n",
    "    of_connection.close()\n",
    "    \n",
    "    # Dictionary with information for evaluation\n",
    "    dict_return = {'loss': loss, 'hyperparameters': hyperparameters, 'iteration': ITERATION,\n",
    "            'train_time': run_time, 'score': best_score, 'status': STATUS_OK}\n",
    "\n",
    "    return dict_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<b> 5.2 Grid Search </b>\n",
    "\n",
    "The grid search algorithm iterates over the hyperparameters incrementally to explore the entire domain space. However, the exhaustive search is very time consuming and cannot be completed with finite computing resources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(param_grid, max_evals = MAX_EVALS):\n",
    "    \"\"\"Grid search algorithm (with limit on max evals)\"\"\"\n",
    "    \n",
    "    # Dataframe to store results\n",
    "    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
    "                              index = list(range(MAX_EVALS)))\n",
    "    \n",
    "    # https://codereview.stackexchange.com/questions/171173/list-all-possible-permutations-from-a-python-dictionary-of-lists\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    \n",
    "    global ITERATION\n",
    "    \n",
    "    ITERATION = 0\n",
    "    \n",
    "    # Iterate through every possible combination of hyperparameters\n",
    "    for v in itertools.product(*values):\n",
    "        \n",
    "        start = timer()\n",
    "        \n",
    "        # Create a hyperparameter dictionary\n",
    "        hyperparameters = dict(zip(keys, v))\n",
    "        \n",
    "        # Set the subsample ratio accounting for boosting type\n",
    "        hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters['subsample']\n",
    "        \n",
    "        # Evalute the hyperparameters\n",
    "        \n",
    "        dict_return = objective(hyperparameters)\n",
    "        eval_results = at(dict_return, 'score', 'hyperparameters', 'iteration')\n",
    "           \n",
    "        results.loc[ITERATION-1, :] = eval_results\n",
    "        \n",
    "        run_time = timer() - start\n",
    "    \n",
    "        print('run time:', run_time)\n",
    "        \n",
    "        # Normally would not limit iterations\n",
    "        if ITERATION-1 > MAX_EVALS:\n",
    "            break\n",
    "       \n",
    "    # Sort with best score on top\n",
    "    results.sort_values('score', ascending = False, inplace = True)\n",
    "    results.reset_index(inplace = True)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The created function can theoretically run on the entire domain space but is limited to maximum of 20 evaluations due to time and computational constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EVALS = 20\n",
    "\n",
    "grid_results = grid_search(param_grid)\n",
    "\n",
    "print('The best validation score was {:.5f}'.format(grid_results.loc[0, 'score']))\n",
    "print('\\nThe best hyperparameters were:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(grid_results.loc[0, 'params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best grid parameters\n",
    "grid_best_params = grid_results.loc[0, 'params']\n",
    "\n",
    "# Create, train, test model\n",
    "model = lgb.LGBMClassifier(**grid_best_params, random_state = 42)\n",
    "model.fit(df_undersampled_X, df_undersampled_Y)\n",
    "\n",
    "preds = model.predict_proba(test_X)[:, 1]\n",
    "\n",
    "print('The best model from grid search scores {:.5f} ROC AUC on the test set.'.format(roc_auc_score(test_Y, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best hyperparameters identified from the limited grid search and the ROC AUC score on test data are as given above"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myvenv)",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
