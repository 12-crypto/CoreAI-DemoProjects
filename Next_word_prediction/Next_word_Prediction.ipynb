{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa97bcc0-de5c-4f79-a82e-754edc2d6d78",
   "metadata": {},
   "source": [
    "# Next Word Prediction\n",
    "\n",
    "## Introduction and Objective\n",
    "\n",
    "In the era of advanced natural language processing, predictive text systems have become an integral part of user interfaces, enhancing user experience by providing suggestions and auto-completing text inputs. This notebook demonstrates the implementation of a next word prediction system using an N-gram model with Good-Turing smoothing and interpolation techniques. An N-gram model is a type of probabilistic language model that predicts the next item in a sequence based on the previous items, making it a powerful tool for understanding and generating human language.\n",
    "\n",
    "The primary objective of this project is to build a robust word prediction system by leveraging statistical language modeling techniques. The N-gram model is trained on a large corpus of text data, allowing it to learn the probabilities of sequences of words. To address the issue of zero probabilities for unseen N-grams, Good-Turing smoothing is applied, which adjusts the probability estimates for rare or unseen events. Additionally, interpolation combines the probabilities of different N-grams (e.g., bigrams, trigrams) to improve prediction accuracy. This combination of techniques ensures that the model can make accurate and reliable predictions, even when encountering new or uncommon word sequences.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The dataset used for training and evaluating the N-gram model is included in this repository. Specifically, the following files are used:\n",
    "- `internet_archive_scifi_v3.txt`: This file is used for training the N-gram model. It contains a large corpus of science fiction text data from which the model learns the probabilities of word sequences.\n",
    "\n",
    "You can find the dataset at this [Kaggle link](https://www.kaggle.com/datasets/jannesklaas/scifi-stories-text-corpus).\n",
    "\n",
    "This science fiction dataset will be utilized to complete sentences and generate text relevant to the genre, ensuring the model can capture the unique vocabulary and stylistic elements typical of science fiction narratives.\n",
    "\n",
    "## Installing Libraries\n",
    "\n",
    "To enhance the functionality of the environment, you may need to install some libraries not pre-installed in the CoreAI environment but required for this notebook. Follow these steps to install the necessary libraries from the `requirements.txt` file:\n",
    "\n",
    "### 1. Create and Activate the Virtual Environment\n",
    "\n",
    "Open your terminal or command prompt within the Jupyter notebook. Navigate to `File -> New -> Terminal` and type `bash` to get a shell compatible with the following commands.\n",
    "\n",
    "Navigate to the project directory where the notebook is to set up the environment.\n",
    "\n",
    "Execute the following commands to create and activate the virtual environment:\n",
    "\n",
    "```sh\n",
    "python3 -m venv --system-site-packages myvenv\n",
    "source myvenv/bin/activate\n",
    "pip3 install ipykernel\n",
    "python -m ipykernel install --user --name=myvenv --display-name=\"Python (myvenv)\"\n",
    "```\n",
    "\n",
    "### 2. Install Required Libraries\n",
    "\n",
    "Before running the following command in the Jupyter notebook, make sure you are in the directory where the Jupyter Notebook and virtual environment is located. Load the newly created \"Python (myvenv)\" kernel. This ensures the `./` path is always current. You can use the `cd` command to change to your project directory and `pwd` to verify your current directory.\n",
    "\n",
    "#### Important Note\n",
    "It is crucial to load the new \"myvenv\" kernel for the notebook to work correctly. If the new \"myvenv\" kernel is not loaded, the required libraries and environment settings will not be applied, and the notebook will not function as expected.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee176c4-9af1-405f-bdc3-836c81467e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!. ./myvenv/bin/activate; pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baef4a67-246e-49e1-90f3-2fc1a36c02d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "import string\n",
    "import time\n",
    "import gc\n",
    "from math import log10\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026e2f14-d34e-4c33-9bd7-07ecc9f6ec7f",
   "metadata": {},
   "source": [
    "The function `removePunctuations(sen)` processes an input string by splitting it into words, removing punctuation from each word (except possessive apostrophes), converting the words to lowercase, and then joining the words back into a single string. This cleaned string is then returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7d7bd0-06d9-4a6a-b3e3-7ef1a83e6cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunctuations(sen):\n",
    "    #split the string into word tokens\n",
    "    temp_l = sen.split()\n",
    "    #print(temp_l)\n",
    "    i = 0\n",
    "    j = 0\n",
    "    \n",
    "    #changes the word to lowercase and removes punctuations from it\n",
    "    for word in temp_l :\n",
    "        j = 0\n",
    "        #print(len(word))\n",
    "        for l in word :\n",
    "            if l in string.punctuation:\n",
    "                if l == \"'\":\n",
    "                    if j+1<len(word) and word[j+1] == 's':\n",
    "                        j = j + 1\n",
    "                        continue\n",
    "                word = word.replace(l,\" \")\n",
    "                #print(j,word[j])\n",
    "            j += 1\n",
    "\n",
    "        temp_l[i] = word.lower()\n",
    "        i=i+1   \n",
    "\n",
    "    #spliting is being don here beacause in sentences line here---so after punctuation removal it should \n",
    "    #become \"here so\"   \n",
    "    content = \" \".join(temp_l)\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a567bf8-6032-48cb-bb8b-e842464c6195",
   "metadata": {},
   "source": [
    "The function `loadCorpus(file_path, bi_dict, tri_dict, quad_dict, vocab_dict)` reads a corpus file line by line, processes each line to remove punctuation and convert words to lowercase, and then generates bigrams, trigrams, and quadgrams. It updates the dictionaries with the frequency counts of these n-grams and the vocabulary. It also ensures continuity across lines by storing the last three words of each line for pairing with the next line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12a8ac1-c2d9-40bb-9ff5-d4b04e49a50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadCorpus(file_path, bi_dict, tri_dict, quad_dict, vocab_dict):\n",
    "\n",
    "    w1 = ''    #for storing the 3rd last word to be used for next token set\n",
    "    w2 = ''    #for storing the 2nd last word to be used for next token set\n",
    "    w3 = ''    #for storing the last word to be used for next token set\n",
    "    token = []\n",
    "    #total no. of words in the corpus\n",
    "    word_len = 0\n",
    "\n",
    "    #open the corpus file and read it line by line\n",
    "    with open(file_path,'r') as file:\n",
    "        for line in file:\n",
    "\n",
    "            #split the string into word tokens\n",
    "            temp_l = line.split()\n",
    "            i = 0\n",
    "            j = 0\n",
    "            \n",
    "            #does the same as the removePunctuations() function,implicit declratation for performance reasons\n",
    "            #changes the word to lowercase and removes punctuations from it\n",
    "            for word in temp_l :\n",
    "                j = 0\n",
    "                #print(len(word))\n",
    "                for l in word :\n",
    "                    if l in string.punctuation:\n",
    "                        if l == \"'\":\n",
    "                            if j+1<len(word) and word[j+1] == 's':\n",
    "                                j = j + 1\n",
    "                                continue\n",
    "                        word = word.replace(l,\" \")\n",
    "                        #print(j,word[j])\n",
    "                    j += 1\n",
    "\n",
    "                temp_l[i] = word.lower()\n",
    "                i=i+1   \n",
    "\n",
    "            #spliting is being done here beacause in sentences line here---so after punctuation removal it should \n",
    "            #become \"here so\"   \n",
    "            content = \" \".join(temp_l)\n",
    "\n",
    "            token = content.split()\n",
    "            word_len = word_len + len(token)  \n",
    "\n",
    "            if not token:\n",
    "                continue\n",
    "\n",
    "            #add the last word from previous line\n",
    "            if w3!= '':\n",
    "                token.insert(0,w3)\n",
    "\n",
    "            temp0 = list(ngrams(token,2))\n",
    "\n",
    "            #since we are reading line by line some combinations of word might get missed for pairing\n",
    "            #for trigram\n",
    "            #first add the previous words\n",
    "            if w2!= '':\n",
    "                token.insert(0,w2)\n",
    "\n",
    "            #tokens for trigrams\n",
    "            temp1 = list(ngrams(token,3))\n",
    "\n",
    "            #insert the 3rd last word from previous line for quadgram pairing\n",
    "            if w1!= '':\n",
    "                token.insert(0,w1)\n",
    "\n",
    "            #add new unique words to the vocaulary set if available\n",
    "            for word in token:\n",
    "                if word not in vocab_dict:\n",
    "                    vocab_dict[word] = 1\n",
    "                else:\n",
    "                    vocab_dict[word]+= 1\n",
    "                  \n",
    "            #tokens for quadgrams\n",
    "            temp2 = list(ngrams(token,4))\n",
    "\n",
    "            #count the frequency of the bigram sentences\n",
    "            for t in temp0:\n",
    "                sen = ' '.join(t)\n",
    "                bi_dict[sen] += 1\n",
    "\n",
    "            #count the frequency of the trigram sentences\n",
    "            for t in temp1:\n",
    "                sen = ' '.join(t)\n",
    "                tri_dict[sen] += 1\n",
    "\n",
    "            #count the frequency of the quadgram sentences\n",
    "            for t in temp2:\n",
    "                sen = ' '.join(t)\n",
    "                quad_dict[sen] += 1\n",
    "\n",
    "\n",
    "            #then take out the last 3 words\n",
    "            n = len(token)\n",
    "\n",
    "            #store the last few words for the next sentence pairing\n",
    "            if (n -3) >= 0:\n",
    "                w1 = token[n -3]\n",
    "            if (n -2) >= 0:\n",
    "                w2 = token[n -2]\n",
    "            if (n -1) >= 0:\n",
    "                w3 = token[n -1]\n",
    "    return word_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c80a636-0e9b-49ca-badb-7647366da4a3",
   "metadata": {},
   "source": [
    "The function `findQuadgramProbGT(vocab_dict, bi_dict, tri_dict, quad_dict, quad_prob_dict, nc_dict, k)` calculates the probabilities of quadgrams using Good-Turing smoothing. For each quadgram, it splits the quadgram into tokens and derives the corresponding trigram. It then applies Good-Turing smoothing to adjust the counts of both the quadgram and trigram if their counts are less than or equal to `k`. The probability of each quadgram is calculated as the ratio of the adjusted quadgram count to the adjusted trigram count. These probabilities are stored in `quad_prob_dict`, with the trigrams as keys and the list of quadgram probabilities and their last word as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78edfdec-8a5f-482c-96a1-9078677b4d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findQuadgramProbGT(vocab_dict, bi_dict, tri_dict, quad_dict, quad_prob_dict, nc_dict, k):\n",
    "    \n",
    "    i = 0\n",
    "    V = len(vocab_dict)\n",
    "   \n",
    "    for quad_sen in quad_dict:\n",
    "        quad_token = quad_sen.split()\n",
    "        \n",
    "        #trigram sentence for key\n",
    "        tri_sen = ' '.join(quad_token[:3])\n",
    "\n",
    "        #find the probability\n",
    "        #Good Turing smoothing has been used\n",
    "        quad_count = quad_dict[quad_sen]\n",
    "        tri_count = tri_dict[tri_sen]\n",
    "        \n",
    "        if quad_dict[quad_sen] <= k  or (quad_sen not in quad_dict):\n",
    "            quad_count = findGoodTuringAdjustCount( quad_dict[quad_sen], k, nc_dict)\n",
    "        if tri_dict[tri_sen] <= k  or (tri_sen not in tri_dict):\n",
    "            tri_count = findGoodTuringAdjustCount( tri_dict[tri_sen], k, nc_dict)\n",
    "        \n",
    "        prob = quad_count / tri_count\n",
    "        \n",
    "        #add the trigram to the quadgram probabiltity dict\n",
    "        if tri_sen not in quad_prob_dict:\n",
    "            quad_prob_dict[tri_sen] = []\n",
    "            quad_prob_dict[tri_sen].append([prob,quad_token[-1]])\n",
    "        else:\n",
    "            quad_prob_dict[tri_sen].append([prob,quad_token[-1]])\n",
    "  \n",
    "    prob = None\n",
    "    quad_token = None\n",
    "    tri_sen = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cefb56-6c8d-43da-8efb-04e9bb7cbaca",
   "metadata": {},
   "source": [
    " This process of `findTrigramProbGT`  is similar to `findQuadgramProbGT`, but operates on trigrams instead of quadgrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1f637c-4121-48bb-aaed-648e5cd4bb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findTrigramProbGT(vocab_dict, bi_dict, tri_dict, tri_prob_dict, nc_dict, k):\n",
    "    \n",
    "    #vocabulary length\n",
    "    V = len(vocab_dict)\n",
    "    \n",
    "    #create a dictionary of probable words with their probabilities for\n",
    "    #trigram probabilites,key is a bigram and value is a list of prob and word\n",
    "    for tri in tri_dict:\n",
    "        tri_token = tri.split()\n",
    "        #bigram sentence for key\n",
    "        bi_sen = ' '.join(tri_token[:2])\n",
    "        \n",
    "        #find the probability\n",
    "        #Good Turing smoothing has been used\n",
    "        tri_count = tri_dict[tri]\n",
    "        bi_count = bi_dict[bi_sen]\n",
    "        \n",
    "        if tri_dict[tri] <= k or (tri not in tri_dict):\n",
    "            tri_count = findGoodTuringAdjustCount( tri_dict[tri], k, nc_dict)\n",
    "        if bi_dict[bi_sen] <= k or (bi_sen not in bi_dict):\n",
    "            bi_count = findGoodTuringAdjustCount( bi_dict[bi_sen], k, nc_dict)\n",
    "        \n",
    "        prob = tri_count / bi_count\n",
    "        \n",
    "        #add the bigram sentence  to the trigram probability dict\n",
    "        #tri_prob_dict is a dict of list\n",
    "        if bi_sen not in tri_prob_dict:\n",
    "            tri_prob_dict[bi_sen] = []\n",
    "            tri_prob_dict[bi_sen].append([prob,tri_token[-1]])\n",
    "        else:\n",
    "            tri_prob_dict[bi_sen].append([prob,tri_token[-1]])\n",
    "    \n",
    "    prob = None\n",
    "    tri_token = None\n",
    "    bi_sen = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482c3fd0-b83b-4a35-a7f4-5e613cbf081e",
   "metadata": {},
   "source": [
    " This process of `findBigramProbGT` is similar to `findQuadgramProbGT`, but operates on bigrams instead of quadgrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7f1bb6-e776-41a3-87c5-e27df7b9c6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findBigramProbGT(vocab_dict, bi_dict, bi_prob_dict, nc_dict, k):\n",
    "   \n",
    "    #vocabulary size\n",
    "    V = len(vocab_dict)\n",
    "    \n",
    "    #create a dictionary of probable words with their probabilities for bigram probabilites\n",
    "    for bi in bi_dict:\n",
    "        bi_token = bi.split()\n",
    "        #unigram for key\n",
    "        unigram = bi_token[0]\n",
    "       \n",
    "        #find the probability\n",
    "        #Good Turing smoothing has been used\n",
    "        bi_count = bi_dict[bi]\n",
    "        uni_count = vocab_dict[unigram]\n",
    "        \n",
    "        if bi_dict[bi] <= k or (bi not in bi_dict):\n",
    "            bi_count = findGoodTuringAdjustCount( bi_dict[bi], k, nc_dict)\n",
    "        if vocab_dict[unigram] <= k or (unigram not in vocab_dict):\n",
    "            uni_count = findGoodTuringAdjustCount( vocab_dict[unigram], k, nc_dict)\n",
    "        \n",
    "        prob = bi_count / uni_count\n",
    "        \n",
    "        #add the unigram to the bigram probability dict\n",
    "        #bi_prob_dict is a dict of list\n",
    "        if unigram not in bi_prob_dict:\n",
    "            bi_prob_dict[unigram] = []\n",
    "            bi_prob_dict[unigram].append([prob,bi_token[-1]])\n",
    "        else:\n",
    "            bi_prob_dict[unigram].append([prob,bi_token[-1]])\n",
    "    \n",
    "   \n",
    "    prob = None\n",
    "    bi_token = None\n",
    "    unigram = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9309f80c-7604-4602-86ca-63dc4414d24a",
   "metadata": {},
   "source": [
    "The function `sortProbWordDict` sorts the probability lists within the `bi_prob_dict`, `tri_prob_dict`, and `quad_prob_dict` dictionaries in descending order. For `quad_prob_dict`, it keeps only the top two probabilities after sorting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513893e1-f58c-4c85-8e18-b81f0c8948f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortProbWordDict(bi_prob_dict, tri_prob_dict, quad_prob_dict):\n",
    "    for key in bi_prob_dict:\n",
    "        if len(bi_prob_dict[key])>1:\n",
    "            bi_prob_dict[key] = sorted(bi_prob_dict[key],reverse = True)\n",
    "    \n",
    "    for key in tri_prob_dict:\n",
    "        if len(tri_prob_dict[key])>1:\n",
    "            tri_prob_dict[key] = sorted(tri_prob_dict[key],reverse = True)\n",
    "    \n",
    "    for key in quad_prob_dict:\n",
    "        if len(quad_prob_dict[key])>1:\n",
    "            quad_prob_dict[key] = sorted(quad_prob_dict[key],reverse = True)[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ff2601-66b8-4bac-9096-f2a4f36083b0",
   "metadata": {},
   "source": [
    "The `takeInput` function prompts the user to input a string, removes any punctuation, and ensures that the input contains at least three words. If the input is valid, it returns the last three words as a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fff64f7-4d03-4daa-a0b5-c9f46e9959ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def takeInput():\n",
    "    cond = False\n",
    "    #take input\n",
    "    while(cond == False):\n",
    "        sen = input('Enter the string\\n')\n",
    "        sen = removePunctuations(sen)\n",
    "        temp = sen.split()\n",
    "        if len(temp) < 3:\n",
    "            print(\"Please enter atleast 3 words !\")\n",
    "        else:\n",
    "            cond = True\n",
    "    sen = \" \".join(temp)\n",
    "    return sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399ef32b-835c-4bdb-97da-6bde6aed6e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib import style\n",
    "\n",
    "#finds the slope for the best fit line\n",
    "def findBestFitSlope(x,y):\n",
    "    m = (( mean(x)*mean(y) - mean(x*y) ) / \n",
    "          ( mean(x)** 2 - mean(x**2)))\n",
    "\n",
    "    return m\n",
    "      \n",
    "#finds the intercept for the best fit line\n",
    "def findBestFitIntercept(x,y,m):\n",
    "    c = mean(y) - m*mean(x)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee298a68-5106-4943-b55c-c072ad4e7473",
   "metadata": {},
   "source": [
    "The function `findFrequencyOfFrequencyCount` calculates the frequency of frequencies (`Nc`) for n-grams in `ngram_dict` up to a count `k`. It ensures all counts up to `k+1` are present, filling any missing values using linear regression based on the log-log relationship between `c` and `Nc`. This helps in applying Good-Turing smoothing by estimating counts for unseen n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0d5b80-6276-42e8-8c6a-ba0fd49289d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findFrequencyOfFrequencyCount(ngram_dict, k, n, V, token_len):\n",
    "    #for keeping count of 'c' value i.e Nc\n",
    "    nc_dict = {}\n",
    "    #we find the value of Nc,c = 0 by V^n - (total n-gram tokens)\n",
    "    nc_dict[0] = V**n - token_len\n",
    "    #find the count Nc till c = k,we will take k = 5\n",
    "    #find counts for n-gram\n",
    "    for key in ngram_dict:\n",
    "        if ngram_dict[key] <= k + 1:\n",
    "            if ngram_dict[key] not in nc_dict:\n",
    "                nc_dict[ ngram_dict[key]] = 1\n",
    "            else:\n",
    "                nc_dict[ ngram_dict[key] ] += 1\n",
    "    \n",
    "    #check if all the values of Nc are there in the nc_dict or not ,if there then return           \n",
    "    val_present = True\n",
    "    for i in range(1,7):\n",
    "        if i not in nc_dict:\n",
    "            val_present = False\n",
    "            break\n",
    "    if val_present == True:\n",
    "        return nc_dict\n",
    "    \n",
    "    #now fill in the values of nc in case it is not there using regression upto c = 6\n",
    "    #we use :[ log(Nc) = blog(c) + a ] as the equation\n",
    "\n",
    "    #we first need to find data for regression that is values(Nc,c) we take 5 data points\n",
    "    data_pts = {}\n",
    "    i = 0\n",
    "    #get first 5 counts value i.e c\n",
    "    #for quadgram\n",
    "    for key in ngram_dict:\n",
    "        if ngram_dict[key] not in data_pts:\n",
    "                data_pts[ ngram_dict[key] ] = 1\n",
    "                i += 1\n",
    "        if i >5:\n",
    "            break\n",
    "            \n",
    "    #now get Nc for those c values\n",
    "    for key in ngram_dict:\n",
    "        if ngram_dict[key] in data_pts:\n",
    "            data_pts[ ngram_dict[key] ] += 1\n",
    "    \n",
    "    #make x ,y coordinates for regression \n",
    "    x_coor = [ np.log(item) for item in data_pts ]\n",
    "    y_coor = [ np.log( data_pts[item] ) for item in data_pts ]\n",
    "    x = np.array(x_coor, dtype = np.float64)\n",
    "    y = np.array(y_coor , dtype = np.float64)\n",
    "   \n",
    "\n",
    "    #now do regression\n",
    "    #find the slope and intercept for the regression equation\n",
    "    slope_m = findBestFitSlope(x,y)\n",
    "    intercept_c = findBestFitIntercept(x,y,slope_m)\n",
    "\n",
    "    #now find the missing Nc terms and give them value using regression\n",
    "    for i in range(1,(k+2)):\n",
    "        if i not in nc_dict:\n",
    "            nc_dict[i] = (slope_m*i) + intercept_c\n",
    "    \n",
    "    return nc_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd8e2ef-c534-4b32-a531-2786b461333c",
   "metadata": {},
   "source": [
    "The function `findGoodTuringAdjustCount` calculates the Good-Turing adjusted count for a given frequency `c` using the `nc_dict` that contains frequency-of-frequency counts. It adjusts the count to account for low-frequency events and provides a more accurate probability estimate for rare n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2669e9a-390b-4f3f-8989-3142566e79da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findGoodTuringAdjustCount(c, k, nc_dict):\n",
    "   \n",
    "    adjust_count = ( ( (( c + 1)*( nc_dict[c + 1] / nc_dict[c])) - ( c * (k+1) * nc_dict[k+1] / nc_dict[1]) ) /\n",
    "                     ( 1 - (( k + 1)*nc_dict[k + 1] / nc_dict[1]) )\n",
    "                   )\n",
    "    return adjust_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a71412f-51b2-4c68-8d64-cbdec8d1c1a1",
   "metadata": {},
   "source": [
    "The function `doPredictionBackoffGT` predicts the next word in a sequence using a backoff model. It first checks for the highest-order n-gram (quadgram) in `quad_prob_dict`, and if not found, it backs off to trigrams (`tri_prob_dict`) and then bigrams (`bi_prob_dict`), returning the most probable word if a valid prediction is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a37947-db9c-4a30-adcc-3e4ebccb496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doPredictionBackoffGT(input_sen, bi_dict, tri_dict, quad_dict, bi_prob_dict, tri_prob_dict, quad_prob_dict):\n",
    "\n",
    "    tokens = input_sen.split()\n",
    "    \n",
    "\n",
    "    pred = []\n",
    "    \n",
    "\n",
    "    if len(tokens) >= 3:\n",
    "        quad_key = ' '.join(tokens[-3:])\n",
    "        if quad_key in quad_prob_dict and quad_prob_dict[quad_key][0][0] > 0:\n",
    "            pred = quad_prob_dict[quad_key][0]\n",
    "            return pred\n",
    "    \n",
    "    # Check trigrams\n",
    "    if len(tokens) >= 2:\n",
    "        tri_key = ' '.join(tokens[-2:])\n",
    "        if tri_key in tri_prob_dict and tri_prob_dict[tri_key][0][0] > 0:\n",
    "            pred = tri_prob_dict[tri_key][0]\n",
    "            return pred\n",
    "    \n",
    "    # Check bigrams\n",
    "    if len(tokens) >= 1:\n",
    "        bi_key = tokens[-1]\n",
    "        if bi_key in bi_prob_dict and bi_prob_dict[bi_key][0][0] > 0:\n",
    "            pred = bi_prob_dict[bi_key][0]\n",
    "            return pred\n",
    "    \n",
    "    # Return the prediction (empty if no match found)\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad3fd72-53ce-428c-9150-24758ef4ca57",
   "metadata": {},
   "source": [
    "The `word_prediction` function initializes dictionaries to store n-gram frequencies and probabilities, loads and processes the training corpus, creates n-gram probability dictionaries with Good-Turing smoothing, and sorts these dictionaries. It then takes user input, predicts the next word using the backoff model, and prints the prediction if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8c5349-8645-4f32-84b6-4af23cbb2ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictss(train_file):\n",
    "    vocab_dict = defaultdict(int)          #for storing the different words with their frequencies    \n",
    "    bi_dict = defaultdict(int)             #for keeping count of sentences of two words\n",
    "    tri_dict = defaultdict(int)            #for keeping count of sentences of three words\n",
    "    quad_dict = defaultdict(int)           #for keeping count of sentences of four words\n",
    "    quad_prob_dict = OrderedDict()              \n",
    "    tri_prob_dict = OrderedDict()\n",
    "    bi_prob_dict = OrderedDict()\n",
    "    \n",
    "        #load corpus\n",
    "    token_len = loadCorpus(train_file, bi_dict, tri_dict, quad_dict, vocab_dict)\n",
    "    \n",
    "        #create the different Nc dictionaries for ngrams\n",
    "        #threshold value\n",
    "    k = 5\n",
    "    V = len(vocab_dict)\n",
    "    quad_nc_dict = findFrequencyOfFrequencyCount(quad_dict, k, 4, V, len(quad_dict))\n",
    "    tri_nc_dict = findFrequencyOfFrequencyCount(tri_dict, k, 3, V, len(tri_dict))\n",
    "    bi_nc_dict = findFrequencyOfFrequencyCount(bi_dict, k, 2, V, len(bi_dict))\n",
    "    uni_nc_dict = findFrequencyOfFrequencyCount(bi_dict, k, 1, V, len(vocab_dict))\n",
    "    return vocab_dict, bi_dict, tri_dict, quad_dict, quad_prob_dict, quad_nc_dict,k, tri_prob_dict,tri_nc_dict,bi_prob_dict,bi_nc_dict\n",
    "\n",
    "def word_prediction(input_sen,vocab_dict, bi_dict, tri_dict, quad_dict, quad_prob_dict, quad_nc_dict,k, tri_prob_dict,tri_nc_dict,bi_prob_dict,bi_nc_dict):\n",
    "    ##WORD PREDICTION \n",
    "    #take user input \n",
    "    print('The sentence given by user is: ', input_sen)\n",
    "\n",
    "    prediction = doPredictionBackoffGT(input_sen, bi_dict, tri_dict, quad_dict, bi_prob_dict, tri_prob_dict, quad_prob_dict)\n",
    "    if prediction:\n",
    "        print('Word Prediction:',prediction[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0b9ae4-f7ee-4019-a7da-14908e9663ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the corpus\n",
    "vocab_dict, bi_dict, tri_dict, quad_dict, quad_prob_dict, quad_nc_dict,k, tri_prob_dict,tri_nc_dict,bi_prob_dict,bi_nc_dict= dictss('data/internet_archive_scifi_v3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9205f3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## variable declaration\n",
    "#create quadgram probability dictionary\n",
    "findQuadgramProbGT(vocab_dict, bi_dict, tri_dict, quad_dict, quad_prob_dict, quad_nc_dict, k)\n",
    "#create trigram probability dictionary\n",
    "findTrigramProbGT(vocab_dict, bi_dict, tri_dict, tri_prob_dict, tri_nc_dict, k)\n",
    "#create bigram probability dictionary\n",
    "findBigramProbGT(vocab_dict, bi_dict, bi_prob_dict, bi_nc_dict, k)\n",
    "#sort the probability dictionaries of quad,tri and bi grams\n",
    "sortProbWordDict(bi_prob_dict, tri_prob_dict, quad_prob_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdf7d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sen = \"rates for IF are set for 6 issues in the U.S. and\"\n",
    "word_prediction(input_sen,vocab_dict, bi_dict, tri_dict, quad_dict, quad_prob_dict, quad_nc_dict,k, tri_prob_dict,tri_nc_dict,bi_prob_dict,bi_nc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888b5d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sen = \"Naia North confesses to being the\"\n",
    "word_prediction(input_sen,vocab_dict, bi_dict, tri_dict, quad_dict, quad_prob_dict, quad_nc_dict,k, tri_prob_dict,tri_nc_dict,bi_prob_dict,bi_nc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecc9229-1f0d-4f56-b48c-e9aa23545c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sen= takeInput()\n",
    "word_prediction(input_sen,vocab_dict, bi_dict, tri_dict, quad_dict, quad_prob_dict, quad_nc_dict,k, tri_prob_dict,tri_nc_dict,bi_prob_dict,bi_nc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ba24ce-97e7-4439-b00f-312cefc05af9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myvenv)",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "c07b937ccaa03fdd8f966dca7af6c234458a34300e501abfd0bf867b89073094"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
