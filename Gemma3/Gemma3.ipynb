{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma3 on CoreAI\n",
    "\n",
    "The CoreAI container is started using the `start_CoreAI.sh` script which uses `podman`, set up a mounted volume called `/iti` where the script exists and will write files with the same user ID and group ID as the calling user.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enhance the functionality of the CoreAI  environment, we need to install some libraries not pre-installed but required for this notebook. \n",
    "\n",
    "## Create and Activate the Virtual Environment:\n",
    "Open your terminal or command prompt within the Jupyter notebook. Navigate via `File -> New -> Terminal`.\n",
    "Type `bash` to access a shell compatible with the following commands.\n",
    "Navigate to the project directory where you want to set up the environment:\n",
    "\n",
    "```bash\n",
    "python3 -m venv --system-site-packages myvenv\n",
    "source myvenv/bin/activate\n",
    "pip3 install ipykernel\n",
    "python -m ipykernel install --user --name=myvenv --display-name=\"Python (myvenv)\"\n",
    "```\n",
    "\n",
    "Those commands can also be executed using the `./create_myvenv.sh` script located in the same folder where this notebook is.\n",
    "\n",
    "## Install Required Libraries:\n",
    "\n",
    "Before running the following commands, load the `Python (myvenv)` kernel.\n",
    "\n",
    "Ensure you are in the directory where the Jupyter Notebook and the `myvenv` directory are located. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!. ./myvenv/bin/activate; pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add `accelerate` to the PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "pwd = os.getcwd()\n",
    "os.environ['PATH'] =  os.path.join(pwd, 'myvenv/bin') + os.pathsep + os.environ['PATH']\n",
    "\n",
    "! echo $PATH\n",
    "! which accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make sure `HF_HOME` is set BEFORE `transformers` is loaded**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('HF_HOME', exist_ok=True)\n",
    "os.environ['HF_HOME'] = 'HF_HOME'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'HF_TOKEN' not in os.environ:\n",
    "    printf(\"No HF_TOKEN set, will not be able to download the model\")\n",
    "    exit(1)\n",
    "\n",
    "hf_token=os.environ['HF_TOKEN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM: Large Language Model queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM: Obtain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"google/gemma-3-4b-it\",\n",
    "    device=\"cuda\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=hf_token\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM: Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def takeInput():\n",
    "    cond = False\n",
    "    #take input\n",
    "    while(cond == False):\n",
    "        sen = input('Enter the string\\n')\n",
    "        temp = sen.split()\n",
    "        if len(temp) < 3:\n",
    "            print(\"Please enter atleast 3 words !\")\n",
    "        else:\n",
    "            cond = True\n",
    "    return sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = takeInput()\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"you are a helpful assistant\"}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": f\"{question}\"}]\n",
    "    },\n",
    "]\n",
    "user_message = messages[1][\"content\"][0][\"text\"]\n",
    "\n",
    "output = pipe(text_inputs=user_message, max_new_tokens=1000)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When done using the LLM: free up GPU memory\n",
    "\n",
    "Note: we are using the same variable for text and image (`pipe`) this code can be used when switching from one method to the next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up the model from memory before testing the VLM (Garbage Collection)\n",
    "import gc\n",
    "\n",
    "# check memory\n",
    "print(torch.cuda.memory_allocated())\n",
    "\n",
    "del pipe\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# check memory again\n",
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLM: Image Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VLM: Obtain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"image-text-to-text\",\n",
    "    model=\"google/gemma-3-4b-it\",\n",
    "    device=\"cuda\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VLM: Exract information from images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify prompt to ask questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data\n",
    "!wget https://raw.githubusercontent.com/Infotrend-Inc/OpenAI_WebUI/refs/heads/main/assets/Infotrend_Logo.png -O data/test1.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL \n",
    "import base64\n",
    "import io\n",
    "from IPython.display import display\n",
    "\n",
    "img_file = \"data/test1.png\" # a framework for a page object detection network based on Mask R-CNN\n",
    "\n",
    "\n",
    "def base64_image(img_file):\n",
    "    img_type = \"png\"\n",
    "    img_b64 = None\n",
    "    img_str = None\n",
    "    img_bytes = io.BytesIO()\n",
    "    with PIL.Image.open(img_file) as image:\n",
    "        display(image)\n",
    "        image.save(img_bytes, format=img_type)\n",
    "        img_b64 = base64.b64encode(img_bytes.getvalue()).decode('utf-8')\n",
    "    if img_b64 is not None:\n",
    "        img_str = f\"data:image/{img_type};base64,{img_b64}\"\n",
    "    \n",
    "    if img_str is None:\n",
    "        print(\"No valid image data\")\n",
    "        exit(1)\n",
    "\n",
    "    return img_str\n",
    "\n",
    "img_str = base64_image(img_file)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image_url\", \"image_url\": { \"url\": img_str } },\n",
    "            {\"type\": \"text\", \"text\": \"Describe the image. what is the field of expertise needed, explain the idea behind the meaning of the image?\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "output = pipe(text=messages, max_new_tokens=1000)\n",
    "print(output[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p data\n",
    "!wget https://raw.githubusercontent.com/Infotrend-Inc/OpenAI_WebUI/refs/heads/main/assets/Screenshot-OAI_WebUI_GPT.jpg -O data/test2.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_file = 'data/test2.png'\n",
    "img_str = base64_image(img_file)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image_url\", \"image_url\": { \"url\": img_str } },\n",
    "            {\"type\": \"text\", \"text\": \"Describe the image.\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "output = pipe(text=messages, max_new_tokens=1000)\n",
    "print(output[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webcam\n",
    "\n",
    "Requires that the `-v /dev/video0:/dev/video0` flag is used when starting the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from IPython.display import display, Image\n",
    "import ipywidgets as widgets\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capture an image and store it to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopButton = widgets.ToggleButton(value=False, description='Stop', disabled=False, button_style='danger', tooltip='Description', icon='square')\n",
    "\n",
    "def view(button):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    display_handle=display(None, display_id=True)\n",
    "\n",
    "    data = None\n",
    "    while stopButton.value == False:\n",
    "        _, data = cap.read()\n",
    "        data = cv2.flip(data, 1) # if your camera reverses your image\n",
    "        _, frame = cv2.imencode('.jpeg', data)\n",
    "        display_handle.update(Image(data=frame.tobytes()))\n",
    "\n",
    "    if stopButton.value==True:\n",
    "        cap.release()\n",
    "        cv2.imwrite(\"data/webcam.png\", data)\n",
    "            \n",
    "display(stopButton)\n",
    "thread = threading.Thread(target=view, args=(stopButton,))\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_file = \"data/webcam.png\"\n",
    "img_str = base64_image(img_file)\n",
    "    \n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image_url\", \"image_url\": { \"url\": img_str } },\n",
    "            {\"type\": \"text\", \"text\": \"Describe the image\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "output = pipe(text=messages, max_new_tokens=1000)\n",
    "print(output[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myvenv)",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
