{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52073b78-8912-4324-9008-fd195fb41f8f",
   "metadata": {},
   "source": [
    "# Fashion-MNIST Classification using Neural Network\n",
    "## Introduction\n",
    "This Jupyter notebook demonstrates the classification of apparel images from the Fashion MNIST dataset using a neural network. Fashion MNIST is a dataset of 60,000 training and 10,000 test images, each a 28x28 grayscale image associated with one of 10 fashion categories. The objective is to build and train a model capable of accurately classifying unseen fashion items. This project leverages the power of TensorFlow and PyTorch within an Infotrend pre-built container which includes CUDA, TensorFlow, PyTor...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c200a1-71f2-4d00-909d-df3a39511117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Download training and testing data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "train_ds = datasets.FashionMNIST('F_MNIST_data', download=True, train=True, transform=transform)\n",
    "test_ds = datasets.FashionMNIST('F_MNIST_data', download=True, train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377be4fc-2100-42b5-b425-af7db34a98b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train set into training (80%) and validation set (20%)\n",
    "train_num = len(train_ds)\n",
    "indices = list(range(train_num))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(0.2 * train_num))\n",
    "val_idx, train_idx = indices[:split], indices[split:]\n",
    "len(val_idx), len(train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a7eb5d-7958-4196-a758-2cffc24b5204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataloaders\n",
    "train_sampler = torch.utils.data.sampler.SubsetRandomSampler(train_idx)\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=64, sampler=train_sampler)\n",
    "val_sampler = torch.utils.data.sampler.SubsetRandomSampler(val_idx)\n",
    "val_dl = torch.utils.data.DataLoader(train_ds, batch_size=64, sampler=val_sampler)\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1f4f7c-66ed-42d8-945d-1e8eb63691a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = next(iter(train_dl))\n",
    "print(image[0].shape, label.shape)\n",
    "desc = ['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle Boot']\n",
    "print(desc[label[0].item()])\n",
    "plt.imshow(image[0].numpy().squeeze(), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0332dd58-a14d-4351-b57f-3b44f801d29c",
   "metadata": {},
   "source": [
    "## Build the Network\n",
    "In this section, we construct a neural network that includes several layers designed to process and classify images. The architecture consists of convolutional layers followed by pooling layers, fully connected layers, and a final softmax layer to classify into one of the ten categories. This architecture is chosen for its efficiency in image recognition tasks, allowing the network to learn hierarchical representations of the images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf7ad4a-0c46-4e72-89c3-1c9a41a664aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network():\n",
    "    model = nn.Sequential(OrderedDict([('fc1', nn.Linear(784, 128)),\n",
    "                                       ('relu1', nn.ReLU()),\n",
    "                                       ('drop1', nn.Dropout(0.25)),                                       \n",
    "                                       ('fc2', nn.Linear(128, 64)),\n",
    "                                       ('relu2', nn.ReLU()),\n",
    "                                       ('drop1', nn.Dropout(0.25)),                                       \n",
    "                                       ('output', nn.Linear(64, 10)),\n",
    "                                       ('logsoftmax', nn.LogSoftmax(dim=1))]))\n",
    "    # Use GPU if available\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(\"Device : \",device)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # define the criterion and optimizer\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "    return model, loss_fn, optimizer, device                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1130f07-25e1-420d-bdb0-ddb6135be4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, loss_fn, optimizer, device = network()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f3a8f9-14ad-47b4-8569-c7d153ee8ee2",
   "metadata": {},
   "source": [
    "## Train the Network\n",
    "Here, we describe the training process of our neural network. The model is trained using a batch size of 64 images over 25 epochs. We use cross-entropy loss to quantify the difference between predicted and actual labels and an Adam optimizer for adjustments. Techniques such as dropout and batch normalization are utilized to prevent overfitting and ensure the model generalizes well to new data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1f7b07-ef48-45e1-a6bd-cf67676cb1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate(model, loss_fn, optimizer, trainloader, testloader, device, n_epochs=25):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    print(f\"Device: {device}\")\n",
    "    for epoch in range(n_epochs):\n",
    "        # Set mode to training - Dropouts will be used here\n",
    "        model.train()\n",
    "        train_epoch_loss = 0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # flatten the images to batch_size x 784\n",
    "            images = images.view(images.shape[0], -1)\n",
    "            # forward pass\n",
    "            outputs = model(images)\n",
    "            # backpropogation\n",
    "            train_batch_loss = loss_fn(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            train_batch_loss.backward()\n",
    "            # Weight updates\n",
    "            optimizer.step()\n",
    "            train_epoch_loss += train_batch_loss.item()\n",
    "        else:\n",
    "            # One epoch of training complete\n",
    "            # calculate average training epoch loss\n",
    "            train_epoch_loss = train_epoch_loss/len(trainloader)\n",
    "\n",
    "            # Now Validate on testset\n",
    "            with torch.no_grad():\n",
    "                test_epoch_acc = 0\n",
    "                test_epoch_loss = 0\n",
    "                # Set mode to eval - Dropouts will NOT be used here\n",
    "                model.eval()\n",
    "                for images, labels in testloader:\n",
    "                    images, labels = images.to(device), labels.to(device)                    \n",
    "                    # flatten images to batch_size x 784\n",
    "                    images = images.view(images.shape[0], -1)\n",
    "                    # make predictions \n",
    "                    test_outputs = model(images)\n",
    "                    # calculate test loss\n",
    "                    test_batch_loss = loss_fn(test_outputs, labels)\n",
    "                    test_epoch_loss += test_batch_loss\n",
    "                    \n",
    "                    # get probabilities, extract the class associated with highest probability\n",
    "                    proba = torch.exp(test_outputs)\n",
    "                    _, pred_labels = proba.topk(1, dim=1)\n",
    "                    \n",
    "                    # compare actual labels and predicted labels\n",
    "                    result = pred_labels == labels.view(pred_labels.shape)\n",
    "                    batch_acc = torch.mean(result.type(torch.FloatTensor))\n",
    "                    test_epoch_acc += batch_acc.item()\n",
    "                else:\n",
    "                    # One epoch of training and validation done\n",
    "                    # calculate average testing epoch loss\n",
    "                    test_epoch_loss = test_epoch_loss/len(testloader)\n",
    "                    # calculate accuracy as correct_pred/total_samples\n",
    "                    test_epoch_acc = test_epoch_acc/len(testloader)\n",
    "                    # save epoch losses for plotting\n",
    "                    train_losses.append(train_epoch_loss)\n",
    "                    test_losses.append(test_epoch_loss)\n",
    "                    # print stats for this epoch\n",
    "                    print(f'Epoch: {epoch} -> train_loss: {train_epoch_loss:.19f}, val_loss: {test_epoch_loss:.19f}, ',\n",
    "                          f'val_acc: {test_epoch_acc*100:.2f}%')\n",
    "    \n",
    "    # Convert tensor values in test_losses to floats\n",
    "    test_losses_floats = [loss.item() for loss in test_losses]\n",
    "    # Finally plot losses\n",
    "    plt.plot(train_losses, label='train-loss')\n",
    "    plt.plot(test_losses_floats, label='val-loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6f7d9e-94cf-4aff-967f-b10884c9354c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validate\n",
    "train_validate(model, loss_fn, optimizer, train_dl, val_dl, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea07e69-1e42-43f2-b629-19e65699e07b",
   "metadata": {},
   "source": [
    "### Predict a Single Image\n",
    "This subsection details the methodology for making predictions with our trained model on a single image. The process involves preprocessing the image to fit the input requirements of the network, performing a forward pass, and interpreting the output probabilities to yield a category prediction. This step is crucial for understanding the model's practical application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0efc579-6aeb-41f0-b348-a73149776158",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Test out the network!\n",
    "dataiter = iter(test_dl)\n",
    "images, labels = next(dataiter)\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "index = 49\n",
    "img, label = images[index], labels[index]\n",
    "# Convert 2D image to 1D vector\n",
    "img = img.view(img.shape[0], -1)\n",
    "\n",
    "# Calculate the class probabilities (softmax) for img\n",
    "proba = torch.exp(model(img))\n",
    "\n",
    "# Plot the image and probabilities\n",
    "desc = ['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle Boot']\n",
    "fig, (ax1, ax2) =  plt.subplots(figsize=(13, 6), nrows=1, ncols=2)\n",
    "ax1.axis('off')\n",
    "ax1.imshow(images[index].cpu().numpy().squeeze())\n",
    "ax1.set_title(desc[label.item()])\n",
    "ax2.bar(range(10), proba.detach().cpu().numpy().squeeze())\n",
    "ax2.set_xticks(range(10))\n",
    "ax2.set_xticklabels(desc, size='small')\n",
    "ax2.set_title('Predicted Probabilities')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e64856-5599-4d8a-b4b8-04ec966aa2d9",
   "metadata": {},
   "source": [
    "## Validate on Test Set\n",
    "Finally, we evaluate the model's performance on a separate test set. This is crucial to ensure our model generalizes well beyond the training data. We assess the model using several metrics, including accuracy, precision, recall, and F1-score. These metrics provide a comprehensive view of the model's performance and help identify areas for improvement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505e7173-f874-4cfa-b8d7-10ff063a9254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate\n",
    "with torch.no_grad():\n",
    "    batch_acc = []\n",
    "    model.eval()\n",
    "    for images, labels in test_dl:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # flatten images to batch_size x 784\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        # make predictions and get probabilities\n",
    "        proba = torch.exp(model(images))\n",
    "        # extract the class associted with highest probability\n",
    "        _, pred_labels = proba.topk(1, dim=1)\n",
    "        # compare actual labels and predicted labels\n",
    "        result = pred_labels == labels.view(pred_labels.shape)\n",
    "        acc = torch.mean(result.type(torch.FloatTensor))\n",
    "        batch_acc.append(acc.item())\n",
    "    else:\n",
    "        print(f'Test Accuracy: {torch.mean(torch.tensor(batch_acc))*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a7f89b-ac8d-4878-874a-3038c4c065c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## More powerful model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aab8295-4ac7-49be-a128-6b17b37a33e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine network with dropout layers in between\n",
    "def network():\n",
    "    model = nn.Sequential(OrderedDict([('fc1', nn.Linear(784, 392)),\n",
    "                                       ('relu1', nn.ReLU()),\n",
    "                                       ('drop1', nn.Dropout(0.25)),\n",
    "                                       ('fc12', nn.Linear(392, 196)),\n",
    "                                       ('relu2', nn.ReLU()),\n",
    "                                       ('drop2', nn.Dropout(0.25)),\n",
    "                                       ('fc3', nn.Linear(196, 98)),\n",
    "                                       ('relu3', nn.ReLU()),\n",
    "                                       ('drop3', nn.Dropout(0.25)),                                       \n",
    "                                       ('fc4', nn.Linear(98, 49)),\n",
    "                                       ('relu4', nn.ReLU()),\n",
    "                                       ('output', nn.Linear(49, 10)),\n",
    "                                       ('logsoftmax', nn.LogSoftmax(dim=1))]))\n",
    "    \n",
    "    # Use GPU if available\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "\n",
    "    # define the criterion and optimizer\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0007)\n",
    "\n",
    "    return model, loss_fn, optimizer, device       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a7eaea-3b40-4d70-a053-cf72010c7c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, loss_fn, optimizer, device = network()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af58e69-ee6d-4555-aa58-443e2b85286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validate again with new architecture\n",
    "train_validate(model, loss_fn, optimizer, train_dl, val_dl, device, n_epochs=35)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a78eaf-139e-473c-8e95-8c3c59d33c43",
   "metadata": {},
   "source": [
    "## Validate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f637ff16-f824-4373-bade-873841d408c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    batch_acc = []\n",
    "    for images, labels in test_dl:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # flatten images to batch_size x 784\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        # make predictions and get probabilities\n",
    "        proba = torch.exp(model(images))\n",
    "        # extract the class associted with highest probability\n",
    "        _, pred_labels = proba.topk(1, dim=1)\n",
    "        # compare actual labels and predicted labels\n",
    "        result = pred_labels == labels.view(pred_labels.shape)\n",
    "        acc = torch.mean(result.type(torch.FloatTensor))\n",
    "        batch_acc.append(acc.item())\n",
    "    else:\n",
    "        print(f'Accuracy: {torch.mean(torch.tensor(batch_acc))*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c05e90-fb3b-4091-9c2f-8e21406a0eb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
