{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyrical Video Generator\n",
    "## Overview of the Lyrical Video Generator Notebook\n",
    "\n",
    "This Jupyter Notebook is designed to automate the creation of continuous lyric videos from audio files using advanced AI models and multimedia processing tools. The notebook provides an interactive environment where users can input audio files, process them through transcription and image generation, and generate a fully synchronized lyric video with guaranteed audio-visual coverage.\n",
    "\n",
    "### **Key Features**\n",
    "\n",
    "- **Audio File Input**: Users can specify or upload audio files to be processed, with predefined paths for ease of use.\n",
    "- **Audio Transcription with Coverage**: Implements English transcription using the Whisper model, ensuring every segment of audio is covered with text or placeholders for instrumental sections.\n",
    "- **Dynamic Visual Segmentation**: Creates time-based segments to guarantee visual content for every second of audio, preventing gaps in the output video.\n",
    "- **AI-Generated Visuals**: Utilizes Stable Diffusion to generate varied and dynamic images for each segment, enhancing visual appeal with diverse styles and color themes.\n",
    "- **Continuous Video Assembly**: Combines text overlays, generated images, and audio into a seamless video using MoviePy, with robust error handling for uninterrupted playback.\n",
    "- **Real-Time Processing Feedback**: Logs each step of the process, from transcription to video rendering, providing transparency and progress updates.\n",
    "- **Output Visualization**: Displays the final lyric video within the notebook or saves it to a designated output directory for review and sharing.\n",
    "\n",
    "### **Technologies Used**\n",
    "\n",
    "- **PyTorch**: Powers the AI models for transcription and image generation, leveraging GPU acceleration when available.\n",
    "- **Whisper**: Used for accurate audio-to-text transcription with forced language settings to prevent errors.\n",
    "- **Stable Diffusion (Diffusers)**: Generates unique images for visual segments, ensuring variety in the video output.\n",
    "- **MoviePy**: Handles video and audio processing, enabling the creation of composite video clips with text overlays.\n",
    "- **FFmpeg**: Supports audio segmentation and processing for transcription, ensuring high-quality audio handling.\n",
    "- **PIL (Python Imaging Library)**: Facilitates text rendering for overlays, creating visually appealing lyric displays.\n",
    "- **tqdm**: Provides progress bars for long-running tasks, improving user experience during processing.\n",
    "\n",
    "This notebook is ideal for musicians, content creators, and developers who want to create professional lyric videos effortlessly or adapt the code for custom multimedia projects.\n",
    "\n",
    "\n",
    "### **Pre-trained Models**\n",
    "\n",
    "The notebook relies on pre-trained models for transcription and image generation. Follow the instructions below to download and set up the necessary models:\n",
    "\n",
    "- **Whisper Model**: The \"base\" model is used for audio transcription. You can download and install Whisper by running the following command in your terminal.\n",
    "- **Stable Diffusion Model**: The \"runwayml/stable-diffusion-v1-5\" model is utilized for image generation and is accessed through the `diffusers` library with a designated cache directory.\n",
    "\n",
    "\n",
    "### **Create and Activate the Virtual Environment**\n",
    "\n",
    "1. Open a terminal or command prompt within the Jupyter Notebook environment (File -> New -> Terminal) and ensure compatibility with bash commands.\n",
    "2. Navigate to the project directory where the notebook is located.\n",
    "3. Execute the following commands to create and activate a virtual environment:\n",
    "\n",
    "```bash\n",
    "export PROJECT_NAME=\"Lyrical-Video-Generator\"\n",
    "export PIP_CACHE_DIR=`pwd`/.cache/pip\n",
    "mkdir -p $PIP_CACHE_DIR\n",
    "python -m venv --system-site-packages myvenv\n",
    "source myvenv/bin/activate\n",
    "pip install ipykernel\n",
    "python -m ipykernel install --user --name=${PROJECT_NAME}-myvenv --display-name=\"Python (${PROJECT_NAME}-myvenv)\"\n",
    "echo \"\"; echo \"Before continuing load the created Python kernel: Python (${PROJECT_NAME}-myvenv)\"\n",
    "```\n",
    "To enhance the functionality of the CoreAI  environment, we need to install some libraries not pre-installed but required for this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Libraries:\n",
    "\n",
    "Before running the following command in jupyter notebook, make sure you are in the directory where the Jupyter Notebook and virtual environment is located. This ensures the ./ path is always current. You can use the cd command to change to your project directory and pwd to verify your current directory.\n",
    "\n",
    "Load the `Python (Lyrical-Video-Generator-myvenv)` kernel before running the below cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def set_env_with_cache_dir(env_var_name: str, subdir: str):\n",
    "    base_cache = os.path.join(os.getcwd(), \".cache\")\n",
    "    full_path = os.path.join(base_cache, subdir)\n",
    "    os.environ[env_var_name] = full_path\n",
    "    os.makedirs(full_path, exist_ok=True)\n",
    "\n",
    "set_env_with_cache_dir(\"PIP_CACHE_DIR\", \"pip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!. ./myvenv/bin/activate; pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Imports and Initial Setup\n",
    "This section imports all the essential Python modules and sets up the project's directory structure. It includes imports for AI models, video and image processing, and utility functions. By defining paths for input, output, and temporary files, it organizes resources for the entire workflow. Suppressing warnings and configuring environment variables also help keep notebook output clean and focused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import shutil\n",
    "import torch\n",
    "import whisper\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# Handle MoviePy imports with fallback for compatibility\n",
    "try:\n",
    "    from moviepy import (\n",
    "        VideoFileClip, AudioFileClip, ImageClip,\n",
    "        CompositeVideoClip, TextClip, ColorClip,\n",
    "        concatenate_videoclips\n",
    "    )\n",
    "    print(\"MoviePy imported successfully\")\n",
    "except ImportError:\n",
    "    try:\n",
    "        from moviepy.editor import (\n",
    "            VideoFileClip, AudioFileClip, ImageClip,\n",
    "            CompositeVideoClip, TextClip, ColorClip,\n",
    "            concatenate_videoclips\n",
    "        )\n",
    "        print(\"MoviePy editor imported successfully\")\n",
    "    except ImportError as e:\n",
    "        print(f\"MoviePy import failed: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define directory paths for input, output, and temporary storage\n",
    "INPUT_DIRECTORY = Path(\"./input\")\n",
    "OUTPUT_DIRECTORY = Path(\"./working\")\n",
    "TEMPORARY_DIRECTORY = Path(\"./temp\")\n",
    "MODEL_STORAGE_DIRECTORY = Path(\"./working/model_cache\")\n",
    "\n",
    "# Set environment variable for ImageMagick binary\n",
    "os.environ[\"IMAGEMAGICK_BINARY\"] = \"convert\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place your input file in the folder of this notebook and update the variable below\n",
    "DEFAULT_AUDIO_TRACK = \"./input.mp3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions for File Management\n",
    "Here, helper functions are defined to manage temporary files and calculate audio durations. These utilities automate cleanup, reclaim disk space, and prevent clutter from intermediate files. They also provide robust error handling for file operations. This ensures efficient resource management throughout the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def clear_temporary_files(file_pattern=None):\n",
    "    \"\"\"\n",
    "    Delete temporary files to reclaim disk space.\n",
    "    \n",
    "    Args:\n",
    "        file_pattern (str, optional): Glob pattern to match specific temporary files.\n",
    "                                      If None, clears entire temporary directory.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if file_pattern:\n",
    "            for file_path in Path(TEMPORARY_DIRECTORY).glob(file_pattern):\n",
    "                os.remove(file_path)\n",
    "        else:\n",
    "            shutil.rmtree(TEMPORARY_DIRECTORY, ignore_errors=True)\n",
    "            os.makedirs(TEMPORARY_DIRECTORY, exist_ok=True)\n",
    "        gc.collect()\n",
    "    except Exception as e:\n",
    "        print(f\"Error clearing temporary files: {e}\")\n",
    "\n",
    "# Initialize necessary directories\n",
    "os.makedirs(TEMPORARY_DIRECTORY, exist_ok=True)\n",
    "os.makedirs(MODEL_STORAGE_DIRECTORY, exist_ok=True)\n",
    "\n",
    "def calculate_audio_length(audio_file_path):\n",
    "    \"\"\"\n",
    "    Calculate the duration of an audio file in seconds.\n",
    "    \n",
    "    Args:\n",
    "        audio_file_path (str): Path to the audio file.\n",
    "    \n",
    "    Returns:\n",
    "        float: Duration of the audio in seconds, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        audio_clip = AudioFileClip(audio_file_path)\n",
    "        duration = audio_clip.duration\n",
    "        audio_clip.close()\n",
    "        return duration\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating audio duration: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Transcription with Full Coverage\n",
    "This section transcribes the input audio file into text segments using OpenAI Whisper, processing the audio in fixed-duration chunks. It ensures that every part of the audio is covered, inserting placeholder text for instrumental or silent sections. The transcription process includes multiple quality checks to minimize errors and hallucinations. The output is a detailed mapping of lyrics and timings for later video synchronization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def transcribe_audio_ensured_coverage(audio_file_path, segment_length=10):\n",
    "    \"\"\"\n",
    "    Transcribe audio into text segments with forced English language detection,\n",
    "    ensuring full coverage by processing in fixed-duration chunks.\n",
    "    \n",
    "    Args:\n",
    "        audio_file_path (str): Path to the input audio file.\n",
    "        segment_length (int): Duration of each audio chunk in seconds.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing transcribed chunks with timestamps.\n",
    "    \"\"\"\n",
    "    print(f\"Transcribing audio with forced English in {segment_length}-second segments...\")\n",
    "    \n",
    "    audio_duration = calculate_audio_length(audio_file_path)\n",
    "    if not audio_duration:\n",
    "        return {\"chunks\": []}\n",
    "    \n",
    "    print(f\"Total audio duration: {audio_duration:.2f} seconds\")\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    transcription_model = whisper.load_model(\"base\", device=device)\n",
    "    transcribed_segments = []\n",
    "    \n",
    "    for start_time in range(0, int(audio_duration), segment_length):\n",
    "        end_time = min(start_time + segment_length, audio_duration)\n",
    "        print(f\"Processing segment {start_time}-{end_time} seconds...\")\n",
    "        \n",
    "        segment_path = os.path.join(TEMPORARY_DIRECTORY, f\"segment_{start_time}.wav\")\n",
    "        \n",
    "        try:\n",
    "            # Extract audio segment using FFmpeg with normalization\n",
    "            ffmpeg_command = [\n",
    "                'ffmpeg', '-i', audio_file_path,\n",
    "                '-ss', str(start_time), '-t', str(end_time - start_time),\n",
    "                '-acodec', 'pcm_s16le', '-ar', '16000',\n",
    "                '-af', 'volume=2.0',\n",
    "                '-y', segment_path\n",
    "            ]\n",
    "            result = subprocess.run(ffmpeg_command, capture_output=True, text=True)\n",
    "            \n",
    "            if result.returncode != 0:\n",
    "                print(f\"FFmpeg failed for segment {start_time}-{end_time}, using placeholder\")\n",
    "                transcribed_segments.append({\n",
    "                    \"text\": \"♪ ♫ ♪\",\n",
    "                    \"timestamp\": (start_time, end_time),\n",
    "                    \"is_placeholder\": True\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            # Transcribe with settings to prevent hallucination and force English\n",
    "            try:\n",
    "                segment_result = transcription_model.transcribe(\n",
    "                    segment_path,\n",
    "                    word_timestamps=True,\n",
    "                    verbose=False,\n",
    "                    language=\"en\",\n",
    "                    condition_on_previous_text=False,\n",
    "                    temperature=0.0,\n",
    "                    suppress_tokens=[-1, 50257, 50362],\n",
    "                    initial_prompt=\"\",\n",
    "                    task=\"transcribe\"\n",
    "                )\n",
    "                \n",
    "                # Validate and process transcription segments\n",
    "                segment_has_content = False\n",
    "                previous_texts = set()\n",
    "                \n",
    "                for seg in segment_result[\"segments\"]:\n",
    "                    text = seg[\"text\"].strip()\n",
    "                    start = seg.get(\"start\", 0) + start_time\n",
    "                    end = seg.get(\"end\", 0) + start_time\n",
    "                    \n",
    "                    if not text or len(text) < 2:\n",
    "                        continue\n",
    "                    \n",
    "                    text_lower = text.lower()\n",
    "                    if text_lower in previous_texts or len(set(text_lower.replace(' ', ''))) < 4 and len(text) > 10:\n",
    "                        print(f\"Skipping repetitive or hallucinated segment: '{text[:50]}...'\")\n",
    "                        continue\n",
    "                    \n",
    "                    if any(ord(char) > 127 for char in text):\n",
    "                        print(f\"Skipping non-English segment: '{text[:50]}...'\")\n",
    "                        continue\n",
    "                    \n",
    "                    alpha_chars = sum(1 for char in text if char.isalpha())\n",
    "                    if len(text) > 5 and alpha_chars / len(text) < 0.5:\n",
    "                        print(f\"Skipping non-alphabetic segment: '{text[:50]}...'\")\n",
    "                        continue\n",
    "                    \n",
    "                    previous_texts.add(text_lower)\n",
    "                    segment_has_content = True\n",
    "                    \n",
    "                    if \"words\" in seg and seg[\"words\"]:\n",
    "                        for word_info in seg[\"words\"]:\n",
    "                            word_text = word_info.get(\"word\", \"\").strip()\n",
    "                            word_start = word_info.get(\"start\", 0) + start_time\n",
    "                            word_end = word_info.get(\"end\", 0) + start_time\n",
    "                            if word_text and word_start < audio_duration and all(ord(char) <= 127 for char in word_text):\n",
    "                                transcribed_segments.append({\n",
    "                                    \"text\": word_text,\n",
    "                                    \"timestamp\": (word_start, min(word_end, audio_duration))\n",
    "                                })\n",
    "                    else:\n",
    "                        if start < audio_duration:\n",
    "                            transcribed_segments.append({\n",
    "                                \"text\": text,\n",
    "                                \"timestamp\": (start, min(end, audio_duration))\n",
    "                            })\n",
    "                \n",
    "                if not segment_has_content:\n",
    "                    print(f\"No valid content for segment {start_time}-{end_time}, adding placeholder\")\n",
    "                    transcribed_segments.append({\n",
    "                        \"text\": \"♪ ♫ ♪\",\n",
    "                        \"timestamp\": (start_time, end_time),\n",
    "                        \"is_placeholder\": True\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Transcription failed for segment {start_time}-{end_time}: {e}\")\n",
    "                transcribed_segments.append({\n",
    "                    \"text\": \"♪ ♫ ♪\",\n",
    "                    \"timestamp\": (start_time, end_time),\n",
    "                    \"is_placeholder\": True\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing segment {start_time}-{end_time}: {e}\")\n",
    "            transcribed_segments.append({\n",
    "                \"text\": \"♪ ♫ ♪\",\n",
    "                \"timestamp\": (start_time, end_time),\n",
    "                \"is_placeholder\": True\n",
    "            })\n",
    "        finally:\n",
    "            try:\n",
    "                os.remove(segment_path)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    print(f\"→ Extracted {len(transcribed_segments)} segments covering full audio duration\")\n",
    "    return {\"chunks\": transcribed_segments}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment Creation for Visual Coverage\n",
    "Here, the notebook converts transcription results into time-aligned visual segments, guaranteeing that every second of audio has a corresponding visual element. It merges overlapping or adjacent text segments and fills gaps with instrumental placeholders. This step is vital for preventing visual gaps in the final video. The result is a comprehensive sequence of segments ready for image generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_complete_visual_segments(transcription_data, audio_file_path, segment_duration=3.0):\n",
    "    \"\"\"\n",
    "    Ensure every second of audio has corresponding visual content by creating\n",
    "    time-based segments.\n",
    "    \n",
    "    Args:\n",
    "        transcription_data (dict): Dictionary of transcribed audio chunks.\n",
    "        audio_file_path (str): Path to the audio file.\n",
    "        segment_duration (float): Duration of each visual segment in seconds.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of segments with text, start/end times, and instrumental flags.\n",
    "    \"\"\"\n",
    "    print(\"Building segments for complete visual coverage...\")\n",
    "    \n",
    "    audio_duration = calculate_audio_length(audio_file_path)\n",
    "    if audio_duration is None:\n",
    "        return []\n",
    "    \n",
    "    print(f\"Audio duration: {audio_duration:.2f} seconds\")\n",
    "    visual_segments = []\n",
    "    current_time = 0\n",
    "    chunks = sorted(transcription_data[\"chunks\"], key=lambda x: x[\"timestamp\"][0])\n",
    "    chunk_index = 0\n",
    "    \n",
    "    while current_time < audio_duration:\n",
    "        segment_end = min(current_time + segment_duration, audio_duration)\n",
    "        segment_text = \"\"\n",
    "        found_content = False\n",
    "        temp_chunk_index = chunk_index\n",
    "        \n",
    "        while temp_chunk_index < len(chunks) and chunks[temp_chunk_index][\"timestamp\"][0] < segment_end:\n",
    "            chunk = chunks[temp_chunk_index]\n",
    "            chunk_start, chunk_end = chunk[\"timestamp\"]\n",
    "            if chunk_end > current_time:\n",
    "                segment_text += \" \" + chunk[\"text\"]\n",
    "                found_content = True\n",
    "            temp_chunk_index += 1\n",
    "        \n",
    "        while chunk_index < len(chunks) and chunks[chunk_index][\"timestamp\"][1] <= current_time:\n",
    "            chunk_index += 1\n",
    "        \n",
    "        segment_text = segment_text.strip()\n",
    "        is_instrumental = not found_content or not segment_text or segment_text == \"♪ ♫ ♪\"\n",
    "        segment_text = \"♪ ♫ ♪\" if is_instrumental else segment_text\n",
    "        \n",
    "        visual_segments.append({\n",
    "            \"text\": segment_text,\n",
    "            \"start\": current_time,\n",
    "            \"end\": segment_end,\n",
    "            \"is_instrumental\": is_instrumental\n",
    "        })\n",
    "        current_time = segment_end\n",
    "    \n",
    "    print(f\"Created {len(visual_segments)} segments with complete coverage\")\n",
    "    return visual_segments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Image Generation for Visuals\n",
    "In this section, the function generates unique and varied images for each visual segment using Stable Diffusion. It leverages different prompts, styles, and color schemes to avoid repetitive visuals. Instrumental sections receive abstract or music-themed art, while lyric segments are illustrated based on their content. This approach enhances the visual diversity and appeal of the lyric video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def produce_varied_visual_images(segment_data, output_directory, model_id=\"runwayml/stable-diffusion-v1-5\"):\n",
    "    \"\"\"\n",
    "    Generate diverse images for each segment using Stable Diffusion to avoid repetitive visuals.\n",
    "    \n",
    "    Args:\n",
    "        segment_data (list): List of segment dictionaries with text and timing.\n",
    "        output_directory (str): Directory to save generated images.\n",
    "        model_id (str): Identifier for the Stable Diffusion model.\n",
    "    \n",
    "    Returns:\n",
    "        list: Updated segment data with paths to generated images.\n",
    "    \"\"\"\n",
    "    print(\"Generating varied visual images...\")\n",
    "    \n",
    "    diffusion_pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16,\n",
    "        cache_dir=MODEL_STORAGE_DIRECTORY\n",
    "    )\n",
    "    diffusion_pipeline.safety_checker = None\n",
    "    diffusion_pipeline = diffusion_pipeline.to(\"cuda\")\n",
    "    diffusion_pipeline.enable_attention_slicing()\n",
    "    \n",
    "    instrumental_prompts = [\n",
    "        \"Abstract music visualization, flowing sound waves, vibrant colors\",\n",
    "        \"Musical energy patterns, dynamic rhythm visualization\",\n",
    "        \"Sound frequency visualization, colorful audio waves\",\n",
    "        \"Music beats in visual form, pulsing lights and colors\",\n",
    "        \"Abstract representation of musical harmony, flowing patterns\",\n",
    "        \"Rhythmic light patterns, musical atmosphere\",\n",
    "        \"Dynamic color flows representing music energy\",\n",
    "        \"Cosmic music visualization, stars and nebulae dancing\",\n",
    "        \"Electric music waves, neon colors pulsing\",\n",
    "        \"Fluid art representing musical emotions\",\n",
    "        \"Geometric patterns synchronized with music beats\",\n",
    "        \"Particle effects dancing to rhythm\",\n",
    "        \"Crystalline structures resonating with sound\",\n",
    "        \"Fire and water elements dancing to music\",\n",
    "        \"Aurora borealis patterns following musical rhythm\",\n",
    "        \"Digital matrix effects with musical flow\"\n",
    "    ]\n",
    "    \n",
    "    text_styles = [\n",
    "        \"artistic, vibrant, emotional\", \"cinematic, dramatic, colorful\",\n",
    "        \"dreamy, ethereal, beautiful\", \"energetic, dynamic, powerful\",\n",
    "        \"serene, peaceful, flowing\", \"bold, striking, vivid\",\n",
    "        \"mystical, magical, enchanting\", \"futuristic, sci-fi, glowing\",\n",
    "        \"romantic, soft, warm\", \"intense, passionate, fiery\"\n",
    "    ]\n",
    "    \n",
    "    color_schemes = [\n",
    "        \"blue and purple tones\", \"warm orange and red hues\",\n",
    "        \"cool green and teal colors\", \"golden and yellow shades\",\n",
    "        \"pink and magenta tones\", \"silver and white highlights\"\n",
    "    ]\n",
    "    \n",
    "    for index, segment in enumerate(tqdm(segment_data, desc=\"Generating varied images\")):\n",
    "        if segment.get(\"is_instrumental\", False):\n",
    "            base_prompt = instrumental_prompts[index % len(instrumental_prompts)]\n",
    "            color_theme = color_schemes[index % len(color_schemes)]\n",
    "            prompt = f\"{base_prompt}, {color_theme}\"\n",
    "        else:\n",
    "            words = segment['text'].split()\n",
    "            clean_text = ' '.join(words[:4])\n",
    "            style = text_styles[index % len(text_styles)]\n",
    "            color_theme = color_schemes[index % len(color_schemes)]\n",
    "            prompt = f\"Scene: '{clean_text}', {style}, {color_theme}\"\n",
    "        \n",
    "        seed = (hash(prompt) + index * 1337 + int(segment[\"start\"]) * 42) % 10000\n",
    "        generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
    "        \n",
    "        try:\n",
    "            image = diffusion_pipeline(\n",
    "                prompt,\n",
    "                num_inference_steps=25,\n",
    "                height=512,\n",
    "                width=512,\n",
    "                generator=generator\n",
    "            ).images[0]\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating image {index}: {e}\")\n",
    "            fallback_seed = (index * 999) % 10000\n",
    "            fallback_generator = torch.Generator(device=\"cuda\").manual_seed(fallback_seed)\n",
    "            image = diffusion_pipeline(\n",
    "                \"Abstract colorful art\",\n",
    "                num_inference_steps=20,\n",
    "                height=512,\n",
    "                width=512,\n",
    "                generator=fallback_generator\n",
    "            ).images[0]\n",
    "        \n",
    "        if image.mode != \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "        \n",
    "        image_path = os.path.join(output_directory, f\"visual_image_{index:03d}.jpg\")\n",
    "        image.save(image_path, \"JPEG\", quality=90)\n",
    "        segment[\"image_path\"] = image_path\n",
    "        \n",
    "        if index % 5 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    del diffusion_pipeline\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return segment_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Overlay Image Creation\n",
    "This part renders lyric or placeholder text as transparent images using the Python Imaging Library (PIL). The overlays are styled for readability and aesthetics, with options for font size and color. Text is automatically wrapped and centered to fit the video frame. These overlays are later composited onto the generated visuals for the final video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def render_text_overlay(text_content, image_width, image_height, font_size=48, text_color=(255, 255, 255)):\n",
    "    \"\"\"\n",
    "    Render text as an image overlay using PIL for video composition.\n",
    "    \n",
    "    Args:\n",
    "        text_content (str): Text to render.\n",
    "        image_width (int): Width of the output image.\n",
    "        image_height (int): Height of the output image.\n",
    "        font_size (int): Size of the font for text rendering.\n",
    "        text_color (tuple): RGB color tuple for the text.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Array representing the text image with transparency.\n",
    "    \"\"\"\n",
    "    overlay_image = Image.new('RGBA', (image_width, image_height), (0, 0, 0, 0))\n",
    "    draw_context = ImageDraw.Draw(overlay_image)\n",
    "    \n",
    "    try:\n",
    "        text_font = ImageFont.truetype(\"DejaVuSans-Bold.ttf\", font_size)\n",
    "    except:\n",
    "        text_font = ImageFont.load_default()\n",
    "    \n",
    "    if not text_content or text_content.strip() == \"\":\n",
    "        text_content = \" \"\n",
    "    \n",
    "    words = text_content.split()\n",
    "    text_lines = []\n",
    "    current_line = \"\"\n",
    "    \n",
    "    for word in words:\n",
    "        test_line = current_line + \" \" + word if current_line else word\n",
    "        try:\n",
    "            text_box = draw_context.textbbox((0, 0), test_line, font=text_font)\n",
    "            text_width = text_box[2] - text_box[0]\n",
    "        except:\n",
    "            text_width = len(test_line) * font_size * 0.6\n",
    "        \n",
    "        if text_width <= image_width - 40:\n",
    "            current_line = test_line\n",
    "        else:\n",
    "            if current_line:\n",
    "                text_lines.append(current_line)\n",
    "            current_line = word\n",
    "    \n",
    "    if current_line:\n",
    "        text_lines.append(current_line)\n",
    "    \n",
    "    line_height = font_size + 10\n",
    "    total_height = len(text_lines) * line_height\n",
    "    start_y = (image_height - total_height) // 2\n",
    "    \n",
    "    for i, line_text in enumerate(text_lines):\n",
    "        try:\n",
    "            text_box = draw_context.textbbox((0, 0), line_text, font=text_font)\n",
    "            text_width = text_box[2] - text_box[0]\n",
    "        except:\n",
    "            text_width = len(line_text) * font_size * 0.6\n",
    "        \n",
    "        x_position = (image_width - text_width) // 2\n",
    "        y_position = start_y + i * line_height\n",
    "        \n",
    "        outline_width = 2\n",
    "        for offset_x in range(-outline_width, outline_width + 1):\n",
    "            for offset_y in range(-outline_width, outline_width + 1):\n",
    "                if abs(offset_x) + abs(offset_y) <= outline_width:\n",
    "                    draw_context.text(\n",
    "                        (x_position + offset_x, y_position + offset_y),\n",
    "                        line_text, font=text_font, fill=(0, 0, 0)\n",
    "                    )\n",
    "        \n",
    "        draw_context.text(\n",
    "            (x_position, y_position), line_text, font=text_font, fill=text_color\n",
    "        )\n",
    "    \n",
    "    return np.array(overlay_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Assembly with Continuous Playback\n",
    "This section brings together all generated images, text overlays, and the original audio track to create a seamless lyric video. Using MoviePy, it ensures that each segment is synchronized and that there are no gaps in playback. The assembly process includes robust error handling for missing or corrupt files. The final output is a professionally rendered video file ready for sharing or further editing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def assemble_continuous_lyric_video(segment_data, audio_file_path, output_video_path):\n",
    "    \"\"\"\n",
    "    Assemble a continuous lyric video ensuring no gaps in playback.\n",
    "    \n",
    "    Args:\n",
    "        segment_data (list): List of segments with text, timing, and image paths.\n",
    "        audio_file_path (str): Path to the audio file.\n",
    "        output_video_path (str): Path to save the final video.\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the generated video file, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    print(\"Assembling continuous lyric video...\")\n",
    "    \n",
    "    audio_track = AudioFileClip(audio_file_path)\n",
    "    total_duration = audio_track.duration\n",
    "    print(f\"Creating video for full duration of {total_duration:.2f} seconds\")\n",
    "    \n",
    "    video_components = []\n",
    "    \n",
    "    for index, segment in enumerate(segment_data):\n",
    "        try:\n",
    "            segment_duration = segment[\"end\"] - segment[\"start\"]\n",
    "            start_time = segment[\"start\"]\n",
    "            if segment_duration < 0.5:\n",
    "                segment_duration = 0.5\n",
    "            \n",
    "            display_text = segment[\"text\"] if segment[\"text\"] else \"♪\"\n",
    "            is_instrumental = segment.get(\"is_instrumental\", False)\n",
    "            text_color = (255, 215, 0) if is_instrumental else (255, 255, 255)\n",
    "            font_size = 56 if is_instrumental else 48\n",
    "            text_image = render_text_overlay(display_text, 512, 512, font_size, text_color)\n",
    "            \n",
    "            try:\n",
    "                text_component = ImageClip(text_image).with_duration(segment_duration)\n",
    "            except AttributeError:\n",
    "                text_component = ImageClip(text_image).set_duration(segment_duration)\n",
    "            \n",
    "            if \"image_path\" in segment and os.path.exists(segment[\"image_path\"]):\n",
    "                try:\n",
    "                    background_component = ImageClip(segment[\"image_path\"]).with_duration(segment_duration)\n",
    "                except AttributeError:\n",
    "                    background_component = ImageClip(segment[\"image_path\"]).set_duration(segment_duration)\n",
    "            else:\n",
    "                color_options = [(20, 20, 40), (40, 20, 20), (20, 40, 20), (40, 40, 20), (40, 20, 40), (20, 40, 40)]\n",
    "                bg_color = color_options[index % len(color_options)]\n",
    "                try:\n",
    "                    background_component = ColorClip(size=(512, 512), color=bg_color).with_duration(segment_duration)\n",
    "                except AttributeError:\n",
    "                    background_component = ColorClip(size=(512, 512), color=bg_color).set_duration(segment_duration)\n",
    "            \n",
    "            combined_component = CompositeVideoClip([background_component, text_component])\n",
    "            try:\n",
    "                combined_component = combined_component.with_start(start_time)\n",
    "            except AttributeError:\n",
    "                combined_component = combined_component.set_start(start_time)\n",
    "            \n",
    "            video_components.append(combined_component)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing segment {index}: {e}\")\n",
    "            try:\n",
    "                fallback_bg = ColorClip(size=(512, 512), color=(50, 50, 50)).set_duration(1.0).set_start(start_time)\n",
    "                video_components.append(fallback_bg)\n",
    "            except:\n",
    "                pass\n",
    "            continue\n",
    "    \n",
    "    if not video_components:\n",
    "        print(\"Critical error: No video components created. Generating fallback video...\")\n",
    "        fallback_clip = ColorClip(size=(512, 512), color=(50, 50, 50)).set_duration(total_duration)\n",
    "        video_components = [fallback_clip]\n",
    "    \n",
    "    try:\n",
    "        try:\n",
    "            base_background = ColorClip(size=(512, 512), color=(0, 0, 0)).with_duration(total_duration)\n",
    "        except AttributeError:\n",
    "            base_background = ColorClip(size=(512, 512), color=(0, 0, 0)).set_duration(total_duration)\n",
    "        \n",
    "        all_components = [base_background] + video_components\n",
    "        final_composition = CompositeVideoClip(all_components, size=(512, 512))\n",
    "        \n",
    "        try:\n",
    "            final_composition = final_composition.with_audio(audio_track)\n",
    "            final_composition = final_composition.with_duration(total_duration)\n",
    "        except AttributeError:\n",
    "            final_composition = final_composition.set_audio(audio_track)\n",
    "            final_composition = final_composition.set_duration(total_duration)\n",
    "        \n",
    "        print(\"Rendering final video...\")\n",
    "        final_composition.write_videofile(\n",
    "            output_video_path,\n",
    "            fps=24,\n",
    "            codec='libx264',\n",
    "            audio_codec='aac',\n",
    "            bitrate=\"2000k\",\n",
    "            audio_bitrate=\"192k\",\n",
    "            temp_audiofile=os.path.join(TEMPORARY_DIRECTORY, \"temp_audio.m4a\"),\n",
    "            remove_temp=True,\n",
    "            logger=None,\n",
    "            threads=2\n",
    "        )\n",
    "        \n",
    "        final_composition.close()\n",
    "        audio_track.close()\n",
    "        print(f\"Video successfully created at: {output_video_path}\")\n",
    "        return output_video_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error assembling video: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Pipeline for Lyric Video Generation\n",
    "The main pipeline orchestrates the entire lyric video creation process, coordinating transcription, segmentation, image generation, and video assembly. It provides a single entry point for users to generate a complete lyric video with minimal manual intervention. The pipeline manages all intermediate steps and handles errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orchestrate_lyric_video_pipeline(audio_file_path, output_video_path=None, diffusion_model_id=\"runwayml/stable-diffusion-v1-5\"):\n",
    "    \"\"\"\n",
    "    Orchestrate the complete pipeline for generating a continuous lyric video with guaranteed coverage.\n",
    "    \n",
    "    Args:\n",
    "        audio_file_path (str): Path to the input audio file.\n",
    "        output_video_path (str, optional): Path to save the final video. If None, derived from audio file name.\n",
    "        diffusion_model_id (str): Identifier for the Stable Diffusion model to use for image generation.\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the generated video file, or None if the process fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(TEMPORARY_DIRECTORY, exist_ok=True)\n",
    "        \n",
    "        if not os.path.exists(audio_file_path):\n",
    "            print(f\"Error: Audio file not found at {audio_file_path}\")\n",
    "            return None\n",
    "        \n",
    "        if output_video_path is None:\n",
    "            base_name = os.path.basename(audio_file_path)\n",
    "            file_name = os.path.splitext(base_name)[0]\n",
    "            output_video_path = str(OUTPUT_DIRECTORY / f\"{file_name}_continuous_lyric_video.mp4\")\n",
    "        \n",
    "        print(f\"Generating lyric video for: {audio_file_path}\")\n",
    "        \n",
    "        # Step 1: Transcribe audio with full coverage and anti-hallucination measures\n",
    "        transcription_data = transcribe_audio_ensured_coverage(audio_file_path, segment_length=10)\n",
    "        \n",
    "        # Step 2: Build visual segments ensuring complete audio coverage\n",
    "        visual_segments = build_complete_visual_segments(transcription_data, audio_file_path, segment_duration=3.0)\n",
    "        \n",
    "        if not visual_segments:\n",
    "            print(\"Critical error: No visual segments created. Generating fallback coverage...\")\n",
    "            audio_duration = calculate_audio_length(audio_file_path)\n",
    "            visual_segments = [{\n",
    "                \"text\": \"♪ ♫ ♪\",\n",
    "                \"start\": 0,\n",
    "                \"end\": audio_duration,\n",
    "                \"is_instrumental\": True\n",
    "            }]\n",
    "        \n",
    "        # Step 3: Generate dynamic images for each segment with visual variety\n",
    "        visual_segments = produce_varied_visual_images(visual_segments, TEMPORARY_DIRECTORY, diffusion_model_id)\n",
    "        \n",
    "        # Step 4: Assemble the final continuous lyric video\n",
    "        final_video_path = assemble_continuous_lyric_video(visual_segments, audio_file_path, output_video_path)\n",
    "        \n",
    "        return final_video_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in lyric video pipeline: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_lyric_video_demo(audio_file_path=DEFAULT_AUDIO_TRACK, output_video_path=None):\n",
    "    \"\"\"\n",
    "    Execute a demo of the lyric video generator with full coverage.\n",
    "    \n",
    "    Args:\n",
    "        audio_file_path (str): Path to the input audio file. Defaults to a predefined track.\n",
    "        output_video_path (str, optional): Path to save the final video. If None, derived from audio file name.\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the generated video file, or None if the process fails.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(audio_file_path):\n",
    "        print(f\"Error: Audio file not found at {audio_file_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Processing full audio file with guaranteed coverage: {audio_file_path}\")\n",
    "    \n",
    "    if output_video_path is None:\n",
    "        base_name = os.path.basename(audio_file_path)\n",
    "        file_name = os.path.splitext(base_name)[0]\n",
    "        output_video_path = str(OUTPUT_DIRECTORY / f\"{file_name}_continuous_lyric_video.mp4\")\n",
    "    \n",
    "    result = orchestrate_lyric_video_pipeline(\n",
    "        audio_file_path=audio_file_path,\n",
    "        output_video_path=output_video_path,\n",
    "        diffusion_model_id=\"runwayml/stable-diffusion-v1-5\"\n",
    "    )\n",
    "    \n",
    "    if result:\n",
    "        print(f\"Success! lyric video saved to: {result}\")\n",
    "        return result\n",
    "    else:\n",
    "        print(\"Video generation process failed!\")\n",
    "        return None\n",
    "\n",
    "\n",
    "clear_temporary_files()\n",
    "    \n",
    "# Run the demo with the default or specified audio track\n",
    "video_output = execute_lyric_video_demo()\n",
    "    \n",
    "clear_temporary_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Lyrical-Video-Generator-myvenv)",
   "language": "python",
   "name": "lyrical-video-generator-myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
