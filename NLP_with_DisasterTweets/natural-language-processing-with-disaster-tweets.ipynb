{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b21e5e75",
   "metadata": {
    "papermill": {
     "duration": 0.026665,
     "end_time": "2024-03-14T21:49:59.518299",
     "exception": false,
     "start_time": "2024-03-14T21:49:59.491634",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1><center> Natural Language Processing with Disaster Tweets </center></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c881c0-7258-4703-b4af-67a2846b3ce1",
   "metadata": {},
   "source": [
    "## Description of Project\n",
    "\n",
    "This project uses natural language processing techniques to analyze tweet data and classify tweets as related to real-world disasters or not. The aim is to automate the detection of disaster-related communications, which are crucial during emergency response situations.\n",
    "\n",
    "## Required Datasets\n",
    "\n",
    "**Disaster Tweets Dataset**:\n",
    "\n",
    "This project utilizes two primary datasets, `train.csv` and `test.csv`, which should be downloaded and stored in a `data` directory accessible by the notebook.\n",
    "\n",
    "### How to Download Dataset\n",
    "To access and set up the datasets, please follow these steps:\n",
    "\n",
    "1. Create a `data` folder in your project directory if it doesn't already exist.\n",
    "2. Download the `train.csv` and `test.csv` files from the following Kaggle competition link:\n",
    "   - [Natural Language Processing with Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started/data) \n",
    "   - you will need to agree to the terms of the dataset\n",
    "3. Place the downloaded `train.csv` and `test.csv` files into the `data` folder. This step ensures that all data files are ready to be accessed by the notebook.\n",
    "\n",
    "### Additional JSON Resources\n",
    "Alongside the main datasets, the project utilizes two JSON files for text preprocessing. Ensure these files are also placed in the `data` directory:\n",
    "- **`english_contractions_lowercase.json`**: Contains mappings for English contractions to their expanded forms, crucial for normalizing the text data.\n",
    "- **`english_acronyms_lowercase.json`**: Provides mappings for common English acronyms to their full forms, assisting in clarifying the text content.\n",
    "\n",
    "### File Descriptions\n",
    "- **train.csv**: Contains the tweets and labels for training your model. Each row in this file corresponds to a tweet, and includes the text of the tweet along with a binary label indicating whether the tweet is about a real disaster (1) or not (0).\n",
    "- **test.csv**: Contains the tweets for which you will predict the disaster relevance. This file includes only the text of the tweets without any labels.\n",
    "\n",
    "### Setup for Dataset Use\n",
    "After placing the files in the `data` directory, ensure your Jupyter notebook is configured to read from this directory. This might involve setting the correct path to the `data` folder in your data loading script or notebook cells.\n",
    "\n",
    "\n",
    "## Contents of the Notebook\n",
    "\n",
    "- **Introduction**: Overview of the project's aim and importance.\n",
    "- **Basic Exploratory Data Analysis**: Initial analysis of the data to understand the distribution and nature of the dataset.\n",
    "- **Text Normalization**: Processing steps to clean and normalize the text data.\n",
    "- **Bag of N-grams Model**: Implementation of a Bag of N-grams model to predict disaster relevance.\n",
    "- **TF-IDF Model**: Utilization of TF-IDF scores for feature extraction and model training.\n",
    "- **Acknowledgements**: Credits to data providers and contributors.\n",
    "- **References**: Sources and inspirations for the project methodology.\n",
    "\n",
    "## Expected Packages and Resource Requirements\n",
    "\n",
    "**Python Packages**:\n",
    "- numpy\n",
    "- pandas\n",
    "- scikit-learn\n",
    "- nltk\n",
    "- xgboost\n",
    "- seaborn\n",
    "- wordcloud\n",
    "\n",
    "## Install Required Packages\n",
    "\n",
    "   - To enhance the functionality of the CoreAI environment, you may need to install some libraries not pre-installed but required for this notebook. Follow these steps to install the necessary libraries from the `requirements.txt` file:\n",
    "\n",
    "   **Create and Activate the Virtual Environment:**\n",
    "   \n",
    "   Open your terminal or command prompt within the jupyter notebook. `File -> New -> Terminal`\n",
    "   \n",
    "   Navigate to the project directory where you want to set up the environment.\n",
    "   \n",
    "   Execute the following commands in a `bash` to create and activate the virtual environment:\n",
    "   \n",
    "   ```\n",
    "   python3 -m venv --system-site-packages myvenv\n",
    "   source myvenv/bin/activate\n",
    "   pip3 install ipykernel\n",
    "   python -m ipykernel install --user --name=myvenv --display-name=\"Python (myvenv)\"\n",
    "   ```\n",
    "   **Install Required Libraries**\n",
    "   \n",
    "   Before running the following command in jupyter notebook, make sure you are in the directory where the Jupyter Notebook and virtual environment is located. This ensures the ./ path is always current. You can use the cd command to change to your project directory and pwd to verify your current directory.\n",
    "   \n",
    "   ```\n",
    "   !. ./myvenv/bin/activate; pip install -r requirements.txt\n",
    "   ```\n",
    "### Important Note\n",
    "\n",
    "It is crucial to load the new \"myvenv\" kernel for the notebook to work correctly. If the new \"myvenv\" kernel is not loaded, the required libraries and environment settings will not be applied, and the notebook will not function as expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e61136",
   "metadata": {},
   "outputs": [],
   "source": [
    "!. ./myvenv/bin/activate; pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568ae006",
   "metadata": {
    "papermill": {
     "duration": 39.557806,
     "end_time": "2024-03-14T21:50:39.200828",
     "exception": false,
     "start_time": "2024-03-14T21:49:59.643022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import time, os, psutil, random, operator, gc\n",
    "from IPython.display import display, HTML\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import string\n",
    "import re\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, TweetTokenizer, WordPunctTokenizer, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from num2words import num2words\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import spacy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \"use_inf_as_na\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67670f64-b7c3-49ea-8142-f0414a605c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your NLTK data directory within the virtual environment\n",
    "nltk_data_dir = './myvenv/nltk_data'\n",
    "if nltk_data_dir not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_data_dir)\n",
    "\n",
    "# Function to check and download necessary NLTK resources\n",
    "def download_nltk_resources():\n",
    "    resources = {\n",
    "        'corpora/stopwords': 'stopwords',\n",
    "        'taggers/averaged_perceptron_tagger': 'averaged_perceptron_tagger'\n",
    "    }\n",
    "    \n",
    "    for resource_path, resource_name in resources.items():\n",
    "        try:\n",
    "            # Check if the resource is available\n",
    "            nltk.data.find(resource_path)\n",
    "            print(f\"NLTK resource '{resource_name}' is already installed.\")\n",
    "        except LookupError:\n",
    "            # If not present, download the resource to the specified directory\n",
    "            nltk.download(resource_name, download_dir=nltk_data_dir)\n",
    "            print(f\"NLTK resource '{resource_name}' downloaded successfully.\")\n",
    "\n",
    "# Call the function to ensure resources are downloaded\n",
    "download_nltk_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131c8589",
   "metadata": {
    "papermill": {
     "duration": 0.033399,
     "end_time": "2024-03-14T21:50:39.261786",
     "exception": false,
     "start_time": "2024-03-14T21:50:39.228387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Recording the start time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb9c644",
   "metadata": {
    "papermill": {
     "duration": 0.033051,
     "end_time": "2024-03-14T21:50:39.321425",
     "exception": false,
     "start_time": "2024-03-14T21:50:39.288374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting random seeds\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf895c25",
   "metadata": {
    "papermill": {
     "duration": 0.025626,
     "end_time": "2024-03-14T21:50:39.417947",
     "exception": false,
     "start_time": "2024-03-14T21:50:39.392321",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72746220",
   "metadata": {
    "papermill": {
     "duration": 0.025412,
     "end_time": "2024-03-14T21:50:39.469012",
     "exception": false,
     "start_time": "2024-03-14T21:50:39.443600",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1e7110",
   "metadata": {
    "papermill": {
     "duration": 0.026661,
     "end_time": "2024-03-14T21:50:39.521069",
     "exception": false,
     "start_time": "2024-03-14T21:50:39.494408",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Source:** **https://www.kaggle.com/c/nlp-getting-started/data**\n",
    "\n",
    "The training dataset contains information on $7613$ tweets, each with a unique id, keyword (if available), location (if available), text and whether or not the tweet indicates a real disaster or not (expressed via a binary variable). The test dataset contains information on $3263$ tweets with the same features as above except the status of real disaster, which is to be predicted. The features of the dataset are described below.\n",
    "\n",
    "**id** : A unique identifier corresponding to the tweet\n",
    "\n",
    "**keyword** : A highlighting word from the tweet\n",
    "\n",
    "**location** : The location from where the tweet is sent\n",
    "\n",
    "**text**: The textual content of the tweet\n",
    "\n",
    "**target** : A binary variable, which is $0$ if the tweet does not indicate a real disaster and $1$ if it does\n",
    "\n",
    "Note that the features **keyword** and **location** may be blank for many tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c60fdf",
   "metadata": {
    "papermill": {
     "duration": 0.584639,
     "end_time": "2024-03-14T21:50:40.131523",
     "exception": false,
     "start_time": "2024-03-14T21:50:39.546884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The dataset\n",
    "data_train = pd.read_csv('./data/train.csv')\n",
    "data_test = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a2b30f",
   "metadata": {
    "papermill": {
     "duration": 0.054979,
     "end_time": "2024-03-14T21:50:40.213188",
     "exception": false,
     "start_time": "2024-03-14T21:50:40.158209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('The training set contains information on {} tweets.'.format(len(data_train)))\n",
    "data_train[['id', 'text', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43823d4",
   "metadata": {
    "papermill": {
     "duration": 0.045515,
     "end_time": "2024-03-14T21:50:40.288616",
     "exception": false,
     "start_time": "2024-03-14T21:50:40.243101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('The test set contains information on {} tweets.'.format(len(data_test)))\n",
    "data_test_target = data_test.copy()\n",
    "data_test_target['target'] = '?'\n",
    "data_test_target[['id', 'text', 'target']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c892d7b7",
   "metadata": {
    "papermill": {
     "duration": 0.027559,
     "end_time": "2024-03-14T21:50:40.344508",
     "exception": false,
     "start_time": "2024-03-14T21:50:40.316949",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Project Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964d83e3",
   "metadata": {
    "papermill": {
     "duration": 0.028185,
     "end_time": "2024-03-14T21:50:40.401213",
     "exception": false,
     "start_time": "2024-03-14T21:50:40.373028",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**The objective of the project is to predict whether a particular tweet, of which the text (occasionally the keyword and the location as well) is provided, indicates a real disaster or not.**\n",
    "\n",
    "Twitter is one of the most active social media platform that many people use to share occurance of incidents including disasters. For example, if a fire breaks out in a building, many people around the particular location are likely to tweet about the incident. These tweets can send early alerts not only to people in the neighbourhood to evacuate, but also to the appropriate authority to take measures to minimize the loss, potentially saving lives. Thus the tweets indicating real disasters can be utilized for emergency disaster management to remarkable effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce709339",
   "metadata": {
    "papermill": {
     "duration": 0.026697,
     "end_time": "2024-03-14T21:50:40.455078",
     "exception": false,
     "start_time": "2024-03-14T21:50:40.428381",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Evaluation Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5baf84f",
   "metadata": {
    "papermill": {
     "duration": 0.027391,
     "end_time": "2024-03-14T21:50:40.510742",
     "exception": false,
     "start_time": "2024-03-14T21:50:40.483351",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Too much false positives, where a model detects disaster in a tweet that does not indicate any such occurance, may be counterproductive and wasteful in terms of resources. Again, a false negative, where the model fails to detect a disaster from a tweet which actually indicates one, would delay disaster management and clearly costs too much. Observe that, in this problem, the class of tweets that indicate actual disasters (positive class) is more important than the class of tweets not indicating any disaster (negative class). Thus **the goal is to build a model that attempts to minimize the proportion of false positives in the predicted positive class (precision) and that of false negatives in the actual positive class (recall), assigning equal emphasis on both.** Let us denote\n",
    "\n",
    "**TP**: Number of true positives\n",
    "\n",
    "**TN**: Number of true negatives\n",
    "\n",
    "**FP**: Number of false positives\n",
    "\n",
    "**FN**: Number of false negatives\n",
    "\n",
    "**Precision** and **Recall** are universally accepted metrics to capture the performance of a model, when restricted respectively to the **predicted positive class** and the **actual positive class**. These are defined as\n",
    "\n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP}.$$\n",
    "\n",
    "$$\\text{Recall} = \\frac{TP}{TP + FN}.$$\n",
    "\n",
    "The **F1-score** provides a balanced measuring stick by considering the *harmonic mean* of the above two matrices.\n",
    "\n",
    "$$F_1\\text{-Score} = \\frac{2 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}.$$\n",
    "\n",
    "For its equal emphasis on both *precision* and *recall*, *F1-score* is one of the most suitable metrics for evaluating the models in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c597fe",
   "metadata": {
    "papermill": {
     "duration": 0.026517,
     "end_time": "2024-03-14T21:50:40.564285",
     "exception": false,
     "start_time": "2024-03-14T21:50:40.537768",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f685ed58",
   "metadata": {
    "papermill": {
     "duration": 0.026971,
     "end_time": "2024-03-14T21:50:40.618061",
     "exception": false,
     "start_time": "2024-03-14T21:50:40.591090",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next we list the classifiers, along with their specific hyperparameters, that are used in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46ab6c5",
   "metadata": {
    "_kg_hide-input": false,
    "papermill": {
     "duration": 0.042974,
     "end_time": "2024-03-14T21:50:40.688205",
     "exception": false,
     "start_time": "2024-03-14T21:50:40.645231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(penalty = 'l2', dual = False, tol = 0.0001, C = 1.0, fit_intercept = False, intercept_scaling = 1, class_weight = 'balanced', random_state = 0, solver = 'saga', max_iter = 1000, multi_class = 'auto', verbose = 0, warm_start = False, n_jobs = -1, l1_ratio = None)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = math.floor(math.sqrt(len(data_train))), weights = 'uniform', algorithm = 'auto', leaf_size = 30, p = 2, metric = 'minkowski', metric_params = None, n_jobs = -1)\n",
    "\n",
    "dt = DecisionTreeClassifier(criterion = 'entropy', splitter = 'best', max_depth = None, min_samples_split = 4, min_samples_leaf = 1, min_weight_fraction_leaf = 0.0, max_features = None, random_state = 0, max_leaf_nodes = None, min_impurity_decrease = 0.0, class_weight = None, ccp_alpha = 0.0)\n",
    "\n",
    "svm_linear = svm.SVC(C = 2.0, kernel = 'linear', degree = 3, gamma = 'scale', coef0 = 0.0, shrinking = True, probability = False, tol = 0.0001, cache_size = 200, class_weight = 'balanced', verbose = False, max_iter = -1, decision_function_shape = 'ovr', break_ties = False, random_state = 0)\n",
    "\n",
    "svm_rbf = svm.SVC(C = 2.0, kernel = 'rbf', degree = 3, gamma = 'scale', coef0 = 0.0, shrinking = True, probability = False, tol = 0.0001, cache_size = 200, class_weight = 'balanced', verbose = False, max_iter = -1, decision_function_shape = 'ovr', break_ties = False, random_state = 0)\n",
    "\n",
    "nb = GaussianNB()\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', max_depth = 10, min_samples_split = 10, min_samples_leaf = 5, min_weight_fraction_leaf = 0.0, max_features = 'sqrt', max_leaf_nodes = None, min_impurity_decrease = 0.0, bootstrap = True, oob_score = False, n_jobs = -1, random_state = 0, verbose = 0, warm_start = False, class_weight = 'balanced', ccp_alpha = 0.0, max_samples = None)\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(solver = 'svd', shrinkage = None, priors = None, n_components = None, store_covariance = False, tol = 0.0001)\n",
    "\n",
    "sgd = SGDClassifier(loss = 'log_loss', penalty = 'l2', alpha = 0.0001, l1_ratio = 0.15, fit_intercept = True, max_iter = 1000, tol = 0.001, shuffle = True, verbose = 0, epsilon = 0.1, n_jobs = -1, random_state = 0, learning_rate = 'optimal', eta0 = 0.0, power_t = 0.5, early_stopping = False, validation_fraction = 0.1, n_iter_no_change = 5, class_weight = None, warm_start = False, average = False) # loss = 'hinge'\n",
    "\n",
    "ridge = RidgeClassifier(alpha = 2.0, fit_intercept = True, copy_X = True, max_iter = None, tol = 0.001, class_weight = 'balanced', solver = 'auto', random_state = 0)\n",
    "\n",
    "xgb = XGBClassifier(use_label_encoder = False, eval_metric = 'logloss', max_depth = 3, learning_rate = 0.3, n_estimators = 100, base_score = 0.5, random_state = 0, objective = 'binary:logistic', booster = 'gbtree', n_jobs = -1, nthread = None, gamma = 0, min_child_weight = 1, max_delta_step = 0, subsample = 1, colsample_bytree = 1, colsample_bylevel = 1, reg_alpha = 0, reg_lambda = 1, scale_pos_weight = 1, seed = None) #, silent = True\n",
    "\n",
    "ada = AdaBoostClassifier(n_estimators = 100, learning_rate = 1.0, algorithm = 'SAMME.R', random_state = 0) # base_estimator = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b9def6",
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.038017,
     "end_time": "2024-03-14T21:50:40.754778",
     "exception": false,
     "start_time": "2024-03-14T21:50:40.716761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf_list = [logreg, knn, dt, svm_linear, svm_rbf, rf, sgd, ridge, xgb, ada] # nb, lda\n",
    "clf_names = [\"Logistic Regression\", \"KNN Classifier\", \"Decision Tree\", \"SVM (linear kernel)\", \"SVM (RBF kernel)\", \"Random Forest\", \"Stochastic Gradient Descent\", \"Ridge Classifier\", \"XGBoost Classifier\", \"AdaBoost Classifier\"] # \"Naive Bayes\", \"Linear Discriminant Analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011aa551",
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.066812,
     "end_time": "2024-03-14T21:50:40.850402",
     "exception": false,
     "start_time": "2024-03-14T21:50:40.783590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Some Useful Functions\n",
    "\n",
    "# Function to create a list of unique elements of a given list (in order of first appearance)\n",
    "def unique(lst):\n",
    "    list_unique = []\n",
    "    for x in lst:\n",
    "        if x not in list_unique:\n",
    "            list_unique.append(x)\n",
    "    return list_unique\n",
    "\n",
    "# Function to convert float nan values to string\n",
    "def nan_type_conv(lst):\n",
    "    for i in range(len(lst)):\n",
    "        if str(lst[i]) == 'nan':\n",
    "            lst[i] = 'NaN'\n",
    "            \n",
    "# Word finder - Finding out if a specific word exists in a given list of words\n",
    "def word_finder(word, lst):\n",
    "    count = 0\n",
    "    for x in lst:\n",
    "        if x == word:\n",
    "            count += 1\n",
    "            break\n",
    "    if count == 0:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# Word counter basic - Counting a specific word in a given list of words\n",
    "def word_counter(word, lst):\n",
    "    lst_word = [x for x in lst if x == word]\n",
    "    return len(lst_word)\n",
    "\n",
    "# Word counter dictionary - Creating a dictionary of unique words in a given list with their frequencies\n",
    "def word_counter_dict(word_list):\n",
    "    counter_dict = {}\n",
    "    for word in word_list:\n",
    "        if word not in counter_dict.keys():\n",
    "            counter_dict[word] = 1\n",
    "        else:\n",
    "            counter_dict[word] += 1\n",
    "    \n",
    "    counter_dict_sorted = dict(sorted(counter_dict.items(), key = operator.itemgetter(1), reverse = True))\n",
    "    return counter_dict_sorted\n",
    "\n",
    "# Word counter dictionary - Creating a dictionary of unique words in a given list with their relative frequencies\n",
    "def word_counter_dict_relative(word_list):\n",
    "    counter_dict = {}\n",
    "    for word in word_list:\n",
    "        if word not in counter_dict.keys():\n",
    "            counter_dict[word] = 1/len(word_list)\n",
    "        else:\n",
    "            counter_dict[word] += 1/len(word_list)\n",
    "    \n",
    "    counter_dict_sorted = dict(sorted(counter_dict.items(), key = operator.itemgetter(1), reverse = True))\n",
    "    return counter_dict_sorted\n",
    "\n",
    "# Function to convert a given dictionary into a dataframe with given column names\n",
    "def dict_to_df(dictionary, C1, C2):\n",
    "    df = pd.DataFrame(dictionary.items(), columns=[C1, C2])\n",
    "    return df\n",
    "\n",
    "# Word counter dataframe - Creating a dataframe of unique words in a given list with their frequencies\n",
    "def word_counter_df(word_list):\n",
    "    return dict_to_df(word_counter_dict(word_list), \"Word\", \"Frequency\")\n",
    "\n",
    "# Word counter dataframe - Creating a dataframe of unique words in a given list with their relative frequencies\n",
    "def word_counter_df_relative(word_list):\n",
    "    return dict_to_df(word_counter_dict_relative(word_list), \"Word\", \"Relative Frequency\")\n",
    "\n",
    "# Function to convert a given list of pairs into a dictionary\n",
    "def list_to_dict(lst):\n",
    "    dct = {lst[i][0]: lst[i][1] for i in range(len(lst))}\n",
    "    return dct\n",
    "\n",
    "# RegexpTokenizer\n",
    "regexp = RegexpTokenizer(\"[\\w']+\")\n",
    "\n",
    "# List of punctuations except the apostrophe\n",
    "punctuation_list = [w for w in punctuation if w not in [\"'\"]]\n",
    "\n",
    "# Function to compute list of punctuations that are present in a given text\n",
    "def text_punct(text):\n",
    "    x = word_tokenize(text)\n",
    "    punct_list = [w for w in x if w in punctuation_list]\n",
    "    return punct_list\n",
    "\n",
    "# Function to compute list of words that are present in a given list of texts\n",
    "def text_list_words(text_list):\n",
    "    word_list = []\n",
    "    for text in text_list:\n",
    "        word_list = word_list + regexp.tokenize(text)\n",
    "    return word_list\n",
    "\n",
    "# Function to compute list of punctuations that are present in a given list of texts\n",
    "def text_list_punct(text_list):\n",
    "    punct_list = []\n",
    "    for text in text_list:\n",
    "        punct_list = punct_list + text_punct(text)\n",
    "    return punct_list\n",
    "\n",
    "# Function to compute count per text of all unique words in a given list of texts\n",
    "def word_count_per_text(text_list):\n",
    "    word_list = text_list_words(text_list) # list of words\n",
    "    word_count_dict = word_counter_dict(word_list) # dictionary of unique words and frequencies\n",
    "    for word in word_count_dict.keys():\n",
    "        word_count_dict[word] = word_count_dict[word]/len(text_list) # converting frequency to count per sentence\n",
    "    return word_count_dict\n",
    "\n",
    "# Function to produce donutplot\n",
    "def donutplot(value, label, color, title):\n",
    "    fig = plt.figure(figsize = (10, 8))\n",
    "    ax = fig.add_axes([0.0, 0.1, 1.0, 0.5], aspect = 1)\n",
    "    pie = ax.pie(value, colors = color, autopct = \"%1.1f%%\", startangle = 90) # labels = label\n",
    "    centre_circle = plt.Circle((0, 0), 0.8, fc = \"white\")\n",
    "    fig = plt.gcf()\n",
    "    fig.gca().add_artist(centre_circle)\n",
    "    fig.suptitle(title, y = 0.65, fontsize = 12)\n",
    "    plt.legend(pie[0], label, loc = \"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "# Function to visualise classwise comparison - joint barplot\n",
    "def classwise_comparison_barplot(df, n, feature, non_disaster, disaster, xlabel, ylabel, title):\n",
    "\n",
    "    labels = df.head(n).iloc[::-1][feature]\n",
    "    feature_non_disaster = df.head(n).iloc[::-1][non_disaster]\n",
    "    feature_disaster = df.head(n).iloc[::-1][disaster]\n",
    "\n",
    "    location = np.arange(len(labels)) # location points of the labels\n",
    "    width = 0.35 # width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(13, 13)\n",
    "    bar1 = ax.barh(location - (width / 2), feature_non_disaster, width, label = \"Non-disaster tweets\")\n",
    "    bar2 = ax.barh(location + (width / 2), feature_disaster, width, label = \"Disaster tweets\")\n",
    "\n",
    "    ax.set_xlabel(xlabel, fontsize = 14)\n",
    "    ax.set_ylabel(ylabel, fontsize = 14)\n",
    "    ax.set_title(title, fontsize = 16)\n",
    "    ax.set_yticks(location)\n",
    "    ax.set_yticklabels(labels)\n",
    "    ax.legend()\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation = 0)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Function to visualise classwise comparison of feature distribution - histograms in two separate subplots\n",
    "def classwise_comparison_subplot(feature_train_0, feature_train_1, binwidth, title_0, title_1, ylimit, xlabel, ylabel, suptitle):\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (15, 6.5))\n",
    "\n",
    "    xmin = np.min([feature_train_0.min(), feature_train_1.min()])\n",
    "    xmax = np.max([feature_train_0.max(), feature_train_1.max()])\n",
    "\n",
    "    sns.histplot(feature_train_0, ax = ax1, color = \"green\", binwidth = binwidth)\n",
    "    ax1.set_title(title_0, fontsize = 14)\n",
    "    ax1.set_xlim([xmin - 0.5, xmax + 0.5])\n",
    "    ax1.set_ylim([0, ylimit])\n",
    "    ax1.set_xlabel(xlabel, fontsize = 14)\n",
    "    ax1.set_ylabel(ylabel, fontsize = 14)\n",
    "\n",
    "    sns.histplot(feature_train_1, ax = ax2, color = \"red\", binwidth = binwidth)\n",
    "    ax2.set_title(title_1, fontsize = 14)\n",
    "    ax2.set_xlim([xmin - 0.5, xmax + 0.5])\n",
    "    ax2.set_ylim([0, ylimit])\n",
    "    ax2.set_xlabel(xlabel, fontsize = 14)\n",
    "    ax2.set_ylabel(\"\")\n",
    "\n",
    "    fig.suptitle(suptitle, y = 1.0, fontsize = 16)\n",
    "    plt.show()\n",
    "\n",
    "# Visualization of embedding\n",
    "def plot_embedding(test_data, test_labels): # savepath = \"filename.csv\"\n",
    "        truncated_SVD = TruncatedSVD(n_components = 2)\n",
    "        truncated_SVD.fit(test_data)\n",
    "        scores = truncated_SVD.transform(test_data)\n",
    "        color_mapper = {label:idx for idx, label in enumerate(set(test_labels))}\n",
    "        color_column = [color_mapper[label] for label in test_labels]\n",
    "        colors = [\"red\", \"blue\", \"blue\"]\n",
    "        \n",
    "        plt.scatter(scores[:, 0], scores[:, 1], s = 8, alpha = 0.8, c = test_labels,\n",
    "                        cmap = matplotlib.colors.ListedColormap(colors))\n",
    "        red_patch = mpatches.Patch(color = \"red\", label = \"Non-disaster tweet\")\n",
    "        green_patch = mpatches.Patch(color = \"blue\", label = \"Disaster tweet\")\n",
    "        plt.legend(handles=[red_patch, green_patch], prop={\"size\" : 12})\n",
    "\n",
    "# Confusion matrix\n",
    "def confusion_matrix(y_test, y_pred):\n",
    "    class_names = [\"Non-disaster\", \"Disaster\"]\n",
    "    tick_marks_y = [0.5, 1.5]\n",
    "    tick_marks_x = [0.5, 1.5]\n",
    "    conf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "    conf_matrix_df = pd.DataFrame(conf_matrix, range(2), range(2))\n",
    "    plt.figure(figsize = (6, 4.75))\n",
    "    sns.set(font_scale = 1.4) # label size\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    sns.heatmap(conf_matrix_df, annot = True, annot_kws = {\"size\" : 16}, fmt = 'd') # font size\n",
    "    plt.yticks(tick_marks_y, class_names, rotation = \"vertical\")\n",
    "    plt.xticks(tick_marks_x, class_names, rotation = \"horizontal\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "# F1-score\n",
    "def f1_score(y_test, y_pred):\n",
    "    conf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "    TN = conf_matrix[0, 0]\n",
    "    FP = conf_matrix[0, 1]\n",
    "    FN = conf_matrix[1, 0]\n",
    "    TP = conf_matrix[1, 1]\n",
    "    F1 = TP/(TP + (0.5*(FP + FN)))\n",
    "    return F1\n",
    "\n",
    "# Function to display dataframes side by side\n",
    "def display_side_by_side(dfs:list, captions:list):\n",
    "    \"\"\"Display tables side by side to save vertical space\n",
    "    Input:\n",
    "        dfs: list of pandas.DataFrame\n",
    "        captions: list of table captions\n",
    "    \"\"\"\n",
    "    output = \"\"\n",
    "    combined = dict(zip(captions, dfs))\n",
    "    for caption, df in combined.items():\n",
    "        output += df.style.set_table_attributes(\"style='display:inline'\").set_caption(caption)._repr_html_()\n",
    "        output += \"\\xa0\\xa0\\xa0\"\n",
    "    display(HTML(output))\n",
    "    \n",
    "def avg_f1_score_list(X, y):\n",
    "    cv = RepeatedStratifiedKFold(n_splits = 6, n_repeats = 5, random_state = 0)\n",
    "    cvs = [cross_val_score(clf, X, y, cv = cv, scoring = 'f1').mean() for clf in clf_list]\n",
    "    return cvs\n",
    "\n",
    "# def cv_f1_score_list(X, y):\n",
    "#     return [cross_val_score(clf, X, y, cv = 6, scoring = 'f1').std()/cross_val_score(clf, X_fit_transform, y, cv = 6, scoring = 'f1_micro').mean() for clf in clf_list]\n",
    "    \n",
    "f1_score_max = []\n",
    "def f1_score_df(X, y):\n",
    "    f1_df = pd.DataFrame()\n",
    "    f1_df[\"Classifier\"] = clf_names\n",
    "    f1_df[\"Average F1-score\"] = avg_f1_score_list(X, y)\n",
    "#     f1_df[\"Coefficient of variation\"] = cv_f1_score_list(X, y)\n",
    "    f1_score_max.append(max(f1_df[\"Average F1-score\"]))\n",
    "    return f1_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df8360a",
   "metadata": {
    "papermill": {
     "duration": 0.026261,
     "end_time": "2024-03-14T21:50:40.904132",
     "exception": false,
     "start_time": "2024-03-14T21:50:40.877871",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Basic Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183ec370",
   "metadata": {
    "papermill": {
     "duration": 0.027278,
     "end_time": "2024-03-14T21:50:40.958197",
     "exception": false,
     "start_time": "2024-03-14T21:50:40.930919",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In order to compare the two classes (non-disaster tweets and disaster tweets), we split the training data based on the **target** feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cab199a",
   "metadata": {
    "papermill": {
     "duration": 0.03578,
     "end_time": "2024-03-14T21:50:41.020663",
     "exception": false,
     "start_time": "2024-03-14T21:50:40.984883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Splitting the training data by target\n",
    "data_train_0 = data_train[data_train[\"target\"] == 0]\n",
    "data_train_1 = data_train[data_train[\"target\"] == 1]\n",
    "\n",
    "# Class frequencies\n",
    "print(\"Number of training tweets not indicating real disasters: {}\".format(len(data_train_0)))\n",
    "print(\"Number of training tweets indicating real disasters: {}\".format(len(data_train_1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd584f5a",
   "metadata": {
    "papermill": {
     "duration": 0.026988,
     "end_time": "2024-03-14T21:50:41.075938",
     "exception": false,
     "start_time": "2024-03-14T21:50:41.048950",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10594f2",
   "metadata": {
    "papermill": {
     "duration": 0.190666,
     "end_time": "2024-03-14T21:50:41.294836",
     "exception": false,
     "start_time": "2024-03-14T21:50:41.104170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualization of class frequencies\n",
    "target_frequency = np.array([len(data_train_0), len(data_train_1)])\n",
    "target_label = [\"Not disaster tweets\", \"Disaster tweets\"]\n",
    "target_color = [\"green\", \"red\"]\n",
    "donutplot(value = target_frequency, label = target_label, color = target_color, title = \"Frequency comparison of non-disaster tweets and disaster tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f67da4",
   "metadata": {
    "papermill": {
     "duration": 0.033344,
     "end_time": "2024-03-14T21:50:41.365420",
     "exception": false,
     "start_time": "2024-03-14T21:50:41.332076",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Keyword"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f846ef85",
   "metadata": {
    "papermill": {
     "duration": 0.027922,
     "end_time": "2024-03-14T21:50:41.420918",
     "exception": false,
     "start_time": "2024-03-14T21:50:41.392996",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We visualize the proportion of NaN values for the **keyword** feature, as well as the top keywords (both as per *total count* and *count per tweet*) for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1507e8",
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.205647,
     "end_time": "2024-03-14T21:50:41.654392",
     "exception": false,
     "start_time": "2024-03-14T21:50:41.448745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Keyword - main dataframe\n",
    "keyword = list(data_train[\"keyword\"])\n",
    "nan_type_conv(keyword)\n",
    "keyword_unique = unique(keyword)\n",
    "keyword_unique_count = [word_counter(word, keyword) for word in keyword_unique]\n",
    "\n",
    "keyword_0 = list(data_train_0[\"keyword\"])\n",
    "nan_type_conv(keyword_0)\n",
    "keyword_0_unique_count = [word_counter(word, keyword_0) for word in keyword_unique]\n",
    "\n",
    "keyword_1 = list(data_train_1[\"keyword\"])\n",
    "nan_type_conv(keyword_1)\n",
    "keyword_1_unique_count = [word_counter(word, keyword_1) for word in keyword_unique]\n",
    "\n",
    "keyword_df = pd.DataFrame()\n",
    "keyword_df[\"keyword\"] = keyword_unique\n",
    "keyword_df[\"count (all tweets)\"] = keyword_unique_count\n",
    "keyword_df[\"proportion (all tweets)\"] = [count/len(keyword) for count in keyword_unique_count]\n",
    "keyword_df[\"count (non-disaster tweets)\"] = keyword_0_unique_count\n",
    "keyword_df[\"proportion (non-disaster tweets)\"] = [count/len(keyword_0) for count in keyword_0_unique_count]\n",
    "keyword_df[\"count (disaster tweets)\"] = keyword_1_unique_count\n",
    "keyword_df[\"proportion (disaster tweets)\"] = [count/len(keyword_1) for count in keyword_1_unique_count]\n",
    "keyword_df[\"absolute difference\"] = abs(keyword_df[\"proportion (disaster tweets)\"] - keyword_df[\"proportion (non-disaster tweets)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4dfd63",
   "metadata": {
    "papermill": {
     "duration": 0.028673,
     "end_time": "2024-03-14T21:50:41.711923",
     "exception": false,
     "start_time": "2024-03-14T21:50:41.683250",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Note:** A lot of keywords contain two words joined by *%20*, which is the URL-encoding of the *space* character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fa84b7",
   "metadata": {
    "papermill": {
     "duration": 0.16646,
     "end_time": "2024-03-14T21:50:41.905816",
     "exception": false,
     "start_time": "2024-03-14T21:50:41.739356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 'NaN' keywords\n",
    "nan_keyword_count = word_counter('NaN', keyword)\n",
    "keyword_frequency = np.array([nan_keyword_count, len(keyword) - nan_keyword_count])\n",
    "keyword_label = [\"Not NaN\", \"NaN\"]\n",
    "keyword_color = [\"green\", \"red\"]\n",
    "donutplot(value = keyword_frequency, label = keyword_label, color = keyword_color, title = \"Frequency comparison of training tweets withs non-NaN keywords and NaN keywords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78ef275",
   "metadata": {
    "papermill": {
     "duration": 0.588042,
     "end_time": "2024-03-14T21:50:42.532321",
     "exception": false,
     "start_time": "2024-03-14T21:50:41.944279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Classwise keyword-count\n",
    "keyword_df_count = keyword_df[[\"keyword\", \"count (all tweets)\", \"count (non-disaster tweets)\", \"count (disaster tweets)\"]].sort_values(by = [\"count (all tweets)\"], ascending = False)\n",
    "keyword_df_count.drop(0, axis = 0, inplace = True) # deleting the rows with keyword NaN\n",
    "\n",
    "classwise_comparison_barplot(df = keyword_df_count,\n",
    "                             n = 20,\n",
    "                             feature = \"keyword\",\n",
    "                             non_disaster = \"count (non-disaster tweets)\",\n",
    "                             disaster = \"count (disaster tweets)\",\n",
    "                             xlabel = \"count of tweets\",\n",
    "                             ylabel = \"keyword\",\n",
    "                             title = \"Top 20 keyword-count (in decreasing order of total count)\"\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0404580f",
   "metadata": {
    "papermill": {
     "duration": 0.027985,
     "end_time": "2024-03-14T21:50:42.589610",
     "exception": false,
     "start_time": "2024-03-14T21:50:42.561625",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Since the size of the two classes are unequal, we cannot directly compare the count of a keyword in non-disaster tweets with the same in disaster tweets. To make valid comparison, we must scale these counts by respective class-sizes to obtain proportions of a keyword in non-disaster tweets and disaster tweets. In particular, the absolute difference of these two quantities can be considered as a measure of ability of a keyword to discriminate between non-disaster tweets and disaster tweets. For instance, if the absolute difference is close to 0, then we cannot infer anything on the status of the tweet based on keyword alone. On the other hand, a high value indicates that the keyword contributes towards classifying the tweet into a particular class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0de2ef0",
   "metadata": {
    "papermill": {
     "duration": 0.568476,
     "end_time": "2024-03-14T21:50:43.186329",
     "exception": false,
     "start_time": "2024-03-14T21:50:42.617853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Classwise keyword-proportion\n",
    "keyword_df_proportion = keyword_df[[\"keyword\", \"proportion (non-disaster tweets)\", \"proportion (disaster tweets)\", \"absolute difference\"]].sort_values(by = [\"absolute difference\"], ascending = False)\n",
    "keyword_df_proportion.drop(0, axis = 0, inplace = True) # deleting the rows with keyword NaN\n",
    "\n",
    "classwise_comparison_barplot(df = keyword_df_proportion,\n",
    "                             n = 20,\n",
    "                             feature = \"keyword\",\n",
    "                             non_disaster = \"proportion (non-disaster tweets)\",\n",
    "                             disaster = \"proportion (disaster tweets)\",\n",
    "                             xlabel = \"keyword\",\n",
    "                             ylabel = \"proportion of tweets\",\n",
    "                             title = \"Top 20 keyword-proportion (in decreasing order of absolute difference)\"\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2794f073",
   "metadata": {
    "papermill": {
     "duration": 0.040739,
     "end_time": "2024-03-14T21:50:43.258570",
     "exception": false,
     "start_time": "2024-03-14T21:50:43.217831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 5 keywords with least absolute difference between proportion in non-disaster tweets and proportion in disaster tweets\n",
    "keyword_df_proportion[\"keyword\"].tail(5).values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026f499f",
   "metadata": {
    "papermill": {
     "duration": 0.031331,
     "end_time": "2024-03-14T21:50:43.321634",
     "exception": false,
     "start_time": "2024-03-14T21:50:43.290303",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Observation:** The $5$ keywords with least absolute difference between their respective proportions in non-disaster tweets and disaster tweets are usually associated with occurances of disasters. Although these words are used in non-disastrous contexts, for example *landslide victory is an election* or *flood of joyful tears* etc, it is still surprising for these to qualify as keywords in the non-disaster tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e18e27",
   "metadata": {
    "papermill": {
     "duration": 0.031541,
     "end_time": "2024-03-14T21:50:43.384481",
     "exception": false,
     "start_time": "2024-03-14T21:50:43.352940",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eed230a",
   "metadata": {
    "papermill": {
     "duration": 0.0307,
     "end_time": "2024-03-14T21:50:43.446830",
     "exception": false,
     "start_time": "2024-03-14T21:50:43.416130",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We visualize the proportion of NaN values for the **location** feature, as well as the top keywords (both as per *total count* and *count per tweet*) for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49bcecb",
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 2.318617,
     "end_time": "2024-03-14T21:50:45.796591",
     "exception": false,
     "start_time": "2024-03-14T21:50:43.477974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Location - main dataframe\n",
    "location = list(data_train[\"location\"])\n",
    "nan_type_conv(location)\n",
    "location_unique = unique(location)\n",
    "location_unique_count = [word_counter(word, location) for word in location_unique]\n",
    "\n",
    "location_0 = list(data_train_0[\"location\"])\n",
    "nan_type_conv(location_0)\n",
    "location_0_unique_count = [word_counter(word, location_0) for word in location_unique]\n",
    "\n",
    "location_1 = list(data_train_1[\"location\"])\n",
    "nan_type_conv(location_1)\n",
    "location_1_unique_count = [word_counter(word, location_1) for word in location_unique]\n",
    "\n",
    "location_df = pd.DataFrame()\n",
    "location_df[\"location\"] = location_unique\n",
    "location_df[\"count (all tweets)\"] = location_unique_count\n",
    "location_df[\"proportion (all tweets)\"] = [count/len(location) for count in location_unique_count]\n",
    "location_df[\"count (non-disaster tweets)\"] = location_0_unique_count\n",
    "location_df[\"proportion (non-disaster tweets)\"] = [count/len(location_0) for count in location_0_unique_count]\n",
    "location_df[\"count (disaster tweets)\"] = location_1_unique_count\n",
    "location_df[\"proportion (disaster tweets)\"] = [count/len(location_1) for count in location_1_unique_count]\n",
    "location_df[\"absolute difference\"] = abs(location_df[\"proportion (disaster tweets)\"] - location_df[\"proportion (non-disaster tweets)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084a4407",
   "metadata": {
    "papermill": {
     "duration": 0.152936,
     "end_time": "2024-03-14T21:50:45.982108",
     "exception": false,
     "start_time": "2024-03-14T21:50:45.829172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 'NaN' locations\n",
    "nan_location_count = word_counter('NaN', location)\n",
    "location_frequency = np.array([nan_location_count, len(location) - nan_location_count])\n",
    "location_label = [\"Not NaN\", \"NaN\"]\n",
    "location_color = [\"green\", \"red\"]\n",
    "donutplot(value = location_frequency, label = location_label, color = location_color, title = \"Frequency comparison of training tweets withs non-NaN locations and NaN locations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f7be2b",
   "metadata": {
    "papermill": {
     "duration": 0.03555,
     "end_time": "2024-03-14T21:50:46.059796",
     "exception": false,
     "start_time": "2024-03-14T21:50:46.024246",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Note:** In the visualizations of classwise comparison of most features, including keyword and location, we produce only a few observations of the feature of interest due to the large number of distinct textual value taken by these features. The selection of these observations are done by considering certain attributes such as *total count* and choosing the top observations according to that attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1feec87",
   "metadata": {
    "papermill": {
     "duration": 0.584565,
     "end_time": "2024-03-14T21:50:46.676283",
     "exception": false,
     "start_time": "2024-03-14T21:50:46.091718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Classwise location-count\n",
    "location_df_count = location_df[[\"location\", \"count (all tweets)\", \"count (non-disaster tweets)\", \"count (disaster tweets)\"]].sort_values(by = [\"count (all tweets)\"], ascending = False)\n",
    "location_df_count.drop(0, axis = 0, inplace = True) # deleting the rows with location NaN\n",
    "\n",
    "classwise_comparison_barplot(df = location_df_count,\n",
    "                             n = 20,\n",
    "                             feature = \"location\",\n",
    "                             non_disaster = \"count (non-disaster tweets)\",\n",
    "                             disaster = \"count (disaster tweets)\",\n",
    "                             xlabel = \"location\",\n",
    "                             ylabel = \"count of tweets\",\n",
    "                             title = \"Top 20 location-count (in decreasing order of total count)\"\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d82f32",
   "metadata": {
    "papermill": {
     "duration": 0.598209,
     "end_time": "2024-03-14T21:50:47.307957",
     "exception": false,
     "start_time": "2024-03-14T21:50:46.709748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Classwise location-proportion\n",
    "location_df_proportion = location_df[[\"location\", \"proportion (non-disaster tweets)\", \"proportion (disaster tweets)\", \"absolute difference\"]].sort_values(by = [\"absolute difference\"], ascending = False)\n",
    "location_df_proportion.drop(0, axis = 0, inplace = True) # deleting the rows with location NaN\n",
    "\n",
    "classwise_comparison_barplot(df = location_df_proportion,\n",
    "                             n = 20,\n",
    "                             feature = \"location\",\n",
    "                             non_disaster = \"proportion (non-disaster tweets)\",\n",
    "                             disaster = \"proportion (disaster tweets)\",\n",
    "                             xlabel = \"location\",\n",
    "                             ylabel = \"proportion of tweets\",\n",
    "                             title = \"Top 20 location-proportion (in decreasing order of absolute difference)\"\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99c209f",
   "metadata": {
    "papermill": {
     "duration": 0.03413,
     "end_time": "2024-03-14T21:50:47.376216",
     "exception": false,
     "start_time": "2024-03-14T21:50:47.342086",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Number of Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661b4b81",
   "metadata": {
    "papermill": {
     "duration": 0.033189,
     "end_time": "2024-03-14T21:50:47.442752",
     "exception": false,
     "start_time": "2024-03-14T21:50:47.409563",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We examine the distribution of number of characters per tweet for both the class of non-disaster tweets and the class of disaster tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e7d770",
   "metadata": {
    "papermill": {
     "duration": 0.044152,
     "end_time": "2024-03-14T21:50:47.521135",
     "exception": false,
     "start_time": "2024-03-14T21:50:47.476983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to visualise classwise comparison of feature distribution - histograms in two separate subplots\n",
    "def classwise_comparison_subplot(feature_train_0, feature_train_1, binwidth, title_0, title_1, ylimit, xlabel, ylabel, suptitle):\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (15, 6.5))\n",
    "\n",
    "    xmin = np.min([feature_train_0.min(), feature_train_1.min()])\n",
    "    xmax = np.max([feature_train_0.max(), feature_train_1.max()])\n",
    "\n",
    "    sns.histplot(feature_train_0, ax = ax1, color = \"green\", binwidth = binwidth)\n",
    "    ax1.set_title(title_0, fontsize = 14)\n",
    "    ax1.set_xlim([xmin - 0.5, xmax + 0.5])\n",
    "    ax1.set_ylim([0, ylimit])\n",
    "    ax1.set_xlabel(xlabel, fontsize = 14)\n",
    "    ax1.set_ylabel(ylabel, fontsize = 14)\n",
    "\n",
    "    sns.histplot(feature_train_1, ax = ax2, color = \"red\", binwidth = binwidth)\n",
    "    ax2.set_title(title_1, fontsize = 14)\n",
    "    ax2.set_xlim([xmin - 0.5, xmax + 0.5])\n",
    "    ax2.set_ylim([0, ylimit])\n",
    "    ax2.set_xlabel(xlabel, fontsize = 14)\n",
    "    ax2.set_ylabel(\"\")\n",
    "\n",
    "    fig.suptitle(suptitle, y = 1.0, fontsize = 16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78e3d92",
   "metadata": {
    "papermill": {
     "duration": 0.618037,
     "end_time": "2024-03-14T21:50:48.174182",
     "exception": false,
     "start_time": "2024-03-14T21:50:47.556145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Distribution of number of characters in tweets\n",
    "data_train_0_char = data_train_0['text'].str.len()\n",
    "data_train_1_char = data_train_1['text'].str.len()\n",
    "\n",
    "classwise_comparison_subplot(feature_train_0 = data_train_0_char,\n",
    "                             feature_train_1 = data_train_1_char,\n",
    "                             binwidth = 5,\n",
    "                             title_0 = \"Tweets not indicating real disasters\",\n",
    "                             title_1 = \"Tweets indicating real disasters\",\n",
    "                             ylimit = 750,\n",
    "                             xlabel = \"Number of characters\",\n",
    "                             ylabel = \"Number of tweets\",\n",
    "                             suptitle = \"Distribution of number of characters in training tweets\"\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b072531d",
   "metadata": {
    "papermill": {
     "duration": 0.033893,
     "end_time": "2024-03-14T21:50:48.245663",
     "exception": false,
     "start_time": "2024-03-14T21:50:48.211770",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Number of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3d92aa",
   "metadata": {
    "papermill": {
     "duration": 0.032412,
     "end_time": "2024-03-14T21:50:48.310704",
     "exception": false,
     "start_time": "2024-03-14T21:50:48.278292",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We examine the distribution of number of words per tweet for both the class of non-disaster tweets and the class of disaster tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4d1778",
   "metadata": {
    "papermill": {
     "duration": 0.85885,
     "end_time": "2024-03-14T21:50:49.202293",
     "exception": false,
     "start_time": "2024-03-14T21:50:48.343443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Distribution of number of words in tweets\n",
    "data_train_0_word = data_train_0[\"text\"].str.split().map(lambda x: len(x))\n",
    "data_train_1_word = data_train_1[\"text\"].str.split().map(lambda x: len(x))\n",
    "\n",
    "classwise_comparison_subplot(feature_train_0 = data_train_0_word,\n",
    "                             feature_train_1 = data_train_1_word,\n",
    "                             binwidth = 1,\n",
    "                             title_0 = \"Tweets not indicating real disasters\",\n",
    "                             title_1 = \"Tweets indicating real disasters\",\n",
    "                             ylimit = 300,\n",
    "                             xlabel = \"Number of words\",\n",
    "                             ylabel = \"Number of tweets\",\n",
    "                             suptitle = \"Distribution of number of words in training tweets\"\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf2b329",
   "metadata": {
    "papermill": {
     "duration": 0.03622,
     "end_time": "2024-03-14T21:50:49.273823",
     "exception": false,
     "start_time": "2024-03-14T21:50:49.237603",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Length of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf18eb1",
   "metadata": {
    "papermill": {
     "duration": 0.036032,
     "end_time": "2024-03-14T21:50:49.346090",
     "exception": false,
     "start_time": "2024-03-14T21:50:49.310058",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next we analyze the distribution of average word-length in tweets for both the class of non-disaster tweets and the class of disaster tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4005be14",
   "metadata": {
    "papermill": {
     "duration": 0.709708,
     "end_time": "2024-03-14T21:50:50.091657",
     "exception": false,
     "start_time": "2024-03-14T21:50:49.381949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Distribution of average word-length in tweets\n",
    "data_train_0_avg = data_train_0[\"text\"].str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x))\n",
    "data_train_1_avg = data_train_1[\"text\"].str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x))\n",
    "\n",
    "classwise_comparison_subplot(feature_train_0 = data_train_0_avg,\n",
    "                             feature_train_1 = data_train_1_avg,\n",
    "                             binwidth = 0.5,\n",
    "                             title_0 = \"Tweets not indicating real disasters\",\n",
    "                             title_1 = \"Tweets indicating real disasters\",\n",
    "                             ylimit = 700,\n",
    "                             xlabel = \"Number of words\",\n",
    "                             ylabel = \"Number of tweets\",\n",
    "                             suptitle = \"Distribution of length of words in training tweets\"\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40fc045",
   "metadata": {
    "papermill": {
     "duration": 0.037828,
     "end_time": "2024-03-14T21:50:50.167886",
     "exception": false,
     "start_time": "2024-03-14T21:50:50.130058",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6001c716",
   "metadata": {
    "papermill": {
     "duration": 0.037993,
     "end_time": "2024-03-14T21:50:50.244414",
     "exception": false,
     "start_time": "2024-03-14T21:50:50.206421",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We examine the distribution of number of URLs per tweet for both the class of non-disaster tweets and the class of disaster tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d900dbad",
   "metadata": {
    "papermill": {
     "duration": 0.536841,
     "end_time": "2024-03-14T21:50:50.819397",
     "exception": false,
     "start_time": "2024-03-14T21:50:50.282556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Distribution of number of urls in tweets\n",
    "url_train_0_count = data_train_0[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
    "url_train_1_count = data_train_1[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
    "\n",
    "classwise_comparison_subplot(feature_train_0 = url_train_0_count,\n",
    "                             feature_train_1 = url_train_1_count,\n",
    "                             binwidth = 1,\n",
    "                             title_0 = \"Tweets not indicating real disasters\",\n",
    "                             title_1 = \"Tweets indicating real disasters\",\n",
    "                             ylimit = 3000,\n",
    "                             xlabel = \"Number of URLs\",\n",
    "                             ylabel = \"Number of tweets\",\n",
    "                             suptitle = \"Distribution of number of URLs in training tweets\"\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693da126",
   "metadata": {
    "papermill": {
     "duration": 0.037401,
     "end_time": "2024-03-14T21:50:50.898397",
     "exception": false,
     "start_time": "2024-03-14T21:50:50.860996",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Hashtags (#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a116cc1c",
   "metadata": {
    "papermill": {
     "duration": 0.036432,
     "end_time": "2024-03-14T21:50:50.972489",
     "exception": false,
     "start_time": "2024-03-14T21:50:50.936057",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We examine the distribution of number of hashtags per tweet for both the class of non-disaster tweets and the class of disaster tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b0aaa8",
   "metadata": {
    "papermill": {
     "duration": 0.607154,
     "end_time": "2024-03-14T21:50:51.616180",
     "exception": false,
     "start_time": "2024-03-14T21:50:51.009026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Distribution of number of hashtags in tweets\n",
    "hashtag_train_0_count = data_train_0[\"text\"].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
    "hashtag_train_1_count = data_train_1[\"text\"].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
    "\n",
    "classwise_comparison_subplot(feature_train_0 = hashtag_train_0_count,\n",
    "                             feature_train_1 = hashtag_train_1_count,\n",
    "                             binwidth = 1,\n",
    "                             title_0 = \"Tweets not indicating real disasters\",\n",
    "                             title_1 = \"Tweets indicating real disasters\",\n",
    "                             ylimit = 3800,\n",
    "                             xlabel = \"Number of hashtags\",\n",
    "                             ylabel = \"Number of tweets\",\n",
    "                             suptitle = \"Distribution of number of hashtags in training tweets\"\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4c77ac",
   "metadata": {
    "papermill": {
     "duration": 0.039414,
     "end_time": "2024-03-14T21:50:51.695106",
     "exception": false,
     "start_time": "2024-03-14T21:50:51.655692",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Mentions (@)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5749c738",
   "metadata": {
    "papermill": {
     "duration": 0.039474,
     "end_time": "2024-03-14T21:50:51.774105",
     "exception": false,
     "start_time": "2024-03-14T21:50:51.734631",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We examine the distribution of number of mentions per tweet for both the class of non-disaster tweets and the class of disaster tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766bbddc",
   "metadata": {
    "papermill": {
     "duration": 0.625426,
     "end_time": "2024-03-14T21:50:52.439733",
     "exception": false,
     "start_time": "2024-03-14T21:50:51.814307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Distribution of number of mentions in tweets\n",
    "mention_train_0_count = data_train_0[\"text\"].apply(lambda x: len([c for c in str(x) if c == '@']))\n",
    "mention_train_1_count = data_train_1[\"text\"].apply(lambda x: len([c for c in str(x) if c == '@']))\n",
    "\n",
    "classwise_comparison_subplot(feature_train_0 = mention_train_0_count,\n",
    "                             feature_train_1 = mention_train_1_count,\n",
    "                             binwidth = 1,\n",
    "                             title_0 = \"Tweets not indicating real disasters\",\n",
    "                             title_1 = \"Tweets indicating real disasters\",\n",
    "                             ylimit = 3200,\n",
    "                             xlabel = \"Number of mentions\",\n",
    "                             ylabel = \"Number of tweets\",\n",
    "                             suptitle = \"Distribution of number of mentions in training tweets\"\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb120840",
   "metadata": {
    "papermill": {
     "duration": 0.04015,
     "end_time": "2024-03-14T21:50:57.917653",
     "exception": false,
     "start_time": "2024-03-14T21:50:57.877503",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Text Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67fd7eb",
   "metadata": {
    "papermill": {
     "duration": 0.039579,
     "end_time": "2024-03-14T21:50:57.997207",
     "exception": false,
     "start_time": "2024-03-14T21:50:57.957628",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We consider a number of [**text normalization**](https://en.wikipedia.org/wiki/Text_normalization) processes, namely conversion to [**lowercase**](https://en.wikipedia.org/wiki/Letter_case), removal of [**whitespaces**](https://en.wikipedia.org/wiki/Whitespace_character), removal of [**punctuations**](https://en.wikipedia.org/wiki/Punctuation), removal of [**unicode characters**](https://en.wikipedia.org/wiki/List_of_Unicode_characters) (including [**HTML**](https://en.wikipedia.org/wiki/HTML) tags, [**emojis**](https://en.wikipedia.org/wiki/Emoji), and [**URL**](https://en.wikipedia.org/wiki/URL)s starting with [**http**](https://en.wikipedia.org/wiki/HTTP)), substitution of [**acronyms**](https://en.wikipedia.org/wiki/Acronym), substitution of [**contractions**](https://en.wikipedia.org/wiki/Contraction_(grammar)), removal of [**stop words**](https://en.wikipedia.org/wiki/Stop_word), [**spelling**](https://en.wikipedia.org/wiki/Spelling) correction, [**stemming**](https://en.wikipedia.org/wiki/Stemming), [**lemmatization**](https://en.wikipedia.org/wiki/Lemmatization), discardment of non-alphabetic words, and retention of relevant [**parts of speech**](https://en.wikipedia.org/wiki/Part_of_speech). At the end of the section, we combine all the processes into one single function and apply it on the training tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5a67ad",
   "metadata": {
    "papermill": {
     "duration": 0.039864,
     "end_time": "2024-03-14T21:50:58.076791",
     "exception": false,
     "start_time": "2024-03-14T21:50:58.036927",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Convertion to Lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b1fa27",
   "metadata": {
    "papermill": {
     "duration": 0.040353,
     "end_time": "2024-03-14T21:50:58.157675",
     "exception": false,
     "start_time": "2024-03-14T21:50:58.117322",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We convert all alphabetical characters of the tweets to lowercase so that the models do not differentiate identical words due to case-sensitivity. For example, without the normalization, *Sun* and *sun* would have been treated as two different words, which is not useful in the present context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e520d45",
   "metadata": {
    "papermill": {
     "duration": 0.046935,
     "end_time": "2024-03-14T21:50:58.244939",
     "exception": false,
     "start_time": "2024-03-14T21:50:58.198004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Converting to lowercase\n",
    "def convert_to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "text = \"This is a FUNCTION that CoNvErTs a Text to lowercase\"\n",
    "print(\"Input: {}\".format(text))\n",
    "print(\"Output: {}\".format(convert_to_lowercase(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86934149",
   "metadata": {
    "papermill": {
     "duration": 0.040513,
     "end_time": "2024-03-14T21:50:58.325030",
     "exception": false,
     "start_time": "2024-03-14T21:50:58.284517",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Removal of Whitespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e838b6f",
   "metadata": {
    "papermill": {
     "duration": 0.039085,
     "end_time": "2024-03-14T21:50:58.404131",
     "exception": false,
     "start_time": "2024-03-14T21:50:58.365046",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We remove the unnecessary empty spaces from the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4288e129",
   "metadata": {
    "papermill": {
     "duration": 0.048007,
     "end_time": "2024-03-14T21:50:58.491765",
     "exception": false,
     "start_time": "2024-03-14T21:50:58.443758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Removing whitespaces\n",
    "def remove_whitespace(text):\n",
    "    return text.strip()\n",
    "\n",
    "text = \" \\t This is a string \\t \"\n",
    "print(\"Input: {}\".format(text))\n",
    "print(\"Output: {}\".format(remove_whitespace(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d89b7dd",
   "metadata": {
    "papermill": {
     "duration": 0.039443,
     "end_time": "2024-03-14T21:50:58.570722",
     "exception": false,
     "start_time": "2024-03-14T21:50:58.531279",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Removal of Punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2453d0",
   "metadata": {
    "papermill": {
     "duration": 0.039023,
     "end_time": "2024-03-14T21:50:58.649156",
     "exception": false,
     "start_time": "2024-03-14T21:50:58.610133",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Mostly the punctuations do not play any role in predicting whether a particular tweet indicate disaster or not. Thus we prevent them from contaminating the classification procedures by removing them from the tweets. However, we keep **apostrophe** since most of the contractions contain this punctuation and will be automatically taken care of once we convert the contractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8e4976",
   "metadata": {
    "papermill": {
     "duration": 0.047827,
     "end_time": "2024-03-14T21:50:58.736352",
     "exception": false,
     "start_time": "2024-03-14T21:50:58.688525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Removing punctuations\n",
    "def remove_punctuation(text):\n",
    "    punct_str = string.punctuation\n",
    "    punct_str = punct_str.replace(\"'\", \"\") # discarding apostrophe from the string to keep the contractions intact\n",
    "    return text.translate(str.maketrans(\"\", \"\", punct_str))\n",
    "\n",
    "text = \"Here's [an] example? {of} &a string. with.? punctuation!!!!\"\n",
    "print(\"Input: {}\".format(text))\n",
    "print(\"Output: {}\".format(remove_punctuation(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb1bfaa",
   "metadata": {
    "papermill": {
     "duration": 0.040267,
     "end_time": "2024-03-14T21:50:58.816974",
     "exception": false,
     "start_time": "2024-03-14T21:50:58.776707",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Removal of Unicode Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ca0006",
   "metadata": {
    "papermill": {
     "duration": 0.039544,
     "end_time": "2024-03-14T21:50:58.936768",
     "exception": false,
     "start_time": "2024-03-14T21:50:58.897224",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The training tweets are typically sprinkled with emojis, URLs, punctuation and other symbols that do not contribute meaningfully to our analysis, but instead create noise in the learning procedure. Some of these symbols are unique, while the rest usually translate into unicode strings. We remove these irrelevant characters from the data using the regular expression module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12912d6",
   "metadata": {
    "papermill": {
     "duration": 0.047723,
     "end_time": "2024-03-14T21:50:59.024289",
     "exception": false,
     "start_time": "2024-03-14T21:50:58.976566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Removing HTML tags\n",
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>')\n",
    "    return html.sub(r'', text)\n",
    "\n",
    "text = '<a href = \"https://www.kaggle.com/c/nlp-getting-started/overview\"> Natural Language Processing with Disaster Tweets </a>'\n",
    "print(\"Input: {}\".format(text))\n",
    "print(\"Output: {}\".format(remove_html(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7e17a2",
   "metadata": {
    "papermill": {
     "duration": 0.051718,
     "end_time": "2024-03-14T21:50:59.117702",
     "exception": false,
     "start_time": "2024-03-14T21:50:59.065984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Removing emojis\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "text = \"Just happened a terrible car crash 😟\"\n",
    "print(\"Input: {}\".format(text))\n",
    "print(\"Output: {}\".format(remove_emoji(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9cc6ec",
   "metadata": {
    "papermill": {
     "duration": 0.053402,
     "end_time": "2024-03-14T21:50:59.211832",
     "exception": false,
     "start_time": "2024-03-14T21:50:59.158430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Removing other unicode characters\n",
    "def remove_http(text):\n",
    "    http = \"https?://\\S+|www\\.\\S+\" # matching strings beginning with http (but not just \"http\")\n",
    "    pattern = r\"({})\".format(http) # creating pattern\n",
    "    return re.sub(pattern, \"\", text)\n",
    "\n",
    "text = \"It's a function that removes links starting with http: or https such as https://en.wikipedia.org/wiki/Unicode_symbols\"\n",
    "print(\"Input: {}\".format(text))\n",
    "print(\"Output: {}\".format(remove_http(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1c3d7e",
   "metadata": {
    "papermill": {
     "duration": 0.040215,
     "end_time": "2024-03-14T21:50:59.293413",
     "exception": false,
     "start_time": "2024-03-14T21:50:59.253198",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Substitution of Acronyms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637012fe",
   "metadata": {
    "papermill": {
     "duration": 0.039463,
     "end_time": "2024-03-14T21:50:59.373254",
     "exception": false,
     "start_time": "2024-03-14T21:50:59.333791",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**What are acronyms?** Acronyms are shortened forms of phrases, generally found in informal writings such as personal messages. For instance, *for your information* is written as *fyi* and *by the way* is written as *btw*. These time and effort-saving acronyms have received almost universal acceptance in social media platforms including twitter. For the sake of proper modeling, we convert the acronyms, appearing in the tweets, back to their respective original forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd20307",
   "metadata": {
    "papermill": {
     "duration": 0.211341,
     "end_time": "2024-03-14T21:50:59.624126",
     "exception": false,
     "start_time": "2024-03-14T21:50:59.412785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dictionary of acronyms\n",
    "acronyms_url = './data/english_acronyms_lowercase.json'\n",
    "acronyms_dict = pd.read_json(acronyms_url, typ = 'series')\n",
    "\n",
    "print(\"Example: Original form of the acronym 'fyi' is '{}'\".format(acronyms_dict[\"fyi\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dbeab9",
   "metadata": {
    "papermill": {
     "duration": 0.051124,
     "end_time": "2024-03-14T21:50:59.716893",
     "exception": false,
     "start_time": "2024-03-14T21:50:59.665769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataframe of acronyms\n",
    "dict_to_df(acronyms_dict, \"acronym\", \"original\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46548c6a",
   "metadata": {
    "papermill": {
     "duration": 0.048073,
     "end_time": "2024-03-14T21:50:59.806577",
     "exception": false,
     "start_time": "2024-03-14T21:50:59.758504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List of acronyms\n",
    "acronyms_list = list(acronyms_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dbaf0a",
   "metadata": {
    "papermill": {
     "duration": 0.04992,
     "end_time": "2024-03-14T21:50:59.897430",
     "exception": false,
     "start_time": "2024-03-14T21:50:59.847510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to convert contractions in a text\n",
    "def convert_acronyms(text):\n",
    "    words = []\n",
    "    for word in regexp.tokenize(text):\n",
    "        if word in acronyms_list:\n",
    "            words = words + acronyms_dict[word].split()\n",
    "        else:\n",
    "            words = words + word.split()\n",
    "    \n",
    "    text_converted = \" \".join(words)\n",
    "    return text_converted\n",
    "\n",
    "text = \"btw you've to fill in the details including dob\"\n",
    "print(\"Input: {}\".format(text))\n",
    "print(\"Output: {}\".format(convert_acronyms(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe21ebf2",
   "metadata": {
    "papermill": {
     "duration": 0.040381,
     "end_time": "2024-03-14T21:50:59.978758",
     "exception": false,
     "start_time": "2024-03-14T21:50:59.938377",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Substitution of Contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49c7581",
   "metadata": {
    "papermill": {
     "duration": 0.041079,
     "end_time": "2024-03-14T21:51:00.060584",
     "exception": false,
     "start_time": "2024-03-14T21:51:00.019505",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**What are contractions?** A contraction is a shortened form of a word or a phrase, obtained by dropping one or more letters.\n",
    "\n",
    "These are commonly used in everyday speech, written dialogue, informal writing and in situations where space is limited or costly, such as advertisements. Usually the missing letters are indicated by an apostrophe, but there are exceptions. Examples: I'm = I am, let's = let us, won't = would not, howdy = how do you do.\n",
    "\n",
    "We have compiled an extensive list of English contractions, which can be found in the attached .json file titled *english_contractions_lowercase*. The list is largely based on information obtained from the wikipedia page on *list of English contractions*. Note that the file only considers contractions in lowercase, i.e. it assumes that the textual data have already been transformed to lowercase before substituting the contractions. For example, the process will convert *i'll* to *i shall* but will leave *I'll* unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708ea98a",
   "metadata": {
    "papermill": {
     "duration": 0.283754,
     "end_time": "2024-03-14T21:51:00.384838",
     "exception": false,
     "start_time": "2024-03-14T21:51:00.101084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dictionary of contractions\n",
    "contractions_url = './data/english_contractions_lowercase.json'\n",
    "contractions_dict = pd.read_json(contractions_url, typ = 'series')\n",
    "\n",
    "print(\"Example: Original form of the contraction 'aren't' is '{}'\".format(contractions_dict[\"aren't\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6775ce",
   "metadata": {
    "papermill": {
     "duration": 0.052633,
     "end_time": "2024-03-14T21:51:00.479411",
     "exception": false,
     "start_time": "2024-03-14T21:51:00.426778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataframe of contractions\n",
    "dict_to_df(contractions_dict, \"contraction\", \"original\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5cdee3",
   "metadata": {
    "papermill": {
     "duration": 0.041192,
     "end_time": "2024-03-14T21:51:00.562218",
     "exception": false,
     "start_time": "2024-03-14T21:51:00.521026",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The contractions do not always have a one-to-one mapping with the original words. For example **i'd** can come from both **i had** and **i would**. In the .json file only one the original words/phrases are chosen. However, this does not affect our analysis since words like **had** and **would**, which do not have any meaningful contribution in achieving the objective of the project, will be discarded in the next subsection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b61f68",
   "metadata": {
    "papermill": {
     "duration": 0.047788,
     "end_time": "2024-03-14T21:51:00.652492",
     "exception": false,
     "start_time": "2024-03-14T21:51:00.604704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List of contractions\n",
    "contractions_list = list(contractions_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9db308",
   "metadata": {
    "papermill": {
     "duration": 0.051326,
     "end_time": "2024-03-14T21:51:00.745551",
     "exception": false,
     "start_time": "2024-03-14T21:51:00.694225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to convert contractions in a text\n",
    "def convert_contractions(text):\n",
    "    words = []\n",
    "    for word in regexp.tokenize(text):\n",
    "        if word in contractions_list:\n",
    "            words = words + contractions_dict[word].split()\n",
    "        else:\n",
    "            words = words + word.split()\n",
    "    \n",
    "    text_converted = \" \".join(words)\n",
    "    return text_converted\n",
    "\n",
    "text = \"he's doin' fine\"\n",
    "print(\"Input: {}\".format(text))\n",
    "print(\"Output: {}\".format(convert_contractions(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4270981",
   "metadata": {
    "papermill": {
     "duration": 0.041861,
     "end_time": "2024-03-14T21:51:00.830217",
     "exception": false,
     "start_time": "2024-03-14T21:51:00.788356",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Removal of Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f988d6e",
   "metadata": {
    "papermill": {
     "duration": 0.042856,
     "end_time": "2024-03-14T21:51:00.915086",
     "exception": false,
     "start_time": "2024-03-14T21:51:00.872230",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Several words, primarily pronouns, prepositions, modal verbs etc, are identified not to have much effect on the classification procedure. To get rid of the unwanted contamination effect, we remove these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a460720",
   "metadata": {
    "papermill": {
     "duration": 0.053559,
     "end_time": "2024-03-14T21:51:01.010563",
     "exception": false,
     "start_time": "2024-03-14T21:51:00.957004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "stops = stopwords.words(\"english\") # stopwords\n",
    "addstops = [\"among\", \"onto\", \"shall\", \"thrice\", \"thus\", \"twice\", \"unto\", \"us\", \"would\"] # additional stopwords\n",
    "allstops = stops + addstops\n",
    "\n",
    "print(allstops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5144a40c",
   "metadata": {
    "papermill": {
     "duration": 0.054078,
     "end_time": "2024-03-14T21:51:01.106761",
     "exception": false,
     "start_time": "2024-03-14T21:51:01.052683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to remove stopwords from a list of texts\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in regexp.tokenize(text) if word not in allstops])\n",
    "\n",
    "text = \"This is a function that removes stopwords in a given text\"\n",
    "print(\"Input: {}\".format(text))\n",
    "print(\"Output: {}\".format(remove_stopwords(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d74139",
   "metadata": {
    "papermill": {
     "duration": 0.043381,
     "end_time": "2024-03-14T21:51:01.194569",
     "exception": false,
     "start_time": "2024-03-14T21:51:01.151188",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Spelling Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6642e2f2",
   "metadata": {
    "papermill": {
     "duration": 0.042355,
     "end_time": "2024-03-14T21:51:01.281062",
     "exception": false,
     "start_time": "2024-03-14T21:51:01.238707",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The classification procedure cannot take mispellings into consideration and treats a word and its misspelt version as separate words. For this reason it is necessary to conduct spelling correction before feeding the data to the classification procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387cc0e1",
   "metadata": {
    "papermill": {
     "duration": 0.176286,
     "end_time": "2024-03-14T21:51:01.500289",
     "exception": false,
     "start_time": "2024-03-14T21:51:01.324003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pyspellchecker\n",
    "spell = SpellChecker()\n",
    "\n",
    "def pyspellchecker(text):\n",
    "    word_list = regexp.tokenize(text)\n",
    "    word_list_corrected = []\n",
    "    for word in word_list:\n",
    "        if word in spell.unknown(word_list):\n",
    "            word_list_corrected.append(spell.correction(word))\n",
    "        else:\n",
    "            word_list_corrected.append(word)\n",
    "    text_corrected = \" \".join(word_list_corrected)\n",
    "    return text_corrected\n",
    "\n",
    "text = \"I'm goinng therre\"\n",
    "print(\"Input: {}\".format(text))\n",
    "print(\"Output: {}\".format(pyspellchecker(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae82ed38",
   "metadata": {
    "papermill": {
     "duration": 0.046911,
     "end_time": "2024-03-14T21:51:01.593743",
     "exception": false,
     "start_time": "2024-03-14T21:51:01.546832",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d51f7c0",
   "metadata": {
    "papermill": {
     "duration": 0.042678,
     "end_time": "2024-03-14T21:51:01.680195",
     "exception": false,
     "start_time": "2024-03-14T21:51:01.637517",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Stemming** is the process of reducing the words to their root form or *stem*. It reduces related words to the same *stem* even if the stem is not a dictionary word. For example, the words *introducing*, *introduced*, *introduction* reduce to a common word *introduce*. However, the process often produces stems that are not actual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16579d2f",
   "metadata": {
    "papermill": {
     "duration": 0.051986,
     "end_time": "2024-03-14T21:51:01.775530",
     "exception": false,
     "start_time": "2024-03-14T21:51:01.723544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "def text_stemmer(text):\n",
    "    text_stem = \" \".join([stemmer.stem(word) for word in regexp.tokenize(text)])\n",
    "    return text_stem\n",
    "\n",
    "text = \"Introducing lemmatization as an improvement over stemming\"\n",
    "print(\"Input: {}\".format(text))\n",
    "print(\"Output: {}\".format(text_stemmer(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229bda43",
   "metadata": {
    "papermill": {
     "duration": 0.042937,
     "end_time": "2024-03-14T21:51:01.861043",
     "exception": false,
     "start_time": "2024-03-14T21:51:01.818106",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The stems *introduc*, *lemmat* and *improv* are not actual words. **Lemmatization** offers a more sophisticated approach by utilizing a corpus to match root forms of the words. Unlike stemming, it uses the context in which a word is being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68524353-cc0b-42a3-8567-411fc1307e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.cli import download\n",
    "\n",
    "def ensure_spacy_model(model_name=\"en_core_web_sm\"):\n",
    "    try:\n",
    "        # Try to load the model to check if it's already installed\n",
    "        spacy.load(model_name)\n",
    "        print(f\"Model '{model_name}' is already installed.\")\n",
    "    except OSError:\n",
    "        # If the model is not installed, download it\n",
    "        print(f\"Model '{model_name}' not found. Downloading...\")\n",
    "        download(model_name)\n",
    "        print(f\"Model '{model_name}' downloaded.\")\n",
    "\n",
    "# Use the function to ensure the model is downloaded\n",
    "ensure_spacy_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab86db9-9238-4ec1-8747-610202522a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "spacy_lemmatizer = spacy.load(\"en_core_web_sm\", disable = ['parser', 'ner'])\n",
    "#lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def text_lemmatizer(text):\n",
    "    text_spacy = \" \".join([token.lemma_ for token in spacy_lemmatizer(text)])\n",
    "    #text_wordnet = \" \".join([lemmatizer.lemmatize(word) for word in word_tokenize(text)]) # regexp.tokenize(text)\n",
    "    return text_spacy\n",
    "    #return text_wordnet\n",
    "\n",
    "text = \"Introducing lemmatization as an improvement over stemming\"\n",
    "print(\"Input: {}\".format(text))\n",
    "print(\"Output: {}\".format(text_lemmatizer(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedc3fcc",
   "metadata": {
    "papermill": {
     "duration": 0.040956,
     "end_time": "2024-03-14T21:51:02.860165",
     "exception": false,
     "start_time": "2024-03-14T21:51:02.819209",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Discardment of Non-alphabetic Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10d3837",
   "metadata": {
    "papermill": {
     "duration": 0.042536,
     "end_time": "2024-03-14T21:51:02.946945",
     "exception": false,
     "start_time": "2024-03-14T21:51:02.904409",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The non-alphabetic words are not numerous and create unnecessary diversions in the context of classifying tweets into non-disaster and disaster categories. Hence we discard these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3163758",
   "metadata": {
    "papermill": {
     "duration": 0.050124,
     "end_time": "2024-03-14T21:51:03.040347",
     "exception": false,
     "start_time": "2024-03-14T21:51:02.990223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Discardment of non-alphabetic words\n",
    "def discard_non_alpha(text):\n",
    "    word_list_non_alpha = [word for word in regexp.tokenize(text) if word.isalpha()]\n",
    "    text_non_alpha = \" \".join(word_list_non_alpha)\n",
    "    return text_non_alpha\n",
    "\n",
    "text = \"It is an ocean of thousands and 1000s of crowd\"\n",
    "print(\"Input: {}\".format(text))\n",
    "print(\"Output: {}\".format(discard_non_alpha(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25dced2",
   "metadata": {
    "papermill": {
     "duration": 0.04279,
     "end_time": "2024-03-14T21:51:03.127492",
     "exception": false,
     "start_time": "2024-03-14T21:51:03.084702",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Parts of Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8872107e",
   "metadata": {
    "papermill": {
     "duration": 0.043522,
     "end_time": "2024-03-14T21:51:03.213685",
     "exception": false,
     "start_time": "2024-03-14T21:51:03.170163",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The **parts of speech** provide a great tool to select a subset of words that are more likely to contribute in the classification procedure and discard the rest to avoid noise. The idea is to select a number of parts of speech that are important to the context of the problem. Then we partition the words in a given text into several subsets corresponding to each part of speech and keep only those subsets corresponding to the selected parts of speech. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a805b80d",
   "metadata": {
    "papermill": {
     "duration": 0.127968,
     "end_time": "2024-03-14T21:51:03.384828",
     "exception": false,
     "start_time": "2024-03-14T21:51:03.256860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "def keep_pos(text):\n",
    "    tokens = regexp.tokenize(text)\n",
    "    tokens_tagged = nltk.pos_tag(tokens)\n",
    "    # keep_tags = ['NN', 'NNS', 'NNP', 'NNPS', 'FW']\n",
    "    keep_tags = ['NN', 'NNS', 'NNP', 'NNPS', 'FW', 'PRP', 'PRPS', 'RB', 'RBR', 'RBS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WPS', 'WRB']\n",
    "    keep_words = [x[0] for x in tokens_tagged if x[1] in keep_tags]\n",
    "    return \" \".join(keep_words)\n",
    "\n",
    "text = \"He arrived at seven o'clock on Wednesday evening\"\n",
    "print(\"Input: {}\".format(text))\n",
    "tokens = regexp.tokenize(text)\n",
    "print(\"Tokens: {}\".format(tokens))\n",
    "tokens_tagged = nltk.pos_tag(tokens)\n",
    "print(\"Tagged Tokens: {}\".format(tokens_tagged))\n",
    "print(\"Output: {}\".format(keep_pos(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb1db82",
   "metadata": {
    "papermill": {
     "duration": 0.044426,
     "end_time": "2024-03-14T21:51:03.474440",
     "exception": false,
     "start_time": "2024-03-14T21:51:03.430014",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For an extensive list of part-of-speech tags, see [**alphabetical list of part-of-speech tags used in the Penn Treebank Project**](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d292d1b6",
   "metadata": {
    "papermill": {
     "duration": 0.04007,
     "end_time": "2024-03-14T21:51:03.555997",
     "exception": false,
     "start_time": "2024-03-14T21:51:03.515927",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Removal of Additional Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfbc6cb",
   "metadata": {
    "papermill": {
     "duration": 0.041049,
     "end_time": "2024-03-14T21:51:03.639861",
     "exception": false,
     "start_time": "2024-03-14T21:51:03.598812",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Analyzing the data, we observe that several unnecessary words, which are not included in the ready-made set of **stopwords**, keep appearing in the text corpus. We discard these words to remove noise in the classification procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8130cc85",
   "metadata": {
    "papermill": {
     "duration": 0.053561,
     "end_time": "2024-03-14T21:51:03.734449",
     "exception": false,
     "start_time": "2024-03-14T21:51:03.680888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Additional stopwords\n",
    "alphabets = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]\n",
    "prepositions = [\"about\", \"above\", \"across\", \"after\", \"against\", \"among\", \"around\", \"at\", \"before\", \"behind\", \"below\", \"beside\", \"between\", \"by\", \"down\", \"during\", \"for\", \"from\", \"in\", \"inside\", \"into\", \"near\", \"of\", \"off\", \"on\", \"out\", \"over\", \"through\", \"to\", \"toward\", \"under\", \"up\", \"with\"]\n",
    "prepositions_less_common = [\"aboard\", \"along\", \"amid\", \"as\", \"beneath\", \"beyond\", \"but\", \"concerning\", \"considering\", \"despite\", \"except\", \"following\", \"like\", \"minus\", \"onto\", \"outside\", \"per\", \"plus\", \"regarding\", \"round\", \"since\", \"than\", \"till\", \"underneath\", \"unlike\", \"until\", \"upon\", \"versus\", \"via\", \"within\", \"without\"]\n",
    "coordinating_conjunctions = [\"and\", \"but\", \"for\", \"nor\", \"or\", \"so\", \"and\", \"yet\"]\n",
    "correlative_conjunctions = [\"both\", \"and\", \"either\", \"or\", \"neither\", \"nor\", \"not\", \"only\", \"but\", \"whether\", \"or\"]\n",
    "subordinating_conjunctions = [\"after\", \"although\", \"as\", \"as if\", \"as long as\", \"as much as\", \"as soon as\", \"as though\", \"because\", \"before\", \"by the time\", \"even if\", \"even though\", \"if\", \"in order that\", \"in case\", \"in the event that\", \"lest\", \"now that\", \"once\", \"only\", \"only if\", \"provided that\", \"since\", \"so\", \"supposing\", \"that\", \"than\", \"though\", \"till\", \"unless\", \"until\", \"when\", \"whenever\", \"where\", \"whereas\", \"wherever\", \"whether or not\", \"while\"]\n",
    "others = [\"ã\", \"å\", \"ì\", \"û\", \"ûªm\", \"ûó\", \"ûò\", \"ìñ\", \"ûªre\", \"ûªve\", \"ûª\", \"ûªs\", \"ûówe\"]\n",
    "additional_stops = alphabets + prepositions + prepositions_less_common + coordinating_conjunctions + correlative_conjunctions + subordinating_conjunctions + others\n",
    "\n",
    "def remove_additional_stopwords(text):\n",
    "    return \" \".join([word for word in regexp.tokenize(text) if word not in additional_stops])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b55581",
   "metadata": {
    "papermill": {
     "duration": 0.042832,
     "end_time": "2024-03-14T21:51:03.821201",
     "exception": false,
     "start_time": "2024-03-14T21:51:03.778369",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Integration of the Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0032462",
   "metadata": {
    "papermill": {
     "duration": 0.057422,
     "end_time": "2024-03-14T21:51:03.922563",
     "exception": false,
     "start_time": "2024-03-14T21:51:03.865141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_normalizer(text):\n",
    "    text = convert_to_lowercase(text)\n",
    "    text = remove_whitespace(text)\n",
    "    text = re.sub('\\n' , '', text) # converting text to one line\n",
    "    text = re.sub('\\[.*?\\]', '', text) # removing square brackets\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_html(text)\n",
    "    text = remove_emoji(text)\n",
    "    text = remove_http(text)\n",
    "    text = convert_acronyms(text)\n",
    "    text = convert_contractions(text)\n",
    "    text = remove_stopwords(text)\n",
    "    # text = pyspellchecker(text)\n",
    "    #text = text_lemmatizer(text) # text = text_stemmer(text)\n",
    "    text = discard_non_alpha(text)\n",
    "    text = keep_pos(text)\n",
    "    text = remove_additional_stopwords(text)\n",
    "    return text\n",
    "\n",
    "text = \"We'll combine all into 1 SINGLE FUNCTION 🙂 & apply on #training_tweets https://en.wikipedia.org/wiki/Text_normalization\"\n",
    "print(\"Input: {}\".format(text))\n",
    "print(\"Output: {}\".format(text_normalizer(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1556f672",
   "metadata": {
    "papermill": {
     "duration": 0.044006,
     "end_time": "2024-03-14T21:51:04.010766",
     "exception": false,
     "start_time": "2024-03-14T21:51:03.966760",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Implementation on Training Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb923306",
   "metadata": {
    "papermill": {
     "duration": 0.043439,
     "end_time": "2024-03-14T21:51:04.098723",
     "exception": false,
     "start_time": "2024-03-14T21:51:04.055284",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next we implement the text normalization on the training tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd48cf8",
   "metadata": {
    "papermill": {
     "duration": 27.077892,
     "end_time": "2024-03-14T21:51:31.220551",
     "exception": false,
     "start_time": "2024-03-14T21:51:04.142659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_train[\"tokens\"] = data_train[\"text\"].apply(lambda x: regexp.tokenize(x))\n",
    "data_train[\"keyword plus\"] = data_train[\"keyword\"].fillna(\" \")\n",
    "data_train[\"location plus\"] = data_train[\"location\"].fillna(\" \")\n",
    "data_train[\"text plus\"] = data_train[\"keyword plus\"] + \" \" + data_train[\"location plus\"] + \" \" + data_train[\"text\"]\n",
    "data_train[\"tokens plus\"] = data_train[\"text plus\"].apply(regexp.tokenize)\n",
    "\n",
    "data_train[\"normalized text\"] = data_train[\"text\"].apply(text_normalizer) # implementing text normalization\n",
    "data_train[\"normalized tokens\"] = data_train[\"normalized text\"].apply(lambda x: regexp.tokenize(x))\n",
    "data_train[\"normalized text plus\"] = data_train[\"keyword plus\"] + \" \" + data_train[\"location plus\"] + \" \" + data_train[\"normalized text\"]\n",
    "data_train[\"normalized tokens plus\"] = data_train[\"normalized text plus\"].apply(lambda x: regexp.tokenize(x))\n",
    "\n",
    "data_train[[\"id\", \"keyword\", \"location\", \"text\", \"normalized text plus\", \"normalized tokens plus\", \"target\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b57580c",
   "metadata": {
    "papermill": {
     "duration": 0.044124,
     "end_time": "2024-03-14T21:51:31.309648",
     "exception": false,
     "start_time": "2024-03-14T21:51:31.265524",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We implement the text normalization on the test tweets as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32433b9c",
   "metadata": {
    "papermill": {
     "duration": 11.63137,
     "end_time": "2024-03-14T21:51:42.984757",
     "exception": false,
     "start_time": "2024-03-14T21:51:31.353387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_test[\"tokens\"] = data_test[\"text\"].apply(lambda x: regexp.tokenize(x))\n",
    "data_test[\"keyword plus\"] = data_test[\"keyword\"].fillna(\" \")\n",
    "data_test[\"location plus\"] = data_test[\"location\"].fillna(\" \")\n",
    "data_test[\"text plus\"] = data_test[\"keyword plus\"] + \" \" + data_test[\"location plus\"] + \" \" + data_test[\"text\"]\n",
    "data_test[\"tokens plus\"] = data_test[\"text plus\"].apply(lambda x: regexp.tokenize(x))\n",
    "\n",
    "data_test[\"normalized text\"] = data_test[\"text\"].apply(text_normalizer) # implementing text normalization\n",
    "data_test[\"normalized tokens\"] = data_test[\"normalized text\"].apply(lambda x: regexp.tokenize(x))\n",
    "data_test[\"normalized text plus\"] = data_test[\"keyword plus\"] + \" \" + data_test[\"location plus\"] + \" \" + data_test[\"normalized text\"]\n",
    "data_test[\"normalized tokens plus\"] = data_test[\"normalized text plus\"].apply(lambda x: regexp.tokenize(x))\n",
    "\n",
    "data_test_target = data_test.copy()\n",
    "data_test_target['target'] = '?'\n",
    "data_test_target[[\"id\", \"keyword\", \"location\", \"text\", \"normalized text plus\", \"normalized tokens plus\", \"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea1318f",
   "metadata": {
    "papermill": {
     "duration": 0.094748,
     "end_time": "2024-03-14T21:51:43.125361",
     "exception": false,
     "start_time": "2024-03-14T21:51:43.030613",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = data_train['normalized text'].tolist()\n",
    "y = data_train['target'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa750cc9",
   "metadata": {
    "papermill": {
     "duration": 0.287026,
     "end_time": "2024-03-14T21:51:43.455727",
     "exception": false,
     "start_time": "2024-03-14T21:51:43.168701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "del data_test_target\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa0a974",
   "metadata": {
    "papermill": {
     "duration": 0.042914,
     "end_time": "2024-03-14T21:51:43.542882",
     "exception": false,
     "start_time": "2024-03-14T21:51:43.499968",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Bag of N-grams Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d4ad40",
   "metadata": {
    "papermill": {
     "duration": 0.042816,
     "end_time": "2024-03-14T21:51:43.629038",
     "exception": false,
     "start_time": "2024-03-14T21:51:43.586222",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The function defined below, when fed with a text corpus, returns a dataframe consisting of all possible words along with their respective frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d544e8",
   "metadata": {
    "_kg_hide-input": false,
    "papermill": {
     "duration": 0.052616,
     "end_time": "2024-03-14T21:51:43.725571",
     "exception": false,
     "start_time": "2024-03-14T21:51:43.672955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_words(text_list):\n",
    "    CountVec = CountVectorizer(ngram_range = (1, 1))\n",
    "    words = CountVec.fit_transform(text_list)\n",
    "    count_words_df = pd.DataFrame()\n",
    "    count_words_df['Words'] = CountVec.get_feature_names_out()\n",
    "    count_words_df['Frequency'] = words.toarray().sum(axis = 0)\n",
    "    count_words_df.sort_values(by = 'Frequency', ascending = False, inplace = True)\n",
    "    return count_words_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e53fda",
   "metadata": {
    "papermill": {
     "duration": 0.043338,
     "end_time": "2024-03-14T21:51:43.812539",
     "exception": false,
     "start_time": "2024-03-14T21:51:43.769201",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Similarly the next two functions return dataframes consisting of all possible bigrams and trigrams, respectively, along with their corresponding frequencies in the given text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58faa89e",
   "metadata": {
    "papermill": {
     "duration": 0.05161,
     "end_time": "2024-03-14T21:51:43.907784",
     "exception": false,
     "start_time": "2024-03-14T21:51:43.856174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_bigrams(text_list):\n",
    "    CountVec = CountVectorizer(ngram_range = (2, 2))\n",
    "    bigrams = CountVec.fit_transform(text_list)\n",
    "    count_bigrams_df = pd.DataFrame()\n",
    "    count_bigrams_df['Bigrams'] = CountVec.get_feature_names_out()\n",
    "    count_bigrams_df['Frequency'] = bigrams.toarray().sum(axis = 0)\n",
    "    count_bigrams_df.sort_values(by = 'Frequency', ascending = False, inplace = True)\n",
    "    return count_bigrams_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82267a7d",
   "metadata": {
    "papermill": {
     "duration": 0.052487,
     "end_time": "2024-03-14T21:51:44.007128",
     "exception": false,
     "start_time": "2024-03-14T21:51:43.954641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_trigrams(text_list):\n",
    "    CountVec = CountVectorizer(ngram_range = (3, 3))\n",
    "    trigrams = CountVec.fit_transform(text_list)\n",
    "    count_trigrams_df = pd.DataFrame()\n",
    "    count_trigrams_df['Trigrams'] = CountVec.get_feature_names_out()\n",
    "    count_trigrams_df['Frequency'] = trigrams.toarray().sum(axis = 0)\n",
    "    count_trigrams_df.sort_values(by = 'Frequency', ascending = False, inplace = True)\n",
    "    return count_trigrams_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171981d8",
   "metadata": {
    "papermill": {
     "duration": 0.044488,
     "end_time": "2024-03-14T21:51:44.095832",
     "exception": false,
     "start_time": "2024-03-14T21:51:44.051344",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next, we construct the lists of normalized texts from both classes (non-disaster and disaster) of the training tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5052de",
   "metadata": {
    "papermill": {
     "duration": 0.057867,
     "end_time": "2024-03-14T21:51:44.197995",
     "exception": false,
     "start_time": "2024-03-14T21:51:44.140128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_0 = data_train[data_train['target'] == 0]['normalized text'].tolist()\n",
    "X_1 = data_train[data_train['target'] == 1]['normalized text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bba536",
   "metadata": {
    "papermill": {
     "duration": 0.04472,
     "end_time": "2024-03-14T21:51:44.288417",
     "exception": false,
     "start_time": "2024-03-14T21:51:44.243697",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now, we display the top words and bigrams for each class of tweets in the training datasets based on their frequency of occurance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e2cc07",
   "metadata": {
    "papermill": {
     "duration": 0.296868,
     "end_time": "2024-03-14T21:51:44.629880",
     "exception": false,
     "start_time": "2024-03-14T21:51:44.333012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_count_words = [count_words(X_0).head(10), count_words(X_1).head(10)]\n",
    "display_title_words = [\"Words in non-disaster tweets\", \"Words in disaster tweets\"]\n",
    "display_side_by_side(display_count_words, display_title_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d1d797",
   "metadata": {
    "papermill": {
     "duration": 0.39185,
     "end_time": "2024-03-14T21:51:45.066463",
     "exception": false,
     "start_time": "2024-03-14T21:51:44.674613",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_count_bigrams = [count_bigrams(X_0).head(10), count_bigrams(X_1).head(10)]\n",
    "display_title_bigrams = [\"Bigrams in non-disaster tweets\", \"Bigrams in disaster tweets\"]\n",
    "display_side_by_side(display_count_bigrams, display_title_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad90201",
   "metadata": {
    "papermill": {
     "duration": 0.045874,
     "end_time": "2024-03-14T21:51:45.157000",
     "exception": false,
     "start_time": "2024-03-14T21:51:45.111126",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We shall not use trigrams in our models. Nonetheless we exhibit the top trigrams appearing in the non-disaster tweets as well as the disaster tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aa636c",
   "metadata": {
    "papermill": {
     "duration": 0.349613,
     "end_time": "2024-03-14T21:51:45.550317",
     "exception": false,
     "start_time": "2024-03-14T21:51:45.200704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_count_trigrams = [count_trigrams(X_0).head(10), count_trigrams(X_1).head(10)]\n",
    "display_title_trigrams = [\"Trigrams in non-disaster tweets\", \"Trigrams in disaster tweets\"]\n",
    "display_side_by_side(display_count_trigrams, display_title_trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6e77ab",
   "metadata": {
    "papermill": {
     "duration": 0.045102,
     "end_time": "2024-03-14T21:51:45.641316",
     "exception": false,
     "start_time": "2024-03-14T21:51:45.596214",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Bag of Words Model (All Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a6f6a2",
   "metadata": {
    "papermill": {
     "duration": 0.044768,
     "end_time": "2024-03-14T21:51:45.730219",
     "exception": false,
     "start_time": "2024-03-14T21:51:45.685451",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The **bag of words** model is a way of representing text data used in *natural language processing*. The model only considers multiplicity of the words and completely disregards the grammatical structure and ordering of the words. Here we fit the model, treating each word as a feature and observe the **average F1-score obtained from $5$ repetitions of $6$-fold cross-validation** using different classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f7a235",
   "metadata": {
    "papermill": {
     "duration": 412.552808,
     "end_time": "2024-03-14T21:58:38.327831",
     "exception": false,
     "start_time": "2024-03-14T21:51:45.775023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CountVec1 = CountVectorizer(ngram_range = (1, 1))\n",
    "X_fit_transform_1 = CountVec1.fit_transform(X)\n",
    "f1_score_df(X_fit_transform_1, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e233d39c",
   "metadata": {
    "papermill": {
     "duration": 0.044521,
     "end_time": "2024-03-14T21:58:38.418053",
     "exception": false,
     "start_time": "2024-03-14T21:58:38.373532",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We observe that **logistic regression**, **SVM (linear kernel)**, **SVM (RBF kernel)**, **stochastic gradient descent** and **ridge classifier** works well in this prediction scheme, compared to the other classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cdef1f",
   "metadata": {
    "papermill": {
     "duration": 0.044455,
     "end_time": "2024-03-14T21:58:38.507141",
     "exception": false,
     "start_time": "2024-03-14T21:58:38.462686",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Bag of Words Model (Selected Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d4e6fb",
   "metadata": {
    "papermill": {
     "duration": 0.044786,
     "end_time": "2024-03-14T21:58:38.596704",
     "exception": false,
     "start_time": "2024-03-14T21:58:38.551918",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next, we fit the same model, considering only the top $10\\%$ words as a feature and observe the mean F1-score resulting from cross-validations using different classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5b6005",
   "metadata": {
    "papermill": {
     "duration": 209.158629,
     "end_time": "2024-03-14T22:02:07.800141",
     "exception": false,
     "start_time": "2024-03-14T21:58:38.641512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cutoff = 0.1 # The model considers top 10% features\n",
    "\n",
    "X_fit_transform_1_df = pd.DataFrame(X_fit_transform_1.toarray(), columns = CountVec1.get_feature_names_out())\n",
    "X_fit_transform_1_df_sorted = X_fit_transform_1_df.copy()\n",
    "X_fit_transform_1_df_sorted.loc[len(X_fit_transform_1_df_sorted.index)] = X_fit_transform_1.toarray().sum(axis = 0)\n",
    "X_fit_transform_1_df_sorted.sort_values(by = len(X_fit_transform_1_df_sorted.index)-1, axis = 1, ascending = False, inplace = True, kind = 'quicksort', na_position = 'last')\n",
    "X_fit_transform_1_df_sorted.drop(X_fit_transform_1_df_sorted.tail(1).index, inplace = True)\n",
    "select = math.floor(cutoff*len(X_fit_transform_1_df_sorted.columns))\n",
    "X_fit_transform_1_df_selected = X_fit_transform_1_df_sorted.iloc[:, 0:select]\n",
    "X_fit_transform_1_selected = sparse.csr_matrix(X_fit_transform_1_df_selected.to_numpy())\n",
    "\n",
    "f1_score_df(X_fit_transform_1_selected, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9d57e1",
   "metadata": {
    "papermill": {
     "duration": 0.044999,
     "end_time": "2024-03-14T22:02:07.890509",
     "exception": false,
     "start_time": "2024-03-14T22:02:07.845510",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As in the model considering all words as features, **logistic regression**, **SVM (linear kernel)**, **SVM (RBF kernel)**, **stochastic gradient descent** and **ridge classifier** works well in the model considering only the top layer of words, compared to the other classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7785828e",
   "metadata": {
    "papermill": {
     "duration": 0.044956,
     "end_time": "2024-03-14T22:02:07.980769",
     "exception": false,
     "start_time": "2024-03-14T22:02:07.935813",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Bag of Bigrams Model (All Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a9cb14",
   "metadata": {
    "papermill": {
     "duration": 0.0436,
     "end_time": "2024-03-14T22:02:08.069105",
     "exception": false,
     "start_time": "2024-03-14T22:02:08.025505",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next we consider bag of bigrams (pair of consecutive words) model instead of bag of words model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2a682b",
   "metadata": {
    "papermill": {
     "duration": 1019.234666,
     "end_time": "2024-03-14T22:19:07.348284",
     "exception": false,
     "start_time": "2024-03-14T22:02:08.113618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CountVec2 = CountVectorizer(ngram_range = (2, 2))\n",
    "X_fit_transform_2 = CountVec2.fit_transform(X)\n",
    "f1_score_df(X_fit_transform_2, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e61232",
   "metadata": {
    "papermill": {
     "duration": 0.043356,
     "end_time": "2024-03-14T22:19:07.435535",
     "exception": false,
     "start_time": "2024-03-14T22:19:07.392179",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Bag of Bigrams Model (Selected Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826bff19",
   "metadata": {
    "papermill": {
     "duration": 0.043171,
     "end_time": "2024-03-14T22:19:07.522033",
     "exception": false,
     "start_time": "2024-03-14T22:19:07.478862",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next we consider the same model with the top quarter of bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106c1bce",
   "metadata": {
    "papermill": {
     "duration": 278.60698,
     "end_time": "2024-03-14T22:23:46.172321",
     "exception": false,
     "start_time": "2024-03-14T22:19:07.565341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cutoff = 0.25\n",
    "\n",
    "X_fit_transform_2_df = pd.DataFrame(X_fit_transform_2.toarray(), columns = CountVec2.get_feature_names_out())\n",
    "X_fit_transform_2_df_sorted = X_fit_transform_2_df.copy()\n",
    "X_fit_transform_2_df_sorted.loc[len(X_fit_transform_2_df_sorted.index)] = X_fit_transform_2.toarray().sum(axis = 0)\n",
    "X_fit_transform_2_df_sorted.sort_values(by = len(X_fit_transform_2_df_sorted.index)-1, axis = 1, ascending = False, inplace = True, kind = 'quicksort', na_position = 'last')\n",
    "X_fit_transform_2_df_sorted.drop(X_fit_transform_2_df_sorted.tail(1).index, inplace = True)\n",
    "select = math.floor(cutoff*len(X_fit_transform_2_df_sorted.columns))\n",
    "X_fit_transform_2_df_selected = X_fit_transform_2_df_sorted.iloc[:, 0:select]\n",
    "X_fit_transform_2_selected = sparse.csr_matrix(X_fit_transform_2_df_selected.to_numpy())\n",
    "\n",
    "f1_score_df(X_fit_transform_2_selected, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc1c5b2",
   "metadata": {
    "papermill": {
     "duration": 0.046973,
     "end_time": "2024-03-14T22:23:46.266003",
     "exception": false,
     "start_time": "2024-03-14T22:23:46.219030",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We observe that **logistic regression**, **decision tree**, **SVM (linear kernel)**, **SVM (RBF kernel)**, **stochastic gradient descent** and **ridge classifier** work moderately well for the bag of bigrams models but not as good as bag of words models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25de55c",
   "metadata": {
    "papermill": {
     "duration": 0.044586,
     "end_time": "2024-03-14T22:23:46.355796",
     "exception": false,
     "start_time": "2024-03-14T22:23:46.311210",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Mixture Model (All Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51401d34",
   "metadata": {
    "papermill": {
     "duration": 0.055744,
     "end_time": "2024-03-14T22:23:46.458637",
     "exception": false,
     "start_time": "2024-03-14T22:23:46.402893",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now we consider mixture models by considering both words as well as bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5add4524",
   "metadata": {
    "papermill": {
     "duration": 1028.959583,
     "end_time": "2024-03-14T22:40:55.467248",
     "exception": false,
     "start_time": "2024-03-14T22:23:46.507665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_fit_transform_merged_df = pd.concat([X_fit_transform_1_df, X_fit_transform_2_df], axis = 1)\n",
    "X_fit_transform_merged = sparse.csr_matrix(X_fit_transform_merged_df.to_numpy())\n",
    "f1_score_df(X_fit_transform_merged, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7360d3ed",
   "metadata": {
    "papermill": {
     "duration": 0.049252,
     "end_time": "2024-03-14T22:40:55.567194",
     "exception": false,
     "start_time": "2024-03-14T22:40:55.517942",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Mixture Model (Selected Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ccbf7e",
   "metadata": {
    "papermill": {
     "duration": 0.050338,
     "end_time": "2024-03-14T22:40:55.664191",
     "exception": false,
     "start_time": "2024-03-14T22:40:55.613853",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here we consider a mixture of features by considering top $10\\%$ words and top $25\\%$ bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f23a0a",
   "metadata": {
    "papermill": {
     "duration": 326.871239,
     "end_time": "2024-03-14T22:46:22.589872",
     "exception": false,
     "start_time": "2024-03-14T22:40:55.718633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_fit_transform_merged_df_selected = pd.concat([X_fit_transform_1_df_selected, X_fit_transform_2_df_selected], axis = 1)\n",
    "X_fit_transform_merged_selected = sparse.csr_matrix(X_fit_transform_merged_df_selected.to_numpy())\n",
    "f1_score_df(X_fit_transform_merged_selected, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27fdd59",
   "metadata": {
    "papermill": {
     "duration": 0.04653,
     "end_time": "2024-03-14T22:46:22.682844",
     "exception": false,
     "start_time": "2024-03-14T22:46:22.636314",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We observe that the performances under mixture models are more or less similar to those under bag of words models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe0a23b",
   "metadata": {
    "papermill": {
     "duration": 0.463578,
     "end_time": "2024-03-14T22:46:23.193302",
     "exception": false,
     "start_time": "2024-03-14T22:46:22.729724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "del X_fit_transform_1_df, X_fit_transform_1_df_sorted, X_fit_transform_2_df, X_fit_transform_2_df_sorted, X_fit_transform_merged_df, X_fit_transform_1_df_selected, X_fit_transform_2_df_selected, X_fit_transform_merged_df_selected\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70baf664",
   "metadata": {
    "papermill": {
     "duration": 0.04743,
     "end_time": "2024-03-14T22:46:23.289253",
     "exception": false,
     "start_time": "2024-03-14T22:46:23.241823",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TF-IDF Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fe2b72",
   "metadata": {
    "papermill": {
     "duration": 0.047377,
     "end_time": "2024-03-14T22:46:23.384093",
     "exception": false,
     "start_time": "2024-03-14T22:46:23.336716",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next we implement the **term frequency-inverse document frequency** (TFIDF) model.\n",
    "\n",
    "The *term frequency* (TF) is the number of times a word appears in a text, divded by the total number of words appearing in the text. On the other hand, *inverse document frequency* (IDF) is the logarithm of the number of texts in the corpus, divided by the number of texts that contain the specific word. IDF determines the weight of rare words across all texts in the corpus. TF-IDF is the product of these two quantities. It objectively evaluates how relevant a word is to a text in a collection of texts, taking into consideration that some words appear more frequently in general."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e1b43c",
   "metadata": {
    "papermill": {
     "duration": 0.047887,
     "end_time": "2024-03-14T22:46:23.480532",
     "exception": false,
     "start_time": "2024-03-14T22:46:23.432645",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Bag of Words Model (All Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8738adc6",
   "metadata": {
    "papermill": {
     "duration": 420.499345,
     "end_time": "2024-03-14T22:53:24.027454",
     "exception": false,
     "start_time": "2024-03-14T22:46:23.528109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TfidfVec1 = TfidfVectorizer(ngram_range = (1, 1))\n",
    "X_fit_transform_tfidf_1 = TfidfVec1.fit_transform(X)\n",
    "f1_score_df(X_fit_transform_tfidf_1, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afae97d",
   "metadata": {
    "papermill": {
     "duration": 0.047394,
     "end_time": "2024-03-14T22:53:24.123719",
     "exception": false,
     "start_time": "2024-03-14T22:53:24.076325",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We observe that **logistic regression**, **SVM (linear kernel)**, **SVM (RBF kernel)**, **stochastic gradient descent** and **ridge classifier** works well in this prediction scheme, compared to the other classifiers. In fact, logistic regression, the classifier returning the highest **F1-score**, has a slight improvement over the same model without TFIDF implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c24e70c",
   "metadata": {
    "papermill": {
     "duration": 0.048091,
     "end_time": "2024-03-14T22:53:24.219265",
     "exception": false,
     "start_time": "2024-03-14T22:53:24.171174",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Bag of Words Model (Selected Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c374b86",
   "metadata": {
    "papermill": {
     "duration": 188.165359,
     "end_time": "2024-03-14T22:56:32.432550",
     "exception": false,
     "start_time": "2024-03-14T22:53:24.267191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cutoff = 0.1\n",
    "\n",
    "X_fit_transform_tfidf_1_df = pd.DataFrame(X_fit_transform_tfidf_1.toarray(), columns = TfidfVec1.get_feature_names_out())\n",
    "X_fit_transform_tfidf_1_df_sorted = X_fit_transform_tfidf_1_df.copy()\n",
    "X_fit_transform_tfidf_1_df_sorted.loc[len(X_fit_transform_tfidf_1_df_sorted.index)] = X_fit_transform_tfidf_1.toarray().sum(axis = 0)\n",
    "X_fit_transform_tfidf_1_df_sorted.sort_values(by = len(X_fit_transform_tfidf_1_df_sorted.index)-1, axis = 1, ascending = False, inplace = True, kind = 'quicksort', na_position = 'last')\n",
    "X_fit_transform_tfidf_1_df_sorted.drop(X_fit_transform_tfidf_1_df_sorted.tail(1).index, inplace = True)\n",
    "select = math.floor(cutoff*len(X_fit_transform_tfidf_1_df_sorted.columns))\n",
    "X_fit_transform_tfidf_1_df_selected = X_fit_transform_tfidf_1_df_sorted.iloc[:, 0:select]\n",
    "X_fit_transform_tfidf_1_selected = sparse.csr_matrix(X_fit_transform_tfidf_1_df_selected.to_numpy())\n",
    "\n",
    "f1_score_df(X_fit_transform_tfidf_1_selected, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5c0352",
   "metadata": {
    "papermill": {
     "duration": 0.052088,
     "end_time": "2024-03-14T22:56:32.534226",
     "exception": false,
     "start_time": "2024-03-14T22:56:32.482138",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Bag of Bigrams Model (All Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95281bc",
   "metadata": {
    "papermill": {
     "duration": 1019.249523,
     "end_time": "2024-03-14T23:13:31.834786",
     "exception": false,
     "start_time": "2024-03-14T22:56:32.585263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TfidfVec2 = TfidfVectorizer(ngram_range = (2, 2))\n",
    "X_fit_transform_tfidf_2 = TfidfVec2.fit_transform(X)\n",
    "f1_score_df(X_fit_transform_tfidf_2, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2e1e22",
   "metadata": {
    "papermill": {
     "duration": 0.046145,
     "end_time": "2024-03-14T23:13:31.926448",
     "exception": false,
     "start_time": "2024-03-14T23:13:31.880303",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Bag of Bigrams Model (Selected Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d3fb68",
   "metadata": {
    "papermill": {
     "duration": 131.199194,
     "end_time": "2024-03-14T23:15:43.172187",
     "exception": false,
     "start_time": "2024-03-14T23:13:31.972993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cutoff = 0.1\n",
    "\n",
    "X_fit_transform_tfidf_2_df = pd.DataFrame(X_fit_transform_tfidf_2.toarray(), columns = TfidfVec2.get_feature_names_out())\n",
    "X_fit_transform_tfidf_2_df_sorted = X_fit_transform_tfidf_2_df.copy()\n",
    "X_fit_transform_tfidf_2_df_sorted.loc[len(X_fit_transform_tfidf_2_df_sorted.index)] = X_fit_transform_tfidf_2.toarray().sum(axis = 0)\n",
    "X_fit_transform_tfidf_2_df_sorted.sort_values(by = len(X_fit_transform_tfidf_2_df_sorted.index)-1, axis = 1, ascending = False, inplace = True, kind = 'quicksort', na_position = 'last')\n",
    "X_fit_transform_tfidf_2_df_sorted.drop(X_fit_transform_tfidf_2_df_sorted.tail(1).index, inplace = True)\n",
    "select = math.floor(cutoff*len(X_fit_transform_tfidf_2_df_sorted.columns))\n",
    "X_fit_transform_tfidf_2_df_selected = X_fit_transform_tfidf_2_df_sorted.iloc[:, 0:select]\n",
    "X_fit_transform_tfidf_2_selected = sparse.csr_matrix(X_fit_transform_tfidf_2_df_selected.to_numpy())\n",
    "\n",
    "f1_score_df(X_fit_transform_tfidf_2_selected, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85cf73f",
   "metadata": {
    "papermill": {
     "duration": 0.047272,
     "end_time": "2024-03-14T23:15:43.266261",
     "exception": false,
     "start_time": "2024-03-14T23:15:43.218989",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Mixture Model (All Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700515e8",
   "metadata": {
    "papermill": {
     "duration": 1042.152562,
     "end_time": "2024-03-14T23:33:05.465866",
     "exception": false,
     "start_time": "2024-03-14T23:15:43.313304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_fit_transform_tfidf_merged_df = pd.concat([X_fit_transform_tfidf_1_df, X_fit_transform_tfidf_2_df], axis = 1)\n",
    "X_fit_transform_tfidf_merged = sparse.csr_matrix(X_fit_transform_tfidf_merged_df.to_numpy())\n",
    "f1_score_df(X_fit_transform_tfidf_merged, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9432169",
   "metadata": {
    "papermill": {
     "duration": 0.047101,
     "end_time": "2024-03-14T23:33:05.559231",
     "exception": false,
     "start_time": "2024-03-14T23:33:05.512130",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Mixture Model (Selected Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c98682",
   "metadata": {
    "papermill": {
     "duration": 229.860325,
     "end_time": "2024-03-14T23:36:55.465851",
     "exception": false,
     "start_time": "2024-03-14T23:33:05.605526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_fit_transform_tfidf_merged_df_selected = pd.concat([X_fit_transform_tfidf_1_df_selected, X_fit_transform_tfidf_2_df_selected], axis = 1)\n",
    "X_fit_transform_tfidf_merged_selected = sparse.csr_matrix(X_fit_transform_tfidf_merged_df_selected.to_numpy())\n",
    "f1_score_df(X_fit_transform_tfidf_merged_selected, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd6922d",
   "metadata": {
    "papermill": {
     "duration": 0.047765,
     "end_time": "2024-03-14T23:36:55.563017",
     "exception": false,
     "start_time": "2024-03-14T23:36:55.515252",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We observe that the results of the mixture models are more or less similar to that of the bag of words models. Also, the KNN classifier works poorly in all the prediction schemes described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8c82ee",
   "metadata": {
    "papermill": {
     "duration": 0.417437,
     "end_time": "2024-03-14T23:36:56.028151",
     "exception": false,
     "start_time": "2024-03-14T23:36:55.610714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "del X_fit_transform_tfidf_1_df, X_fit_transform_tfidf_1_df_sorted, X_fit_transform_tfidf_2_df, X_fit_transform_tfidf_2_df_sorted, X_fit_transform_tfidf_merged_df, X_fit_transform_tfidf_1_df_selected, X_fit_transform_tfidf_2_df_selected, X_fit_transform_tfidf_merged_df_selected\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8205e598",
   "metadata": {
    "papermill": {
     "duration": 0.064489,
     "end_time": "2024-03-15T02:14:29.552038",
     "exception": false,
     "start_time": "2024-03-15T02:14:29.487549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Best score: {}\".format(max(f1_score_max)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b57b2b",
   "metadata": {
    "papermill": {
     "duration": 0.05167,
     "end_time": "2024-03-15T02:14:29.654331",
     "exception": false,
     "start_time": "2024-03-15T02:14:29.602661",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Acknowledgements\n",
    "\n",
    "- [**Basic EDA,Cleaning and GloVe**](https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove) by [**Shahules**](https://www.kaggle.com/shahules)\n",
    "- [**NLP with Disaster Tweets - EDA, Cleaning and BERT**](https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert) by [**Gunes Evitan**](https://www.kaggle.com/gunesevitan)\n",
    "- [**Concrete solutions to real problems - An NLP workshop**](https://github.com/hundredblocks/concrete_NLP_tutorial/blob/master/NLP_notebook.ipynb) by [**Emmanuel Ameisen**](https://github.com/hundredblocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d83fb04",
   "metadata": {
    "papermill": {
     "duration": 0.052804,
     "end_time": "2024-03-15T02:14:29.759808",
     "exception": false,
     "start_time": "2024-03-15T02:14:29.707004",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# References\n",
    "\n",
    "- [**List of English contractions (Wikipedia)**](https://en.wikipedia.org/wiki/Wikipedia:List_of_English_contractions)\n",
    "- [**Alphabetical list of part-of-speech tags used in the Penn Treebank Project**](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56db78d",
   "metadata": {
    "papermill": {
     "duration": 0.058058,
     "end_time": "2024-03-15T02:14:29.868581",
     "exception": false,
     "start_time": "2024-03-15T02:14:29.810523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Runtime and memory usage\n",
    "stop = time.time()\n",
    "process = psutil.Process(os.getpid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e89d40",
   "metadata": {
    "papermill": {
     "duration": 0.058784,
     "end_time": "2024-03-15T02:14:29.978416",
     "exception": false,
     "start_time": "2024-03-15T02:14:29.919632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Process runtime: %.2f seconds\" %float(stop - start))\n",
    "print(\"Process memory usage: %.2f MB\" %float(process.memory_info()[0]/(1024*1024)))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1595713,
     "sourceId": 2624724,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30664,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python (myvenv)",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15875.340456,
   "end_time": "2024-03-15T02:14:32.673897",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-14T21:49:57.333441",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
