{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9505d0bf-9404-43d2-bc6a-fcffac623ab0",
   "metadata": {},
   "source": [
    "# SDXL DreamBooth LoRA Training\n",
    "\n",
    "## Description of Project\n",
    "\n",
    "This project aims to utilize advanced techniques in machine learning and image processing to train a Stable Diffusion model for generating specific images. The primary technique employed is DreamBooth, enhanced with Low-Rank Adaptation (LoRA). DreamBooth allows for fine-tuning pre-trained diffusion models on custom datasets, significantly improving the model's ability to generate desired images based on provided examples.\n",
    "\n",
    "In this particular project, the model is trained on a dataset of 10 images featuring the cartoon character Tom from \"Tom and Jerry\". By fine-tuning the model with these specific images, it learns to generate images that closely resemble the style, features, and characteristics of Tom. This capability can be extended to other datasets and characters, making the model highly versatile for various image generation tasks.\n",
    "\n",
    "## Objective of Project\n",
    "\n",
    "The primary objectives of this project are:\n",
    "\n",
    "1. **Custom Image Generation**:\n",
    "   - Develop a model capable of generating high-quality images based on a specific dataset of images.\n",
    "   - Fine-tune the Stable Diffusion model to replicate the style and features of the provided training images.\n",
    "\n",
    "2. **Application of DreamBooth and LoRA Techniques**:\n",
    "   - Implement DreamBooth and LoRA techniques to enhance the fine-tuning process.\n",
    "   - Demonstrate the effectiveness of these techniques in improving the model's ability to generate accurate and detailed images.\n",
    "\n",
    "3. **User-Friendly Image Upload and Training**:\n",
    "   - Add functionality to the notebook that allows users to upload their own images for training.\n",
    "   - Ensure that the uploaded images are properly stored and used for model fine-tuning, making the process accessible and user-friendly.\n",
    "\n",
    "4. **Efficiency and Optimization**:\n",
    "   - Optimize the training process to reduce redundant operations and improve efficiency.\n",
    "   - Implement methods to streamline data loading, preprocessing, and model training steps.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The dataset used for this project consists of 10 images. \n",
    "You can train the model on any dataset of your choice by providing the appropriate images.\n",
    "No dataset is provided in this example.\n",
    "\n",
    "## How To Use\n",
    "\n",
    "1. **Setup Environment**:\n",
    "   - Clone the repository or download the specific project files.\n",
    "   - Ensure Python 3.x is installed.\n",
    "   - Install the required packages listed in the `requirements.txt` file.\n",
    "\n",
    "2. **Install Required Packages**:\n",
    "   \n",
    "   - To enhance the functionality of the CoreAI environment, you may need to install some libraries not pre-installed but required for this notebook. Follow these steps to install the necessary libraries:\n",
    "\n",
    "   **2.1 Create and Activate the Virtual Environment:**\n",
    "   \n",
    "   Open your terminal or command prompt within the Jupyter notebook. Navigate to `File -> New -> Terminal` and type `bash` to get a shell compatible with the following commands.\n",
    "\n",
    "   Navigate to the project directory where you want to set up the environment.\n",
    "\n",
    "   Execute the following commands to create and activate the virtual environment:\n",
    "\n",
    "   ```sh\n",
    "   python3 -m venv --system-site-packages myvenv\n",
    "   source myvenv/bin/activate\n",
    "   pip3 install ipykernel\n",
    "   python -m ipykernel install --user --name=myvenv --display-name=\"Python (myvenv)\"\n",
    "   ```\n",
    "\n",
    "\n",
    "   **2.2 Install Required Libraries**\n",
    "\n",
    "   Before running the following command in the Jupyter notebook, make sure you are in the directory where the Jupyter Notebook and virtual environment is located. Load the newly created \"Python (myvenv)\" kernel. This ensures the `./` path is always current. You can use the `cd` command to change to your project directory and `pwd` to verify your current directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91c365e-9bbe-4e9f-84a0-9987f5aee233",
   "metadata": {},
   "outputs": [],
   "source": [
    "!. ./myvenv/bin/activate; pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0116642d-452c-4c7c-a4a8-ad7e4eccf278",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "pwd = os.getcwd()\n",
    "os.environ['PATH'] =  os.path.join(pwd, 'myvenv/bin') + os.pathsep + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25f713c-60b1-4132-87e2-1fb47128d2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo $PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3363a9c5-e802-4c8a-8078-e822c26a97f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! which accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd48e317-9ec6-4fa8-81ff-2fc95a02f77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://raw.githubusercontent.com/huggingface/diffusers/main/examples/dreambooth/train_dreambooth_lora_sdxl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0fd037-7e41-441f-9c5f-b496415ee783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Image\n",
    "import os\n",
    "target_directory = 'input_images' # You can change the name of target directory.\n",
    "\n",
    "# Ensure the target directory exists\n",
    "if not os.path.exists(target_directory):\n",
    "    os.makedirs(target_directory, exist_ok=True)\n",
    "\n",
    "print(f\"Target directory: {target_directory}\")\n",
    "\n",
    "# Function to handle uploaded files\n",
    "def handle_upload(change):\n",
    "    print(\"Upload started...\")\n",
    "    # Print the structure of 'change' to understand its content\n",
    "    print(change)\n",
    "    \n",
    "    for file_upload in change['new']:\n",
    "        print(f\"Handling file: {file_upload}\")\n",
    "        filepath = os.path.join(target_directory, file_upload['name'])\n",
    "        print(f\"Saving to: {filepath}\")\n",
    "        with open(filepath, 'wb') as f:\n",
    "            f.write(file_upload['content'])\n",
    "        print(f'Saved {file_upload[\"name\"]} to {filepath}')\n",
    "    # List the files in the target directory after upload\n",
    "    print(f'Files in target directory ({target_directory}): {list(Path(target_directory).glob(\"*\"))}')\n",
    "    print(\"Upload completed.\")\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "upload_widget = widgets.FileUpload()\n",
    "\n",
    "def handle_upload_with_output(change):\n",
    "    with output:\n",
    "        handle_upload(change)\n",
    "\n",
    "upload_widget.observe(handle_upload_with_output, names='value')\n",
    "display(upload_widget, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092db262-dba1-4357-8065-7031fd8b4307",
   "metadata": {},
   "source": [
    "The `image_grid` function arranges a list of images into a grid with specified rows and columns, optionally resizing the images, and returns the resulting combined image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5f75f6-8b89-4be0-890f-dc3bbfa1fcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def image_grid(imgs, rows, cols, resize=256):\n",
    "\n",
    "    if resize is not None:\n",
    "        imgs = [img.resize((resize, resize)) for img in imgs]\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
    "    grid_w, grid_h = grid.size\n",
    "\n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i % cols * w, i // cols * h))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7002dcc7-99dc-42ae-82df-bc657a316ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "os.environ['DATASET_NAME'] = target_directory\n",
    "! rm -rf ${DATASET_NAME}/metadata.jsonl\n",
    "# change path to display images from your local dir\n",
    "img_paths = f\"{target_directory}/*.*\"  # Make sure you change name of directory to whatever name you to target directory while uploading the images.\n",
    "imgs = [Image.open(path) for path in glob.glob(img_paths)]\n",
    "\n",
    "num_imgs_to_preview = 6\n",
    "image_grid(imgs[:num_imgs_to_preview], 1, num_imgs_to_preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e192cc-eb0a-489c-b4af-cb0703ed1023",
   "metadata": {},
   "source": [
    "The below cell will loads a pre-trained BLIP image captioning model and uses it to generate descriptive captions for input images, utilizing GPU if available for faster processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabe4cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('HF_HOME', exist_ok=True)\n",
    "os.environ['HF_HOME'] = 'HF_HOME'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb15000-0d59-49b0-acfb-e3c13ef9d769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# load the processor and the captioning model\n",
    "blip_processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\",torch_dtype=torch.float16).to(device)\n",
    "\n",
    "# captioning utility\n",
    "def caption_images(input_image):\n",
    "    inputs = blip_processor(images=input_image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "    pixel_values = inputs.pixel_values\n",
    "\n",
    "    generated_ids = blip_model.generate(pixel_values=pixel_values, max_length=50)\n",
    "    generated_caption = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return generated_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f12002-c2b3-4947-b37b-25d4912322c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "# create a list of (Pil.Image, path) pairs\n",
    "local_dir = target_directory + \"/\"\n",
    "imgs_and_paths = [(path,Image.open(path)) for path in glob.glob(f\"{local_dir}*.*\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2cc13e-73d4-4cb2-8470-97e85d627c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_and_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a06aca-2504-472f-90e4-403afc1c7938",
   "metadata": {},
   "source": [
    "The below code cell generates captions for images with a specified prefix and saves the image filename and caption as JSON entries in a `metadata.jsonl` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5d3001-105d-4efe-b7f2-0e7de7c394a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"INSTANCE_PROMPT\"] = \"a photo of xyza\"\n",
    "\n",
    "import json\n",
    "# you can change the caption_prefix but make sure you use some unique token as okdme used here.\n",
    "caption_prefix = os.environ[\"INSTANCE_PROMPT\"] + \", \" #@param \n",
    "with open(f'{local_dir}metadata.jsonl', 'w') as outfile:\n",
    "  for img in imgs_and_paths:\n",
    "      caption = caption_prefix + caption_images(img[1]).split(\"\\n\")[0]\n",
    "      entry = {\"file_name\":img[0].split(\"/\")[-1], \"prompt\": caption}\n",
    "      json.dump(entry, outfile)\n",
    "      outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ce90cf-8346-43f5-ba2d-f9dcc17bfc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# delete the BLIP pipelines and free up some memory\n",
    "del blip_processor, blip_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9604d2-d6bb-47b9-b71e-5a775eb33c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "! accelerate config default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce9f7dd-9892-44bb-b148-d76a37811d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env bash\n",
    "! accelerate launch train_dreambooth_lora_sdxl.py \\\n",
    "  --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-xl-base-1.0\" \\\n",
    "  --pretrained_vae_model_name_or_path=\"madebyollin/sdxl-vae-fp16-fix\" \\\n",
    "  --dataset_name=\"${DATASET_NAME}\" \\\n",
    "  --output_dir=\"output__LoRA\" \\\n",
    "  --caption_column=\"prompt\" \\\n",
    "  --mixed_precision=\"fp16\" \\\n",
    "  --instance_prompt=\"${INSTANCE_PROMPT}\" \\\n",
    "  --resolution=1024 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --gradient_accumulation_steps=3 \\\n",
    "  --gradient_checkpointing \\\n",
    "  --learning_rate=1e-4 \\\n",
    "  --snr_gamma=5.0 \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --use_8bit_adam \\\n",
    "  --max_train_steps=500 \\\n",
    "  --checkpointing_steps=717 \\\n",
    "  --seed=\"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1c3aea-a813-4d5a-b240-19c4ae49b6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline, AutoencoderKL\n",
    "import torch\n",
    "# Load the VAE\n",
    "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\n",
    "\n",
    "# Load the diffusion pipeline with the VAE\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    vae=vae,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    "    use_safetensors=True\n",
    ")\n",
    "\n",
    "# Specify the local path to the LoRA weights\n",
    "local_lora_weights_path = \"output__LoRA/pytorch_lora_weights.safetensors\" #Ensure you add correct path where your safetensors are installed\n",
    "\n",
    "# Load the LoRA weights\n",
    "pipe.load_lora_weights(local_lora_weights_path)\n",
    "\n",
    "# Move the pipeline to GPU\n",
    "_ = pipe.to(\"cuda\")\n",
    "\n",
    "print(\"Pipeline loaded with LoRA weights and moved to GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7a8646-26e9-4c77-8682-937ac0d6eaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a photo of xyza eating dessert\" # @param\n",
    "\n",
    "image = pipe(prompt=prompt, num_inference_steps=100).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044ad8a4-3a62-4cac-80a4-01a313c47255",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a photo of xyza eating dessert on beach\" # @param\n",
    "\n",
    "image = pipe(prompt=prompt, num_inference_steps=100).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3160d09c-2275-43e6-89c9-e444d92109c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a photo of xyza playing baseball with red t-shirt\" # @param\n",
    "\n",
    "image = pipe(prompt=prompt, num_inference_steps=100).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb62db78-b073-4f06-885b-f820593f785c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This project successfully fine-tuned a Stable Diffusion model using DreamBooth and LoRA techniques to generate high-quality images of the cartoon character Tom from \"Tom and Jerry\". Enhancements included user-friendly image upload functionality, efficiency improvements. The project demonstrated the effectiveness of these techniques for custom image generation, providing a versatile framework adaptable to various datasets and applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myvenv)",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
